[
  {
    "id": 41,
    "post_id": "8FWCrJ7WW5z9qF9bn",
    "title": "Using the EA Forum\u2019s Poll Feature",
    "title_normalized": "using the ea forums poll feature",
    "page_url": "https://forum.effectivealtruism.org/posts/8FWCrJ7WW5z9qF9bn/using-the-ea-forum-s-poll-feature",
    "html_body": "<p>Including a poll in your EA Forum post is a great way to encourage more discussion. Once a user votes on a poll, they are prompted to leave a comment explaining their vote. This has been surprisingly effective at encouraging comments, and comments lead to discussion.&nbsp;<br><br>You can also share a screenshot of your poll when you share your post \u2014 this gives readers a clear call to action (go vote!)</p><h2 data-internal-id=\"How_does_the_poll_feature_work_\"><strong>How does the poll feature work?</strong></h2><p>It\u2019ll look like the polls we\u2019ve had for other debate weeks \u2013 it\u2019ll be one axis, with a custom title, and custom labels on each end. You can use the poll to:</p><ul><li>Ask a yes or no question.</li><li>Determine the reader\u2019s preference between two outcomes.</li><li>Ask for a level of agreement with a statement.</li></ul><h3 data-internal-id=\"Putting_a_poll_in_your_post\"><strong>Putting a poll in your post</strong></h3><p>When you are drafting a post, select some text. An editor toolbar will appear, with the option to insert a poll. Click the button.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/8FWCrJ7WW5z9qF9bn/bmarewk1gxqfyanyyx4g\" alt=\"\"></p><p>This will insert a poll into your post. If you click on the poll a form will open to edit it, like so:</p><figure class=\"table\"><table><tbody><tr><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><img style=\"width:99.9%\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/8FWCrJ7WW5z9qF9bn/pb1icn6qovey2m4y33c1\" alt=\"\"></td></tr></tbody></table></figure><p>Some things to note:</p><ul><li>You can add as many polls as you like to a post.</li><li>If you add a poll to a draft, the clock won't start counting down until the post is published. The default time on the clock is set to 7 days.</li><li>The widget that shows in the editor is a simplified version of what will show in the post (e.g. it doesn't have the vote count), so don't worry if the published version looks slightly different.</li></ul><h3 data-internal-id=\"Interacting_with_the_poll_as_a_reader\"><strong>Interacting with the poll as a reader</strong></h3><p>You drag your avatar along the axis to vote, and you'll then have the option to leave a comment.</p><figure class=\"table\"><table><tbody><tr><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><img style=\"width:100%\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/8FWCrJ7WW5z9qF9bn/gtfzvutbfzqttz2v5uw2\" alt=\"\"></td></tr></tbody></table></figure><p>If you leave a comment, it\u2019ll appear on the poll, and in the comment section of the post the poll is hosted on.</p><figure class=\"table\"><table><tbody><tr><td style=\"padding:5pt;vertical-align:top\" colspan=\"1\" rowspan=\"1\"><img style=\"width:100%\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/8FWCrJ7WW5z9qF9bn/x3mogsa21ttgrk2fyyjy\" alt=\"\">View when hovering over a user who left a comment on the poll</td></tr></tbody></table></figure><p>So that readers can contextualise your comment, a percentage will appear on the comment to indicate the strength of your vote, and whether you have changed your mind since your first vote. To avoid confusion on posts with multiple polls, the title of the poll will be quoted at the top of your comment.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/9e9216d3e9882b828c52973b29ce55ad2bef47c120076a95c499b7a1bb7aba0f/ctskmydjqxqljiunbimj\" alt=\"\"></p>",
    "base_score": 12,
    "comment_count": 0,
    "posted_at": "2025-08-20T08:19:13.410000Z",
    "created_at": "2025-08-21T03:27:02.335502Z",
    "author_id": "CF3HuBjWBDXgeTp9p",
    "author_display_name": "Toby Tremlett\ud83d\udd39",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "75HnrKT9FNWKxxtPY",
      "9tdEph2aXYqwzmmRm",
      "EHLmbEmJ2Qd5WfwTb",
      "ZCihBFp5P64JCvQY6",
      "zeAAdDDJTW6GdGKHF"
    ],
    "tag_names": [
      "Building effective altruism",
      "Collections and resources",
      "Community",
      "Effective Altruism Forum",
      "Polls"
    ],
    "markdown_content": "Including a poll in your EA Forum post is a great way to encourage more discussion. Once a user votes on a poll, they are prompted to leave a comment explaining their vote. This has been surprisingly effective at encouraging comments, and comments lead to discussion.\u00a0  \n  \nYou can also share a screenshot of your poll when you share your post \u2014 this gives readers a clear call to action (go vote!)\n\n## **How does the poll feature work?**\n\nIt\u2019ll look like the polls we\u2019ve had for other debate weeks \u2013 it\u2019ll be one axis, with a custom title, and custom labels on each end. You can use the poll to:\n\n- Ask a yes or no question.\n- Determine the reader\u2019s preference between two outcomes.\n- Ask for a level of agreement with a statement.\n\n### **Putting a poll in your post**\n\nWhen you are drafting a post, select some text. An editor toolbar will appear, with the option to insert a poll. Click the button.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/8FWCrJ7WW5z9qF9bn/bmarewk1gxqfyanyyx4g)\n\nThis will insert a poll into your post. If you click on the poll a form will open to edit it, like so:\n\n|  |\n| --- |\n|  |\n\nSome things to note:\n\n- You can add as many polls as you like to a post.\n- If you add a poll to a draft, the clock won't start counting down until the post is published. The default time on the clock is set to 7 days.\n- The widget that shows in the editor is a simplified version of what will show in the post (e.g. it doesn't have the vote count), so don't worry if the published version looks slightly different.\n\n### **Interacting with the poll as a reader**\n\nYou drag your avatar along the axis to vote, and you'll then have the option to leave a comment.\n\n|  |\n| --- |\n|  |\n\nIf you leave a comment, it\u2019ll appear on the poll, and in the comment section of the post the poll is hosted on.\n\n|  |\n| --- |\n| View when hovering over a user who left a comment on the poll |\n\nSo that readers can contextualise your comment, a percentage will appear on the comment to indicate the strength of your vote, and whether you have changed your mind since your first vote. To avoid confusion on posts with multiple polls, the title of the poll will be quoted at the top of your comment.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/9e9216d3e9882b828c52973b29ce55ad2bef47c120076a95c499b7a1bb7aba0f/ctskmydjqxqljiunbimj)",
    "word_count": 413,
    "reading_time_minutes": 2,
    "external_links": null,
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T03:27:04.672671Z"
  },
  {
    "id": 1546,
    "post_id": "bL99PnBJBGnquHmed",
    "title": "Quadrillion-dollar bills on the sidewalk? Quantifying animal welfare changes",
    "title_normalized": "quadrilliondollar bills on the sidewalk quantifying animal welfare changes",
    "page_url": "https://forum.effectivealtruism.org/posts/bL99PnBJBGnquHmed/quadrillion-dollar-bills-on-the-sidewalk-quantifying-animal",
    "html_body": "<p>How important is animal welfare as an issue? And how should it be traded against other concerns (if we must)?</p><p>Work over the last few years has begun to <a href=\"https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/article/regulators-should-value-nonhuman-animals/45FDAE481414E32463E396F0A50C2E5C\">highlight</a> the case for quantifying animal welfare impacts in dollar terms \u2013 for the purpose of comparing different causes and costs and benefits \u2013 and arguably made progress with the necessary foundations to do that, including assessing how we should weight the experience of different animals. This post summarises some of that work, particularly '<a href=\"https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/article/monetizing-animal-welfare-impacts-for-benefitcost-analysis/4558DB0D80DA18D4DC25788E967C0ACD\">Monetizing Animal Welfare Impacts for Benefit\u2013Cost Analysis</a>' by <a href=\"https://forum.effectivealtruism.org/users/bob-fischer?mention=user\">@Bob Fischer</a> and others, and tries to impatiently rush to one end of this process by putting some $ numbers on animal suffering changes. Hopefully this post helps provide a useful walk-through of <i>some </i>of the concepts, makes the case for further work, and demonstrates why some people think farm animal welfare is not just a 'nice-to-have' consideration but one of the most important topics in the world. (I\u2019d like to do more on <a href=\"https://sentientmedia.org/animal-welfare-economics/\">animal welfare economics</a>, and this is my first foray, so feedback would be very welcome! Apologies to the real experts for any butchering, so to speak.)</p><h2><strong>Quantifying health changes</strong></h2><p>Our goal here is to put a $ or \u00a3 value on changes in animal welfare \u2013 and I'll be focusing here on <a href=\"https://ourworldindata.org/data-insights/billions-of-chickens-ducks-and-pigs-are-slaughtered-for-meat-every-year\">farmed chickens</a> given how numerous our dinosaur cousins are. The idea is that quantifying animal welfare, sharing a common unit with many other areas of life and public policy, can help people gauge the relative importance of the issue, and to include these changes in cost-benefit analyses, alongside many other considerations which are already included, such as impacts on incomes, health and global warming. (The inclusion of estimated climate impacts is a good example of how it may be worthwhile to put a value on some things that were previously excluded from such analysis, but also creates significant risks where emissions and animal welfare considerations are in conflict but only the former is included.)</p><p>To begin, it\u2019s worth giving a human example. UK Treasury <a href=\"https://www.gov.uk/government/publications/the-green-book-appraisal-and-evaluation-in-central-government/the-green-book-2020\">guidance</a> says that the societal value of a \u2018Quality-Adjusted Life Year\u2019 is \u00a370,000 in 2020-21 prices, so around \u00a389,000 in 2025-26. Extending someone\u2019s life in full health by a year may then, on paper, be valued at \u00a389,000. Going from half-health to full-health for two years is also worth \u00a389,000 ('0.5' health-improvement * 2 years = 1 QALY). Doing this for 1,000 people would be worth \u00a389,000,000. (This kind of valuation is standard practice, even if it sounds funky, and can be based on the public's actual views about potential income versus health trade-offs.)</p><p><a href=\"https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/article/monetizing-animal-welfare-impacts-for-benefitcost-analysis/4558DB0D80DA18D4DC25788E967C0ACD\">'Monetizing animal welfare...'</a> makes the case that we can and should do similar calculations for other species, and to do this we may 'only' need 4 numbers that can then be multiplied to quantify the scale of an impact.</p><ol><li>('Easy') The value of a human QALY. Standards for valuing human QALYs will vary over time and by country and are of course debateable, but we don't need to reinvent them here. As a Brit, I'll use \u00a389,000. The most common value used in US research now <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC10114019/\">appears to be $100,000</a> per QALY (around \u00a380,000) while for US impact assessments the <a href=\"https://aspe.hhs.gov/sites/default/files/documents/cd2a1348ea0777b1aa918089e4965b8c/standard-ria-values.pdf\">standard</a> might be $591,000. (I think \u00a389,000 makes sense as an order of magnitude: for me specifically, an extra year of healthy life would definitely be worth more than \u00a39,000 of my money, but \u00a3900,000 would be unaffordable.)</li><li>(Easy) The number of animals involved.</li><li>(Hard!) A factor for comparing each species\u2019s 'welfare potential' to that of humans. This could optionally include some arbitrary speciesism if you like, but for now let\u2019s try to take a utilitarian, <s>bird's </s>god's eye view (e.g. a <a href=\"https://en.wikipedia.org/wiki/Original_position\">veil of ignorance</a> approach in which you don't know whether you are going to be incarnated as a human or a chicken) and assess only \"how good and bad the life of an individual of a given species can be relative to a human\". Bob Fischer and others have put <a href=\"https://rethinkpriorities.org/new-open-access-book-weighing-animal-welfare-edited-by-bob-fischer/\">a lot of work</a> into this. Using a range of different <a href=\"https://academic.oup.com/book/58809/chapter/488794382\">approaches and uncertainties</a>, this suggests that perhaps a chicken\u2019s welfare potential is only 3% that of a human; or maybe it\u2019s 95%; or it could well be 100%. While this is a big, uncertain and philosophically debateable range, for the purpose of illustration I will use their average value across a few methods of 40%. (This is not to say that we should be prepared to kill 1 human to save 3 chickens: only that both humans and chickens are very likely sentient and that experiencing severe pain as a chicken is not <i>too disimilar </i>from experiencing severe pain as a human, i.e. it sucks.)</li><li>(Hard!) How bad is a particular experience compared to full health, and how much can public or private policy change that? Can policy take animals from 'half health' to full health or well-being (0.5 QALYs/year), or is the potential change bigger or smaller than that? While this is hard to gauge, such assessments are made for human injuries and the examples below hopefully give a sense that making plausible estimates is possible.</li></ol><p>In the discussion section I look at some of the extra-tricky questions that I've skipped over here, but for now let's look at what numbers this method gives us if we try to value farm animal welfare improvements or harms. And bear in mind that if you think any of the values I've chosen as inputs are too high or too low, you can of course scale down/up the results accordingly.</p><h2>An impact assessment example</h2><p>In the UK, official policy impact assessments are common but \u2013 even when the policy is about animal welfare \u2013 the costs and benefits for other animals are not quantified. As an example, the relevant government department has done <a href=\"https://consult.defra.gov.uk/transforming-farm-animal-health-and-welfare-team/consultation-on-fairer-food-labelling/supporting_documents/Fairer%20Food%20Labelling%20Impact%20Asessment%20%20PUBLIC.pdf\">an impact assessment</a> of the costs and benefits of adding mandatory animal welfare labels to some chicken, egg and pig products. (This is a very good policy proposal and I <a href=\"https://bsky.app/profile/adamcorlett.bsky.social/post/3lebvz5obzc2c\">hope</a> the new UK government is not going to kill it off, having vaguely stated that they \"will introduce the biggest boost for animal welfare in a generation\".)</p><p>In this example: the assessment suggests that reforming labels will come with one-off transitional costs for businesses of around \u00a312 million (in 2025-26 terms); annual compliance costs of around \u00a34 million; a negative \u00a320 million per year impact from increased greenhouse gas emissions associated with higher chicken welfare; and a positive \u00a353 million per year gain for British farmers (at the expense of lower-welfare imports). Don't worry about whether these numbers are good guesses or not, but the example shows the kinds of things that are valued, and some typical numbers. Overall, these modelled benefits already exceed the modelled costs of the policy. But, as noted, the potential impact on animal welfare (the whole point of the policy!) is not quantified in this official assessment.</p><p>But armed with the 4-number approach above we can have a go! The impact assessment estimates that 1 in 10 UK broilers (plus a smaller fraction of laying hens) might move into higher welfare categories as a result of greater transparency for consumers, which <a href=\"https://www.gov.uk/government/statistics/livestock-populations-in-the-united-kingdom/livestock-populations-in-the-united-kingdom-at-1-june-2024\">equates</a> to around 12 million at each point in time. With a value for human QALYs and a chosen relative welfare potential figure for chickens, we therefore only need to know by how much those chickens' lives will be improved.</p><p>To be brief, I think the life of a typical broiler (meat) chicken is&nbsp;extremely&nbsp;bad. Instead of living in the jungle, they are in barns of maybe 50,000 other chickens, with few features. Lights may be on for 20 hours a day. They never see their parents or any other older bird. If they are not given enough food (as will be the case for breeders and before slaughter), they will be hungry. If they have access to food they will grow enormous. Stress on their hearts and lungs causes a range of health problems. The skeleton struggles to keep up with this fast growth, causing bone deformities, fractures and infections: 90% of birds have definite gait defects. Birds will increasingly spend their time just sitting, contributing to ulcers and lesions. As their litter is never changed over their 40-day lives, the ground becomes increasingly dominated by excrement, leading to a build-up of ammonia that <a href=\"https://www.bbc.co.uk/news/science-environment-68406398\">burns the skin</a>. The birds may be pecked by others, while the presence of dead birds will be common. Then one day they are stuffed into crates and transported to the abbatoir where they will be grabbed by their legs and hung upside down from their possibly-broken feet, unable to breath properly. If they are lucky, a blade will quickly slit their throat. If not they may be boiled alive. (Or if mass culling is required, they may be fatally overheated or suffocated in foam.)</p><p>Even without radical change, this quality of life can be improved! The <a href=\"https://welfarefootprint.org/broilers\">Welfare Footprint Project</a> helps give some numbers here. Remarkably, while a chicken could naturally live for 7+ years, a typical broiler's entire life (including sleep) is now under 1,000 hours. With that rough point of comparison, the '<a href=\"https://betterchickencommitment.com/uk/\">Better Chicken Commitment</a>' can deliver an estimated reduction of at least 33 hours of Disabling pain, 79 hours of Hurtful pain and 25 seconds of Excruciating pain: \"a reduction of approximately 66%, 24% and 78% , respectively\".</p><p>As a rough but conservative guess, then, let's say that labelling reform could push up life quality for affected chickens by 0.1 QALY/year, where 1 would be the difference between zero and full-health (though see later for a discussion of negative QALYs). In <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4901342\">other work</a> that tries to convert the 'Five-Freedoms' animal welfare framework into QALYs, I think such a 0.1 change is \u2013 under some assumptions \u2013 equivalent to going from \"very severe\" to merely \"severe\" violations in one of the five freedoms (e.g. \"freedom from pain, injury, and disease\"). &nbsp;&nbsp;</p><p>The annual chicken health welfare gain from such a labelling change then is worth \u00a389,000 * 0.4 (species factor) * 12 million chickens * 0.1 (average QALY change), and this equals... <strong>\u00a344 billion a year ($54 billion)</strong>. This is about 1000x bigger than the economic and environmental considerations considered in the existing impact assessment, and indeed is about 1.5% of UK GDP. And this is the impact just from a small change in UK food packaging. So 'how much good' could more radical&nbsp;change&nbsp;achieve, or 'how much harm' does the status quo entail?&nbsp;</p><h2><strong>What is the maximum potential welfare impact of changes to chicken farming?</strong></h2><p>Going further, <i>what if</i> we could <u>raise standards by 0.1 QALY/year</u> not just for 10% of UK broiler chickens through better labelling, but for (say) 95% of all chickens?</p><p>In the UK, that would be worth \u00a3560 billion a year \u2013 around a fifth of UK GDP.&nbsp;</p><p>In the US, where the chicken population is nearing <a href=\"https://ourworldindata.org/grapher/poultry-livestock-count?tab=chart&amp;country=OWID_WRL~USA\">2 billion</a>, the potential gain is around $7 trillion \u2013 over a fifth of US GDP.&nbsp;</p><p>With <a href=\"https://ourworldindata.org/grapher/poultry-livestock-count?tab=chart&amp;country=OWID_WRL~USA\">28 billion</a> farmed chickens alive globally at any one point, the global equivalent would be \u00a395 trillion or <strong>$118 trillion a year </strong>\u2013 similar to global GDP (although here you might argue that British or American QALY values are not affordable on a global scale)<strong>.</strong></p><p>To reiterate, that is an estimate based on raising most chickens' well-being by 0.1 <u>QALY/year on an (arguably) 0-1 scale; assuming chicken welfare changes are only worth 0.4x &nbsp;the equivalent changes for humans; and ignoring all </u><a href=\"https://forum.effectivealtruism.org/posts/6bpQTtzfZTmLaJADJ/rebutting-every-objection-to-giving-to-the-shrimp-welfare\"><u>other species</u></a><u>. If, for example, you thought you could make changes worth 1 QALY/year \u2013 as I explore below \u2013 and removed the 0.4x factor, you'd be up to </u><strong><u>$3 quadrillion a year</u></strong><u> from improved chicken welfare. On the other hand, if you were far less ambitious, or gave chickens less moral weight, maybe you could get down to a mere $1 trillion a year of beneficial impact. But a </u><a href=\"https://sites.utexas.edu/pwi/files/2021/11/AgInclusiveWelfare.pdf\"><u>separate estimate from 2021</u></a><u> (which I discovered very late when writing this), puts the animal welfare cost of a non-vegetarian American diet at $123,000 per year, which globally would add up to $100s of trillions: similar to my numbers.</u></p><p>To be clear, reducing or ending chicken suffering would not necessarily make us any richer: it is not going to be a significant boost to GDP and may even make humans worse off. But expressing the potential gains in $ terms can be one lens through which to assess the importance of the topic. And these rough results show (for those who need more persuading) that chicken welfare is extremely important! Few policy decisions are going to matter as much: to pick a couple of big ones for comparison, trade policy decisions might make countries or humanity a few % of GDP better off (or worse off), while <u>\u2013</u> at the extreme <u>\u2013</u> open borders might in economic theory add around 100% to global GDP (\"<a href=\"https://www.aeaweb.org/articles?id=10.1257/jep.25.3.83\">Trillion-Dollar Bills on the Sidewalk?</a>\").</p><p>We can also compare global chicken welfare changes (e.g. the $118 trillion for marginal improvements) to the global value of the poultry industry <u>\u2013</u> around $400 billion <u>\u2013</u> to quantify why we should prioritise welfare concerns over the industry's concerns. This is a similar conclusion (and through similar methods) to work calculating what '<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4901342\">animal welfare levies</a>' might be appropriate: which suggests that a levy of up to \u20ac3,000/kg on chicken might be justified, which is again saying that factory chicken farming does vastly more harm than good. And this also matches the non-quantitative view that this suffering is simply very bad and worth making radical change to prevent.</p><h2>Discussion / a few possible points of disagreement</h2><p>There are a lot of big questions that can be asked about these calculations and some of the assumptions I've skirted around, and it is worth briefly pointing to a few of them.</p><h3>Welfare potential values for different species&nbsp;</h3><p>There is a whole <a href=\"https://academic.oup.com/book/58809?login=false\">free book on comparing well-being across species</a>. A basic consideration is whether (in this case) chickens are sentient at all, but this project's median probability was an 89% chance. And in the specific UK context of potentially including animal welfare in impact assessments, this question has perhaps already been settled as the Animal Sentience Act states that all vertebrates are sentient beings. Conditional on sentience, that project suggests the relevant value for chickens is in the range of 4% to 100%, depending on what matters for welfare comparisons. Again, this is a big range and there is lots more scientific and philosophical work to be done here, but you could choose 4% rather than 40% and still come to the same basic conclusion that animal welfare concerns dominate.</p><p>Thought experiments may also be able to help here. For example, if you came across a body-swapping machine, would you rather be a person with a newly broken leg for 1 minute or a chicken with a newly broken leg for 10 minutes, or how about 100 minutes? Would it would be worse to be suffocated once as a human or ten times as a chicken? Again, it seems extreme to weight moment-to-moment chicken welfare at less than 1% of a human's. &nbsp;</p><h3>Questioning the use of human-based QALY values&nbsp;</h3><p>While I've included a cross-species adjustment (40% for chickens), an initial reaction might be \"with an \u00a389,000 human QALY and saying chickens have 40% of the welfare potential of a human, you're implying that regulators should value a year of full-health chicken life at \u00a336,000: that seems... high... The value of a QALY has been determined based on human valuations, but no-one one has asked people how much they value a chicken life. What gives?\" Here are 6 jumbled points in response:&nbsp;</p><ol><li>Yes, this approach is deliberately non-speciesist and utilitarian. It's not meant to be how much humans are willing to pay for animal welfare. Rather, as noted above, it is what a perfectly altruistic utilitarian might want to pay, or (equivalently) someone behind the veil of ignorance who didn't know what species they were going to be a part of.</li><li><a href=\"https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/article/monetizing-animal-welfare-impacts-for-benefitcost-analysis/4558DB0D80DA18D4DC25788E967C0ACD\">This paper</a> discusses one way in which using human QALY numbers is <i>conservative</i>. These may be designed to reflect the value of perfect health relative to death, but not to reflect total well-being: only the health component of that. If we are talking about changing farmed animals' well-being in a broader sense than health, the values we should use should perhaps be even higher. On the other hand, I wonder: even if <a href=\"https://bmchealthservres.biomedcentral.com/articles/10.1186/1472-6963-14-287\">willingness-to-pay</a> for a QALY is driven by selfish individual rather than society-wide valuations, perhaps people value their health for a broader set of reasons such as ability to work, ability to socialise and do activities, and wanting not to be a burden on others. And at least some QALY valuations seem to be based on the value of a year of life overall, which is about more than avoiding ill-health.</li><li>We know that many pet owners are willing to spend thousands of dollars on surgery or health insurance for their animals. And while poll responses should be taken with a grain of salt, <a href=\"https://yougov.co.uk/topics/society/survey-results/daily/2021/08/27/efc5e/1\">43% of Brits</a> say that animal lives are worth the same as (or more than) a human life!</li><li>QALYs may be driven by real-world affordability. Even if this line of calculation suggested a spider's health was worth \u00a34,000 a year, would we commit to spending that to help each wild spider?: no, even rich countries could not afford that, <i>suggesting</i> that (while we may be far richer and more capable in the future) here in 2025, &nbsp;animal QALY values should be lower than I've suggested or perhaps even that a firm utilitarian should put a lower value on human QALYs, given the need to consider non-human QALYs too.</li><li>The UK values a QALY at \u00a389,000, yet its National Health Service can provide a marginal QALY for <a href=\"https://assets.publishing.service.gov.uk/media/6712445e8a62ffa8df77b36e/Impact_assessment_update_to_the_statutory_scheme_to_control_the_cost_of_branded_health_service_medicines_August_2024.pdf#page=48\">\u00a315,000</a> (in 2016 prices). So, shouldn't the UK spend far more on healthcare? Arguably yes! But my point is that there may be a distinction (in practice) between valuing something at \u00a3X/outcome on paper and actually having a policy of being willing to spend \u00a3X/outcome. So, perhaps we wouldn't actively spend \u00a336,000 for a chicken QALY, but perhaps we should still be \u00a336,000-worth of happy when an extra chicken QALY is delivered.</li><li>Above all (and trying to avoid discounting chickens twice <u>\u2013</u> through both the QALY value and the species adjustment factor), note that even with much lower QALY values the conclusions would hold: that farm animal welfare will <u>\u2013</u> if it is included in considerations <u>\u2013</u> tend to absolutely dominate the costs of making changes to food and farming practices.</li></ol><h3>Population ethics and states worse than death</h3><p>As '<a href=\"https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/article/monetizing-animal-welfare-impacts-for-benefitcost-analysis/4558DB0D80DA18D4DC25788E967C0ACD\">Monetizing Animal Welfare Impacts for Benefit\u2013Cost Analysis</a>' discusses, it's one thing to use QALYs to value states of health, but it's trickier to know how to proceed if you need to compare not just better or worse lives but also non-existence. i.e. Ignoring effects on humans, it would superficially be better for the 28 billion chickens if we made their lives as good as possible, but would it be better if those standards were so expensive that in reality their population were reduced to 1 billion? This is the classic 'logic of the larder' argument for eating meat. I think there are very strong counterarguments \u2013 see '<a href=\"https://link.springer.com/article/10.1007/s10806-005-1805-x\">the illogic of the larder</a>' \u2013 including the fact that farm animals are not merely wished into existence: in reality the crops required to feed chickens mean that we are really arguing about is what usage of land is optimal, bearing in mind the welfare of humans, human-reared animals and wild animals. This is a tricky topic in all sorts of ways. It is something for animal welfare valuations to grapple with, but it's not clear that it undermines any of the above.</p><p>A related question is how to treat states that are worse than death. In the examples above I have discussed raising chicken standards by 0.1 on a 0 to 1 scale, or even an apparent extreme of going from 0 to 1. Human QALY scores do usually range from 0 to 1, ranging from death-equivalent to perfect health. But in principle at least they can be negative, for 'states worse than death' and you can see how that might make sense for extreme suffering, e.g. being tortured. This consideration adds further complexity to quantifying relevant welfare changes. But it also makes it even easier to achieve changes of +0.1QALY/year or +1QALY/year, if existing chicken welfare ranges from some negative number up to 1, rather than only 0 to 1. And in cases where lives are not worth living, there is broader philosophical agreement that it is better not to bring those lives into existence.</p><p>This is another case where thought experiments might be useful. Imagine you're age 80 and you're all set to live for another 20 years in perfect health before dying at age 100. Then the devil comes along and gives you the chance to live an extra year at the end, but as a series of typical broiler chickens (suspending disbelief that this transfer of the self can meaningfully happen). Having done all the research about broiler lives you can, do you accept this bonus year on earth? I don't think I would: those lives have negative value. Indeed, I am so concerned about these lives being torture that \u2013 if the devil forced me to make a choice \u2013 I would agree to cut short my human life in order to avoid that year of chickenhood (and let's ignore family ties here). Perhaps I wouldn't sacrifice <i>20 years</i> of human life to avoid the 1 year of torture-adjacent chickenhood, but I'd quite possibly sacrifice <i>3 years</i> \u2013 once you really think about how bad broiler stress, disease and death might be. This might inform calculations, e.g. if we mostly eliminate chicken farming and ending their existence takes them from -3 to 0 on the QALY scale, my earlier calculations could rise further to $9 quadrillion of prevented suffering per year. &nbsp;</p><h2>Conclusion</h2><p>It may seem silly to put a $ number on the potential worth of reducing chicken suffering worldwide. That calculation itself doesn't directly achieve anything. But given the cultural and economic dominance of animal agriculture, it's worth exploring every possible way of demonstrating and presenting the broad idea that \"the treatment of domesticated animals in industrial farms is <a href=\"https://www.theguardian.com/books/2015/sep/25/industrial-farming-one-worst-crimes-history-ethical-question\">perhaps the worst crime in history</a>\". Maybe some people used to thinking in monetary terms will be swayed by the numbers on how many non-human-but-sentient beings are under our control, how awful their lives are, and how (to a much greater degree than with human welfare) we could change that at almost the stroke of a pen. Farm animal welfare is not a minor consideration to come back to after we've nailed all other problems in the world: compared to at least most other issues, it is a giant, yet one that can be tackled relatively quickly and cost-effectively.</p><p>And, to get back to the more practical use of these calculations, hopefully this post highlights the possibility of including non-human animal welfare in official regulatory analyses and perhaps any similar corporate accounting, just as the costs of emissions are increasingly valued. Bearing in mind that cost-benefit analyses will always include fairly-ballpark estimates, this blog has shown that it's possible to have a go. If government departments did this, it wouldn't be surprising if they chose input values that (by design) gave outputs 100x or 1000x smaller, but that may still be enough to sway some analyses. More importantly it ensures civil servants working on animal agriculture don't spend all of their time thinking about the impacts on humans and none on the animals; and any transparent methodology would at least show people what assumptions and trade-offs have been made.</p><p>Thanks for reading! Comments welcome! In particular: if you did want to value animal welfare changes within cost-benefit calculations, how could the approach or numbers used here be improved?</p>",
    "base_score": 49,
    "comment_count": 2,
    "posted_at": "2025-02-09T22:23:36.431000Z",
    "created_at": "2025-08-21T21:48:55.278900Z",
    "author_id": "9MGHsDHKimksoFxL3",
    "author_display_name": "AdamC",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "4cYxRNGdZNWbdxPb7",
      "4eyeLKC64Yvznzt6Z",
      "ABhAuXcG6rsrSdyq5",
      "Fm9Jpm3Z9Y3aezDv7",
      "QdH9f8TC6G8oGYdgt",
      "TteDwtS2DckL4kLpT",
      "hxRMaKvwGqPb43TWB",
      "pjafFR2EwL8xAjgvB",
      "psBzwdY8ipfCeExJ7",
      "qX4dg4wEZy9sHorYi",
      "y4b45yXXrFJpPmx7d"
    ],
    "tag_names": [
      "Adjusted life year",
      "Animal sentience",
      "Animal welfare",
      "Cause prioritization",
      "Cost-benefit analysis",
      "Dietary change",
      "Farmed animal welfare",
      "Moral weight",
      "Opinion",
      "Philosophy",
      "Research"
    ],
    "markdown_content": "How important is animal welfare as an issue? And how should it be traded against other concerns (if we must)?\n\nWork over the last few years has begun to [highlight](https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/article/regulators-should-value-nonhuman-animals/45FDAE481414E32463E396F0A50C2E5C) the case for quantifying animal welfare impacts in dollar terms \u2013 for the purpose of comparing different causes and costs and benefits \u2013 and arguably made progress with the necessary foundations to do that, including assessing how we should weight the experience of different animals. This post summarises some of that work, particularly '[Monetizing Animal Welfare Impacts for Benefit\u2013Cost Analysis](https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/article/monetizing-animal-welfare-impacts-for-benefitcost-analysis/4558DB0D80DA18D4DC25788E967C0ACD)' by [@Bob Fischer](https://forum.effectivealtruism.org/users/bob-fischer?mention=user) and others, and tries to impatiently rush to one end of this process by putting some $ numbers on animal suffering changes. Hopefully this post helps provide a useful walk-through of *some* of the concepts, makes the case for further work, and demonstrates why some people think farm animal welfare is not just a 'nice-to-have' consideration but one of the most important topics in the world. (I\u2019d like to do more on [animal welfare economics](https://sentientmedia.org/animal-welfare-economics/), and this is my first foray, so feedback would be very welcome! Apologies to the real experts for any butchering, so to speak.)\n\n## **Quantifying health changes**\n\nOur goal here is to put a $ or \u00a3 value on changes in animal welfare \u2013 and I'll be focusing here on [farmed chickens](https://ourworldindata.org/data-insights/billions-of-chickens-ducks-and-pigs-are-slaughtered-for-meat-every-year) given how numerous our dinosaur cousins are. The idea is that quantifying animal welfare, sharing a common unit with many other areas of life and public policy, can help people gauge the relative importance of the issue, and to include these changes in cost-benefit analyses, alongside many other considerations which are already included, such as impacts on incomes, health and global warming. (The inclusion of estimated climate impacts is a good example of how it may be worthwhile to put a value on some things that were previously excluded from such analysis, but also creates significant risks where emissions and animal welfare considerations are in conflict but only the former is included.)\n\nTo begin, it\u2019s worth giving a human example. UK Treasury [guidance](https://www.gov.uk/government/publications/the-green-book-appraisal-and-evaluation-in-central-government/the-green-book-2020) says that the societal value of a \u2018Quality-Adjusted Life Year\u2019 is \u00a370,000 in 2020-21 prices, so around \u00a389,000 in 2025-26. Extending someone\u2019s life in full health by a year may then, on paper, be valued at \u00a389,000. Going from half-health to full-health for two years is also worth \u00a389,000 ('0.5' health-improvement \\* 2 years = 1 QALY). Doing this for 1,000 people would be worth \u00a389,000,000. (This kind of valuation is standard practice, even if it sounds funky, and can be based on the public's actual views about potential income versus health trade-offs.)\n\n['Monetizing animal welfare...'](https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/article/monetizing-animal-welfare-impacts-for-benefitcost-analysis/4558DB0D80DA18D4DC25788E967C0ACD) makes the case that we can and should do similar calculations for other species, and to do this we may 'only' need 4 numbers that can then be multiplied to quantify the scale of an impact.\n\n1. ('Easy') The value of a human QALY. Standards for valuing human QALYs will vary over time and by country and are of course debateable, but we don't need to reinvent them here. As a Brit, I'll use \u00a389,000. The most common value used in US research now [appears to be $100,000](https://pmc.ncbi.nlm.nih.gov/articles/PMC10114019/) per QALY (around \u00a380,000) while for US impact assessments the [standard](https://aspe.hhs.gov/sites/default/files/documents/cd2a1348ea0777b1aa918089e4965b8c/standard-ria-values.pdf) might be $591,000. (I think \u00a389,000 makes sense as an order of magnitude: for me specifically, an extra year of healthy life would definitely be worth more than \u00a39,000 of my money, but \u00a3900,000 would be unaffordable.)\n2. (Easy) The number of animals involved.\n3. (Hard!) A factor for comparing each species\u2019s 'welfare potential' to that of humans. This could optionally include some arbitrary speciesism if you like, but for now let\u2019s try to take a utilitarian, ~~bird's~~ god's eye view (e.g. a [veil of ignorance](https://en.wikipedia.org/wiki/Original_position) approach in which you don't know whether you are going to be incarnated as a human or a chicken) and assess only \"how good and bad the life of an individual of a given species can be relative to a human\". Bob Fischer and others have put [a lot of work](https://rethinkpriorities.org/new-open-access-book-weighing-animal-welfare-edited-by-bob-fischer/) into this. Using a range of different [approaches and uncertainties](https://academic.oup.com/book/58809/chapter/488794382), this suggests that perhaps a chicken\u2019s welfare potential is only 3% that of a human; or maybe it\u2019s 95%; or it could well be 100%. While this is a big, uncertain and philosophically debateable range, for the purpose of illustration I will use their average value across a few methods of 40%. (This is not to say that we should be prepared to kill 1 human to save 3 chickens: only that both humans and chickens are very likely sentient and that experiencing severe pain as a chicken is not *too disimilar* from experiencing severe pain as a human, i.e. it sucks.)\n4. (Hard!) How bad is a particular experience compared to full health, and how much can public or private policy change that? Can policy take animals from 'half health' to full health or well-being (0.5 QALYs/year), or is the potential change bigger or smaller than that? While this is hard to gauge, such assessments are made for human injuries and the examples below hopefully give a sense that making plausible estimates is possible.\n\nIn the discussion section I look at some of the extra-tricky questions that I've skipped over here, but for now let's look at what numbers this method gives us if we try to value farm animal welfare improvements or harms. And bear in mind that if you think any of the values I've chosen as inputs are too high or too low, you can of course scale down/up the results accordingly.\n\n## An impact assessment example\n\nIn the UK, official policy impact assessments are common but \u2013 even when the policy is about animal welfare \u2013 the costs and benefits for other animals are not quantified. As an example, the relevant government department has done [an impact assessment](https://consult.defra.gov.uk/transforming-farm-animal-health-and-welfare-team/consultation-on-fairer-food-labelling/supporting_documents/Fairer%20Food%20Labelling%20Impact%20Asessment%20%20PUBLIC.pdf) of the costs and benefits of adding mandatory animal welfare labels to some chicken, egg and pig products. (This is a very good policy proposal and I [hope](https://bsky.app/profile/adamcorlett.bsky.social/post/3lebvz5obzc2c) the new UK government is not going to kill it off, having vaguely stated that they \"will introduce the biggest boost for animal welfare in a generation\".)\n\nIn this example: the assessment suggests that reforming labels will come with one-off transitional costs for businesses of around \u00a312 million (in 2025-26 terms); annual compliance costs of around \u00a34 million; a negative \u00a320 million per year impact from increased greenhouse gas emissions associated with higher chicken welfare; and a positive \u00a353 million per year gain for British farmers (at the expense of lower-welfare imports). Don't worry about whether these numbers are good guesses or not, but the example shows the kinds of things that are valued, and some typical numbers. Overall, these modelled benefits already exceed the modelled costs of the policy. But, as noted, the potential impact on animal welfare (the whole point of the policy!) is not quantified in this official assessment.\n\nBut armed with the 4-number approach above we can have a go! The impact assessment estimates that 1 in 10 UK broilers (plus a smaller fraction of laying hens) might move into higher welfare categories as a result of greater transparency for consumers, which [equates](https://www.gov.uk/government/statistics/livestock-populations-in-the-united-kingdom/livestock-populations-in-the-united-kingdom-at-1-june-2024) to around 12 million at each point in time. With a value for human QALYs and a chosen relative welfare potential figure for chickens, we therefore only need to know by how much those chickens' lives will be improved.\n\nTo be brief, I think the life of a typical broiler (meat) chicken is\u00a0extremely\u00a0bad. Instead of living in the jungle, they are in barns of maybe 50,000 other chickens, with few features. Lights may be on for 20 hours a day. They never see their parents or any other older bird. If they are not given enough food (as will be the case for breeders and before slaughter), they will be hungry. If they have access to food they will grow enormous. Stress on their hearts and lungs causes a range of health problems. The skeleton struggles to keep up with this fast growth, causing bone deformities, fractures and infections: 90% of birds have definite gait defects. Birds will increasingly spend their time just sitting, contributing to ulcers and lesions. As their litter is never changed over their 40-day lives, the ground becomes increasingly dominated by excrement, leading to a build-up of ammonia that [burns the skin](https://www.bbc.co.uk/news/science-environment-68406398). The birds may be pecked by others, while the presence of dead birds will be common. Then one day they are stuffed into crates and transported to the abbatoir where they will be grabbed by their legs and hung upside down from their possibly-broken feet, unable to breath properly. If they are lucky, a blade will quickly slit their throat. If not they may be boiled alive. (Or if mass culling is required, they may be fatally overheated or suffocated in foam.)\n\nEven without radical change, this quality of life can be improved! The [Welfare Footprint Project](https://welfarefootprint.org/broilers) helps give some numbers here. Remarkably, while a chicken could naturally live for 7+ years, a typical broiler's entire life (including sleep) is now under 1,000 hours. With that rough point of comparison, the '[Better Chicken Commitment](https://betterchickencommitment.com/uk/)' can deliver an estimated reduction of at least 33 hours of Disabling pain, 79 hours of Hurtful pain and 25 seconds of Excruciating pain: \"a reduction of approximately 66%, 24% and 78% , respectively\".\n\nAs a rough but conservative guess, then, let's say that labelling reform could push up life quality for affected chickens by 0.1 QALY/year, where 1 would be the difference between zero and full-health (though see later for a discussion of negative QALYs). In [other work](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4901342) that tries to convert the 'Five-Freedoms' animal welfare framework into QALYs, I think such a 0.1 change is \u2013 under some assumptions \u2013 equivalent to going from \"very severe\" to merely \"severe\" violations in one of the five freedoms (e.g. \"freedom from pain, injury, and disease\").\n\nThe annual chicken health welfare gain from such a labelling change then is worth \u00a389,000 \\* 0.4 (species factor) \\* 12 million chickens \\* 0.1 (average QALY change), and this equals... **\u00a344 billion a year ($54 billion)**. This is about 1000x bigger than the economic and environmental considerations considered in the existing impact assessment, and indeed is about 1.5% of UK GDP. And this is the impact just from a small change in UK food packaging. So 'how much good' could more radical\u00a0change\u00a0achieve, or 'how much harm' does the status quo entail?\n\n## **What is the maximum potential welfare impact of changes to chicken farming?**\n\nGoing further, *what if* we could raise standards by 0.1 QALY/year not just for 10% of UK broiler chickens through better labelling, but for (say) 95% of all chickens?\n\nIn the UK, that would be worth \u00a3560 billion a year \u2013 around a fifth of UK GDP.\n\nIn the US, where the chicken population is nearing [2 billion](https://ourworldindata.org/grapher/poultry-livestock-count?tab=chart&country=OWID_WRL~USA), the potential gain is around $7 trillion \u2013 over a fifth of US GDP.\n\nWith [28 billion](https://ourworldindata.org/grapher/poultry-livestock-count?tab=chart&country=OWID_WRL~USA) farmed chickens alive globally at any one point, the global equivalent would be \u00a395 trillion or **$118 trillion a year** \u2013 similar to global GDP (although here you might argue that British or American QALY values are not affordable on a global scale)**.**\n\nTo reiterate, that is an estimate based on raising most chickens' well-being by 0.1 QALY/year on an (arguably) 0-1 scale; assuming chicken welfare changes are only worth 0.4x \u00a0the equivalent changes for humans; and ignoring all [other species](https://forum.effectivealtruism.org/posts/6bpQTtzfZTmLaJADJ/rebutting-every-objection-to-giving-to-the-shrimp-welfare). If, for example, you thought you could make changes worth 1 QALY/year \u2013 as I explore below \u2013 and removed the 0.4x factor, you'd be up to **$3 quadrillion a year** from improved chicken welfare. On the other hand, if you were far less ambitious, or gave chickens less moral weight, maybe you could get down to a mere $1 trillion a year of beneficial impact. But a [separate estimate from 2021](https://sites.utexas.edu/pwi/files/2021/11/AgInclusiveWelfare.pdf) (which I discovered very late when writing this), puts the animal welfare cost of a non-vegetarian American diet at $123,000 per year, which globally would add up to $100s of trillions: similar to my numbers.\n\nTo be clear, reducing or ending chicken suffering would not necessarily make us any richer: it is not going to be a significant boost to GDP and may even make humans worse off. But expressing the potential gains in $ terms can be one lens through which to assess the importance of the topic. And these rough results show (for those who need more persuading) that chicken welfare is extremely important! Few policy decisions are going to matter as much: to pick a couple of big ones for comparison, trade policy decisions might make countries or humanity a few % of GDP better off (or worse off), while \u2013 at the extreme \u2013 open borders might in economic theory add around 100% to global GDP (\"[Trillion-Dollar Bills on the Sidewalk?](https://www.aeaweb.org/articles?id=10.1257/jep.25.3.83)\").\n\nWe can also compare global chicken welfare changes (e.g. the $118 trillion for marginal improvements) to the global value of the poultry industry \u2013 around $400 billion \u2013 to quantify why we should prioritise welfare concerns over the industry's concerns. This is a similar conclusion (and through similar methods) to work calculating what '[animal welfare levies](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4901342)' might be appropriate: which suggests that a levy of up to \u20ac3,000/kg on chicken might be justified, which is again saying that factory chicken farming does vastly more harm than good. And this also matches the non-quantitative view that this suffering is simply very bad and worth making radical change to prevent.\n\n## Discussion / a few possible points of disagreement\n\nThere are a lot of big questions that can be asked about these calculations and some of the assumptions I've skirted around, and it is worth briefly pointing to a few of them.\n\n### Welfare potential values for different species\n\nThere is a whole [free book on comparing well-being across species](https://academic.oup.com/book/58809?login=false). A basic consideration is whether (in this case) chickens are sentient at all, but this project's median probability was an 89% chance. And in the specific UK context of potentially including animal welfare in impact assessments, this question has perhaps already been settled as the Animal Sentience Act states that all vertebrates are sentient beings. Conditional on sentience, that project suggests the relevant value for chickens is in the range of 4% to 100%, depending on what matters for welfare comparisons. Again, this is a big range and there is lots more scientific and philosophical work to be done here, but you could choose 4% rather than 40% and still come to the same basic conclusion that animal welfare concerns dominate.\n\nThought experiments may also be able to help here. For example, if you came across a body-swapping machine, would you rather be a person with a newly broken leg for 1 minute or a chicken with a newly broken leg for 10 minutes, or how about 100 minutes? Would it would be worse to be suffocated once as a human or ten times as a chicken? Again, it seems extreme to weight moment-to-moment chicken welfare at less than 1% of a human's.\n\n### Questioning the use of human-based QALY values\n\nWhile I've included a cross-species adjustment (40% for chickens), an initial reaction might be \"with an \u00a389,000 human QALY and saying chickens have 40% of the welfare potential of a human, you're implying that regulators should value a year of full-health chicken life at \u00a336,000: that seems... high... The value of a QALY has been determined based on human valuations, but no-one one has asked people how much they value a chicken life. What gives?\" Here are 6 jumbled points in response:\n\n1. Yes, this approach is deliberately non-speciesist and utilitarian. It's not meant to be how much humans are willing to pay for animal welfare. Rather, as noted above, it is what a perfectly altruistic utilitarian might want to pay, or (equivalently) someone behind the veil of ignorance who didn't know what species they were going to be a part of.\n2. [This paper](https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/article/monetizing-animal-welfare-impacts-for-benefitcost-analysis/4558DB0D80DA18D4DC25788E967C0ACD) discusses one way in which using human QALY numbers is *conservative*. These may be designed to reflect the value of perfect health relative to death, but not to reflect total well-being: only the health component of that. If we are talking about changing farmed animals' well-being in a broader sense than health, the values we should use should perhaps be even higher. On the other hand, I wonder: even if [willingness-to-pay](https://bmchealthservres.biomedcentral.com/articles/10.1186/1472-6963-14-287) for a QALY is driven by selfish individual rather than society-wide valuations, perhaps people value their health for a broader set of reasons such as ability to work, ability to socialise and do activities, and wanting not to be a burden on others. And at least some QALY valuations seem to be based on the value of a year of life overall, which is about more than avoiding ill-health.\n3. We know that many pet owners are willing to spend thousands of dollars on surgery or health insurance for their animals. And while poll responses should be taken with a grain of salt, [43% of Brits](https://yougov.co.uk/topics/society/survey-results/daily/2021/08/27/efc5e/1) say that animal lives are worth the same as (or more than) a human life!\n4. QALYs may be driven by real-world affordability. Even if this line of calculation suggested a spider's health was worth \u00a34,000 a year, would we commit to spending that to help each wild spider?: no, even rich countries could not afford that, *suggesting* that (while we may be far richer and more capable in the future) here in 2025, \u00a0animal QALY values should be lower than I've suggested or perhaps even that a firm utilitarian should put a lower value on human QALYs, given the need to consider non-human QALYs too.\n5. The UK values a QALY at \u00a389,000, yet its National Health Service can provide a marginal QALY for [\u00a315,000](https://assets.publishing.service.gov.uk/media/6712445e8a62ffa8df77b36e/Impact_assessment_update_to_the_statutory_scheme_to_control_the_cost_of_branded_health_service_medicines_August_2024.pdf#page=48) (in 2016 prices). So, shouldn't the UK spend far more on healthcare? Arguably yes! But my point is that there may be a distinction (in practice) between valuing something at \u00a3X/outcome on paper and actually having a policy of being willing to spend \u00a3X/outcome. So, perhaps we wouldn't actively spend \u00a336,000 for a chicken QALY, but perhaps we should still be \u00a336,000-worth of happy when an extra chicken QALY is delivered.\n6. Above all (and trying to avoid discounting chickens twice \u2013 through both the QALY value and the species adjustment factor), note that even with much lower QALY values the conclusions would hold: that farm animal welfare will \u2013 if it is included in considerations \u2013 tend to absolutely dominate the costs of making changes to food and farming practices.\n\n### Population ethics and states worse than death\n\nAs '[Monetizing Animal Welfare Impacts for Benefit\u2013Cost Analysis](https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/article/monetizing-animal-welfare-impacts-for-benefitcost-analysis/4558DB0D80DA18D4DC25788E967C0ACD)' discusses, it's one thing to use QALYs to value states of health, but it's trickier to know how to proceed if you need to compare not just better or worse lives but also non-existence. i.e. Ignoring effects on humans, it would superficially be better for the 28 billion chickens if we made their lives as good as possible, but would it be better if those standards were so expensive that in reality their population were reduced to 1 billion? This is the classic 'logic of the larder' argument for eating meat. I think there are very strong counterarguments \u2013 see '[the illogic of the larder](https://link.springer.com/article/10.1007/s10806-005-1805-x)' \u2013 including the fact that farm animals are not merely wished into existence: in reality the crops required to feed chickens mean that we are really arguing about is what usage of land is optimal, bearing in mind the welfare of humans, human-reared animals and wild animals. This is a tricky topic in all sorts of ways. It is something for animal welfare valuations to grapple with, but it's not clear that it undermines any of the above.\n\nA related question is how to treat states that are worse than death. In the examples above I have discussed raising chicken standards by 0.1 on a 0 to 1 scale, or even an apparent extreme of going from 0 to 1. Human QALY scores do usually range from 0 to 1, ranging from death-equivalent to perfect health. But in principle at least they can be negative, for 'states worse than death' and you can see how that might make sense for extreme suffering, e.g. being tortured. This consideration adds further complexity to quantifying relevant welfare changes. But it also makes it even easier to achieve changes of +0.1QALY/year or +1QALY/year, if existing chicken welfare ranges from some negative number up to 1, rather than only 0 to 1. And in cases where lives are not worth living, there is broader philosophical agreement that it is better not to bring those lives into existence.\n\nThis is another case where thought experiments might be useful. Imagine you're age 80 and you're all set to live for another 20 years in perfect health before dying at age 100. Then the devil comes along and gives you the chance to live an extra year at the end, but as a series of typical broiler chickens (suspending disbelief that this transfer of the self can meaningfully happen). Having done all the research about broiler lives you can, do you accept this bonus year on earth? I don't think I would: those lives have negative value. Indeed, I am so concerned about these lives being torture that \u2013 if the devil forced me to make a choice \u2013 I would agree to cut short my human life in order to avoid that year of chickenhood (and let's ignore family ties here). Perhaps I wouldn't sacrifice *20 years* of human life to avoid the 1 year of torture-adjacent chickenhood, but I'd quite possibly sacrifice *3 years* \u2013 once you really think about how bad broiler stress, disease and death might be. This might inform calculations, e.g. if we mostly eliminate chicken farming and ending their existence takes them from -3 to 0 on the QALY scale, my earlier calculations could rise further to $9 quadrillion of prevented suffering per year.\n\n## Conclusion\n\nIt may seem silly to put a $ number on the potential worth of reducing chicken suffering worldwide. That calculation itself doesn't directly achieve anything. But given the cultural and economic dominance of animal agriculture, it's worth exploring every possible way of demonstrating and presenting the broad idea that \"the treatment of domesticated animals in industrial farms is [perhaps the worst crime in history](https://www.theguardian.com/books/2015/sep/25/industrial-farming-one-worst-crimes-history-ethical-question)\". Maybe some people used to thinking in monetary terms will be swayed by the numbers on how many non-human-but-sentient beings are under our control, how awful their lives are, and how (to a much greater degree than with human welfare) we could change that at almost the stroke of a pen. Farm animal welfare is not a minor consideration to come back to after we've nailed all other problems in the world: compared to at least most other issues, it is a giant, yet one that can be tackled relatively quickly and cost-effectively.\n\nAnd, to get back to the more practical use of these calculations, hopefully this post highlights the possibility of including non-human animal welfare in official regulatory analyses and perhaps any similar corporate accounting, just as the costs of emissions are increasingly valued. Bearing in mind that cost-benefit analyses will always include fairly-ballpark estimates, this blog has shown that it's possible to have a go. If government departments did this, it wouldn't be surprising if they chose input values that (by design) gave outputs 100x or 1000x smaller, but that may still be enough to sway some analyses. More importantly it ensures civil servants working on animal agriculture don't spend all of their time thinking about the impacts on humans and none on the animals; and any transparent methodology would at least show people what assumptions and trade-offs have been made.\n\nThanks for reading! Comments welcome! In particular: if you did want to value animal welfare changes within cost-benefit calculations, how could the approach or numbers used here be improved?",
    "word_count": 4019,
    "reading_time_minutes": 20,
    "external_links": [
      "https://academic.oup.com/book/58809/chapter/488794382",
      "https://academic.oup.com/book/58809?login=false",
      "https://aspe.hhs.gov/sites/default/files/documents/cd2a1348ea0777b1aa918089e4965b8c/standard-ria-values.pdf",
      "https://assets.publishing.service.gov.uk/media/6712445e8a62ffa8df77b36e/Impact_assessment_update_to_the_statutory_scheme_to_control_the_cost_of_branded_health_service_medicines_August_2024.pdf#page=48",
      "https://betterchickencommitment.com/uk/",
      "https://bmchealthservres.biomedcentral.com/articles/10.1186/1472-6963-14-287",
      "https://bsky.app/profile/adamcorlett.bsky.social/post/3lebvz5obzc2c",
      "https://consult.defra.gov.uk/transforming-farm-animal-health-and-welfare-team/consultation-on-fairer-food-labelling/supporting_documents/Fairer%20Food%20Labelling%20Impact%20Asessment%20%20PUBLIC.pdf",
      "https://en.wikipedia.org/wiki/Original_position",
      "https://forum.effectivealtruism.org/posts/6bpQTtzfZTmLaJADJ/rebutting-every-objection-to-giving-to-the-shrimp-welfare",
      "https://forum.effectivealtruism.org/users/bob-fischer?mention=user",
      "https://link.springer.com/article/10.1007/s10806-005-1805-x",
      "https://ourworldindata.org/data-insights/billions-of-chickens-ducks-and-pigs-are-slaughtered-for-meat-every-year",
      "https://ourworldindata.org/grapher/poultry-livestock-count?tab=chart&country=OWID_WRL~USA",
      "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4901342",
      "https://pmc.ncbi.nlm.nih.gov/articles/PMC10114019/",
      "https://rethinkpriorities.org/new-open-access-book-weighing-animal-welfare-edited-by-bob-fischer/",
      "https://sentientmedia.org/animal-welfare-economics/",
      "https://sites.utexas.edu/pwi/files/2021/11/AgInclusiveWelfare.pdf",
      "https://welfarefootprint.org/broilers",
      "https://www.aeaweb.org/articles?id=10.1257/jep.25.3.83",
      "https://www.bbc.co.uk/news/science-environment-68406398",
      "https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/article/monetizing-animal-welfare-impacts-for-benefitcost-analysis/4558DB0D80DA18D4DC25788E967C0ACD",
      "https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/article/regulators-should-value-nonhuman-animals/45FDAE481414E32463E396F0A50C2E5C",
      "https://www.gov.uk/government/publications/the-green-book-appraisal-and-evaluation-in-central-government/the-green-book-2020",
      "https://www.gov.uk/government/statistics/livestock-populations-in-the-united-kingdom/livestock-populations-in-the-united-kingdom-at-1-june-2024",
      "https://www.theguardian.com/books/2015/sep/25/industrial-farming-one-worst-crimes-history-ethical-question",
      "https://yougov.co.uk/topics/society/survey-results/daily/2021/08/27/efc5e/1"
    ],
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T21:48:55.725478Z"
  },
  {
    "id": 1156,
    "post_id": "BRqBvkjskZ6c2G6rn",
    "title": "The Upcoming PEPFAR Cut Will Kill Millions, Many of Them Children",
    "title_normalized": "the upcoming pepfar cut will kill millions many of them children",
    "page_url": "https://forum.effectivealtruism.org/posts/BRqBvkjskZ6c2G6rn/the-upcoming-pepfar-cut-will-kill-millions-many-of-them",
    "html_body": "<p>Edit 1/29: Funding is back, baby!&nbsp;</p><p>Crossposted from <a href=\"https://benthams.substack.com/p/the-upcoming-pepfar-cut-will-kill\">my blog</a>. &nbsp;</p><p>(This could end up being the most important thing I\u2019ve ever written. Please like and restack it\u2014if you have a big blog, please write about it).</p><p>A mother holds her sick baby to her chest. She knows he doesn\u2019t have long to live. She hears him coughing\u2014those body-wracking coughs\u2014that expel mucus and phlegm, leaving him desperately gasping for air. He is just a few months old. And yet that\u2019s how old he will be when he dies.</p><p>The aforementioned scene is likely to become increasingly common in the coming years. Fortunately, there is still hope.</p><p>Trump recently signed an executive order shutting off almost all foreign aid. Most terrifyingly, this included shutting off the PEPFAR program\u2014the single most successful foreign aid program in my lifetime. PEPFAR provides treatment and prevention of HIV and AIDS\u2014it has saved about <a href=\"https://www.state.gov/results-and-impact-pepfar#:~:text=PEPFAR%20has%20enabled%205.5%20million,to%20mothers%20living%20with%20HIV.\"><u>25 million people</u></a> since its implementation in 2001, despite only taking less than 0.1% of the federal budget. Every <a href=\"https://www.amfar.org/press-releases/trump-administrations-order-requiring-immediate-suspension-of-all-hiv-services-provided-by-pepfar-will-place-lives-at-risk/\"><u>single day that it is operative</u></a>, PEPFAR supports:</p><blockquote><ul><li>More than 222,000 people on treatment in the program collecting ARVs to stay healthy;</li><li>More than 224,000 HIV tests, newly diagnosing 4,374 people with HIV \u2013 10% of whom are pregnant women attending antenatal clinic visits;</li><li>Services for 17,695 orphans and vulnerable children impacted by HIV;</li><li>7,163 cervical cancer screenings, newly diagnosing 363 women with cervical cancer or pre-cancerous lesions, and treating 324 women with positive cervical cancer results;</li><li>Care and support for 3,618 women experiencing gender-based violence, including 779 women who experienced sexual violence.</li></ul></blockquote><p>The most important thing PEPFAR does is provide life-saving anti-retroviral treatments to <a href=\"https://www.state.gov/results-and-impact-pepfar\"><u>millions of victims of HIV</u></a>. More than <a href=\"https://www.state.gov/results-and-impact-pepfar\"><u>20 million people living</u></a> with HIV globally depend on daily anti-retrovirals, including over half a million children. These children, facing a deadly illness in desperately poor countries, are now going to be left on their own, and a lot of them are going to die.</p><p>Imagine a baby you care about having to face that. Imagine them having a horrible disease, a disease that will likely kill them if untreated. But they have been getting treatment\u2014treatment that makes it so that they will not die from the disease but be able to grow and flourish. Now, with the stroke of a pen, their medicine will be cut off\u2014probably they will die in a few months. <a href=\"https://x.com/KelseyTuoc/status/1883601122524537144\"><u>Kelsey Piper writes</u></a>:</p><blockquote><p>There are five hundred thousand kids who PEPFAR provides HIV drugs that keep their immune system working. AIDS kills babies very quickly if they aren't on antivirals. Two to six months, which is also about when most babies with SCID die. It's about how long you can make it in this world without an immune system.</p><p>[Context: Kelsey\u2019s baby has a cold].</p><p>I've spent a lot of time yesterday and today in the shower - the hot steam helps the baby - cradling her to my chest while she whimpers \"why, why, why?\" and knowing that she will be completely fine. The world where this cold could have killed her feels like it is terribly nearby. The children that immunodeficiencies do kill don't feel very far away, either.</p></blockquote><p>Earlier I said PEPFAR has saved 25 million people. In fact, the 25 million people saved number is probably an underestimate. It doesn\u2019t take into account that PEPFAR <a href=\"https://www.vox.com/future-perfect/24036223/pepfar-aids-hiv-africa-global-health-george-w-bush-republicans-congress\"><u>has built up infrastructure</u></a> in poor countries that makes it easier for them to combat other diseases. As a result, the little-known PEPFAR program is probably the single most effective federal program per dollar. A temporary pause in PEPFAR means a temporary pause in providing life-saving medicine\u2014it means a lot of people will die. Many of those will be vulnerable children.</p><p>About half a million people died in the Iraq war. The PEPFAR program saved about 50 times that number of people. A program might be gutted that saved enough lives to offset the Iraq war 50 times over. The end of PEPFAR will kill, in a year, more people than died during the entire Iraq war. PEPFAR was responsible for kicking the crap out of AIDS in Africa, for making it so that the number of victims of AIDS over time looked like this:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BRqBvkjskZ6c2G6rn/i5ynmsxqtlnh6tb7maqu\" alt=\"Image\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BRqBvkjskZ6c2G6rn/fgydehsyxha8ls1x5coj 424w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BRqBvkjskZ6c2G6rn/hfrck35rcybzegoty5jc 848w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BRqBvkjskZ6c2G6rn/fwkderknnvai3k6j7ruy 1272w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BRqBvkjskZ6c2G6rn/i5ynmsxqtlnh6tb7maqu 1456w\"></figure><p>&nbsp;</p><p>Fortunately, the end of PEPFAR is not inevitable. Rubio kept up <a href=\"https://www.aljazeera.com/news/2025/1/25/us-suspends-new-funds-for-aid-programmes-excepting-israel-and-egypt\"><u>famine relief </u></a>and military aid to Morocco and Israel. With the stroke of a pen, Trump or Rubio could do the same for PEPFAR. If either Trump or Rubio wanted to, they could make sure PEPFAR relief continues. Neither has any special reason to be opposed to PEPFAR\u2014the program is traditionally bipartisan! The only reason they\u2019re cutting it is likely that they haven\u2019t heard about it.</p><p>So let\u2019s get the word out! The PEPFAR cut is unpopular; it can only thrive in the darkness. Help make it a national issue so that people know what PEPFAR does. Honestly, if Rubio just read one article about PEPFAR, I think he could be talked into keeping up the program rather than cutting it. Time is of the essence\u2014the longer PEPFAR is cut, the more people\u2014many of them children\u2014will die.</p><p>Being against kids dying isn\u2019t a partisan issue. Help make sure that no mothers have to clutch their sick babies to their chest, knowing that they have only weeks to live, because of an executive order\u2019s collateral damage. Rather than surrendering, let\u2019s win the war against this wretched disease\u2014this killer of children. No more babies need to die of this scourge, this plague that we could wipe out if we put in the effort\u2014as we\u2019ve been doing for decades. If Trump takes pivotal action on AIDS, he could be one of the only people in history to successfully eradicate a disease!</p><p>The stakes are high\u2014I cannot describe just how horrible it is that many children might die because of this, but hopefully this poem from a grieving mother can help <a href=\"https://newmercymoms.com/blog/loss-of-child-poem-collection\"><u>show some of the seriousness</u></a> of this issue. If PEPFAR is cut, she will not be the only mother to grieve a dead child.</p><p><i>In fields where once you played and ran,</i><br><i>Where first you walked, my little man,</i><br><i>I see your footprints in the sand,</i><br><i>And feel the warmth of your small hand.</i></p><p><i>Your first steps wobbly, yet so brave,</i><br><i>The joy of learning, waves you gave,</i><br><i>To ride a bike, your laughter free,</i><br><i>Each memory, now bittersweet to me.</i></p><p><i>The mud beneath the summer rain,</i><br><i>You splashed and played, forgot the pain,</i><br><i>Catching bugs in the hot sun's glow,</i><br><i>Your spirit bright, your heart aglow.</i></p><p><i>We built a snowman in the yard,</i><br><i>And shared a bond, so true, so hard,</i><br><i>But now those moments turn to dust,</i><br><i>As grief engulfs, as memories rust.</i></p><p><i>The years we had, I'm thankful for,</i><br><i>Yet sorrow knocks at every door,</i><br><i>For now your life on earth is done,</i><br><i>Your journey ceased, my precious son.</i></p><p><i>I did not see your end draw near,</i><br><i>And guilt, it whispers in my ear,</i><br><i>I should have known, I should have seen,</i><br><i>The shadows fall where light had been.</i></p><p><i>Your goodbye letter in my hand,</i><br><i>A testament to dreams unplanned,</i><br><i>I long to tell you, one last time,</i><br><i>How much you're loved, my dear, sublime.</i></p><p><i>You're needed here, your laugh, your light,</i><br><i>Your absence turns my days to night,</i><br><i>So many questions, answers few,</i><br><i>The pain so deep, I can't construe.</i></p><p>(If you want to do something to help stop PEPFAR cuts, share this article and as other sources about PEPFAR\u2019s importance as widely as you can, especially if you know someone who might be influential on this issue. Additionally, contact your local congresspeople and donate to <a href=\"https://www.givewell.org/charities/top-charities\"><u>other charities helping save children from disease</u></a>!)</p><p>&nbsp;</p><p>&nbsp;</p><p><br>&nbsp;</p>",
    "base_score": 179,
    "comment_count": 38,
    "posted_at": "2025-01-27T16:03:14.395000Z",
    "created_at": "2025-08-21T18:06:50.052517Z",
    "author_id": "bha2eNe4xCAw2kYfg",
    "author_display_name": "Bentham's Bulldog",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "L6NqHZkLc4xZ7YtDr",
      "SPiJDaKCYaEt6HNTA",
      "YJ3CNWY5Lv4aK5CW4",
      "qX4dg4wEZy9sHorYi",
      "sWcuTyTB5dP3nas2t",
      "xEnFbH3FjEzXhinGc"
    ],
    "tag_names": [
      "Adjusted life year",
      "Effective giving",
      "Foreign aid",
      "Global health & development",
      "HIV/AIDS",
      "News relevant to effective altruism"
    ],
    "markdown_content": "Edit 1/29: Funding is back, baby!\n\nCrossposted from [my blog](https://benthams.substack.com/p/the-upcoming-pepfar-cut-will-kill).\n\n(This could end up being the most important thing I\u2019ve ever written. Please like and restack it\u2014if you have a big blog, please write about it).\n\nA mother holds her sick baby to her chest. She knows he doesn\u2019t have long to live. She hears him coughing\u2014those body-wracking coughs\u2014that expel mucus and phlegm, leaving him desperately gasping for air. He is just a few months old. And yet that\u2019s how old he will be when he dies.\n\nThe aforementioned scene is likely to become increasingly common in the coming years. Fortunately, there is still hope.\n\nTrump recently signed an executive order shutting off almost all foreign aid. Most terrifyingly, this included shutting off the PEPFAR program\u2014the single most successful foreign aid program in my lifetime. PEPFAR provides treatment and prevention of HIV and AIDS\u2014it has saved about [25 million people](https://www.state.gov/results-and-impact-pepfar#:~:text=PEPFAR%20has%20enabled%205.5%20million,to%20mothers%20living%20with%20HIV.) since its implementation in 2001, despite only taking less than 0.1% of the federal budget. Every [single day that it is operative](https://www.amfar.org/press-releases/trump-administrations-order-requiring-immediate-suspension-of-all-hiv-services-provided-by-pepfar-will-place-lives-at-risk/), PEPFAR supports:\n\n> - More than 222,000 people on treatment in the program collecting ARVs to stay healthy;\n> - More than 224,000 HIV tests, newly diagnosing 4,374 people with HIV \u2013 10% of whom are pregnant women attending antenatal clinic visits;\n> - Services for 17,695 orphans and vulnerable children impacted by HIV;\n> - 7,163 cervical cancer screenings, newly diagnosing 363 women with cervical cancer or pre-cancerous lesions, and treating 324 women with positive cervical cancer results;\n> - Care and support for 3,618 women experiencing gender-based violence, including 779 women who experienced sexual violence.\n\nThe most important thing PEPFAR does is provide life-saving anti-retroviral treatments to [millions of victims of HIV](https://www.state.gov/results-and-impact-pepfar). More than [20 million people living](https://www.state.gov/results-and-impact-pepfar) with HIV globally depend on daily anti-retrovirals, including over half a million children. These children, facing a deadly illness in desperately poor countries, are now going to be left on their own, and a lot of them are going to die.\n\nImagine a baby you care about having to face that. Imagine them having a horrible disease, a disease that will likely kill them if untreated. But they have been getting treatment\u2014treatment that makes it so that they will not die from the disease but be able to grow and flourish. Now, with the stroke of a pen, their medicine will be cut off\u2014probably they will die in a few months. [Kelsey Piper writes](https://x.com/KelseyTuoc/status/1883601122524537144):\n\n> There are five hundred thousand kids who PEPFAR provides HIV drugs that keep their immune system working. AIDS kills babies very quickly if they aren't on antivirals. Two to six months, which is also about when most babies with SCID die. It's about how long you can make it in this world without an immune system.\n>\n> [Context: Kelsey\u2019s baby has a cold].\n>\n> I've spent a lot of time yesterday and today in the shower - the hot steam helps the baby - cradling her to my chest while she whimpers \"why, why, why?\" and knowing that she will be completely fine. The world where this cold could have killed her feels like it is terribly nearby. The children that immunodeficiencies do kill don't feel very far away, either.\n\nEarlier I said PEPFAR has saved 25 million people. In fact, the 25 million people saved number is probably an underestimate. It doesn\u2019t take into account that PEPFAR [has built up infrastructure](https://www.vox.com/future-perfect/24036223/pepfar-aids-hiv-africa-global-health-george-w-bush-republicans-congress) in poor countries that makes it easier for them to combat other diseases. As a result, the little-known PEPFAR program is probably the single most effective federal program per dollar. A temporary pause in PEPFAR means a temporary pause in providing life-saving medicine\u2014it means a lot of people will die. Many of those will be vulnerable children.\n\nAbout half a million people died in the Iraq war. The PEPFAR program saved about 50 times that number of people. A program might be gutted that saved enough lives to offset the Iraq war 50 times over. The end of PEPFAR will kill, in a year, more people than died during the entire Iraq war. PEPFAR was responsible for kicking the crap out of AIDS in Africa, for making it so that the number of victims of AIDS over time looked like this:\n\n![Image](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/BRqBvkjskZ6c2G6rn/i5ynmsxqtlnh6tb7maqu)\n\nFortunately, the end of PEPFAR is not inevitable. Rubio kept up [famine relief](https://www.aljazeera.com/news/2025/1/25/us-suspends-new-funds-for-aid-programmes-excepting-israel-and-egypt) and military aid to Morocco and Israel. With the stroke of a pen, Trump or Rubio could do the same for PEPFAR. If either Trump or Rubio wanted to, they could make sure PEPFAR relief continues. Neither has any special reason to be opposed to PEPFAR\u2014the program is traditionally bipartisan! The only reason they\u2019re cutting it is likely that they haven\u2019t heard about it.\n\nSo let\u2019s get the word out! The PEPFAR cut is unpopular; it can only thrive in the darkness. Help make it a national issue so that people know what PEPFAR does. Honestly, if Rubio just read one article about PEPFAR, I think he could be talked into keeping up the program rather than cutting it. Time is of the essence\u2014the longer PEPFAR is cut, the more people\u2014many of them children\u2014will die.\n\nBeing against kids dying isn\u2019t a partisan issue. Help make sure that no mothers have to clutch their sick babies to their chest, knowing that they have only weeks to live, because of an executive order\u2019s collateral damage. Rather than surrendering, let\u2019s win the war against this wretched disease\u2014this killer of children. No more babies need to die of this scourge, this plague that we could wipe out if we put in the effort\u2014as we\u2019ve been doing for decades. If Trump takes pivotal action on AIDS, he could be one of the only people in history to successfully eradicate a disease!\n\nThe stakes are high\u2014I cannot describe just how horrible it is that many children might die because of this, but hopefully this poem from a grieving mother can help [show some of the seriousness](https://newmercymoms.com/blog/loss-of-child-poem-collection) of this issue. If PEPFAR is cut, she will not be the only mother to grieve a dead child.\n\n*In fields where once you played and ran,*  \n*Where first you walked, my little man,*  \n*I see your footprints in the sand,*  \n*And feel the warmth of your small hand.*\n\n*Your first steps wobbly, yet so brave,*  \n*The joy of learning, waves you gave,*  \n*To ride a bike, your laughter free,*  \n*Each memory, now bittersweet to me.*\n\n*The mud beneath the summer rain,*  \n*You splashed and played, forgot the pain,*  \n*Catching bugs in the hot sun's glow,*  \n*Your spirit bright, your heart aglow.*\n\n*We built a snowman in the yard,*  \n*And shared a bond, so true, so hard,*  \n*But now those moments turn to dust,*  \n*As grief engulfs, as memories rust.*\n\n*The years we had, I'm thankful for,*  \n*Yet sorrow knocks at every door,*  \n*For now your life on earth is done,*  \n*Your journey ceased, my precious son.*\n\n*I did not see your end draw near,*  \n*And guilt, it whispers in my ear,*  \n*I should have known, I should have seen,*  \n*The shadows fall where light had been.*\n\n*Your goodbye letter in my hand,*  \n*A testament to dreams unplanned,*  \n*I long to tell you, one last time,*  \n*How much you're loved, my dear, sublime.*\n\n*You're needed here, your laugh, your light,*  \n*Your absence turns my days to night,*  \n*So many questions, answers few,*  \n*The pain so deep, I can't construe.*\n\n(If you want to do something to help stop PEPFAR cuts, share this article and as other sources about PEPFAR\u2019s importance as widely as you can, especially if you know someone who might be influential on this issue. Additionally, contact your local congresspeople and donate to [other charities helping save children from disease](https://www.givewell.org/charities/top-charities)!)",
    "word_count": 1288,
    "reading_time_minutes": 6,
    "external_links": [
      "https://benthams.substack.com/p/the-upcoming-pepfar-cut-will-kill",
      "https://newmercymoms.com/blog/loss-of-child-poem-collection",
      "https://www.aljazeera.com/news/2025/1/25/us-suspends-new-funds-for-aid-programmes-excepting-israel-and-egypt",
      "https://www.amfar.org/press-releases/trump-administrations-order-requiring-immediate-suspension-of-all-hiv-services-provided-by-pepfar-will-place-lives-at-risk/",
      "https://www.givewell.org/charities/top-charities",
      "https://www.state.gov/results-and-impact-pepfar",
      "https://www.state.gov/results-and-impact-pepfar#:~:text=PEPFAR%20has%20enabled%205.5%20million,to%20mothers%20living%20with%20HIV.",
      "https://www.vox.com/future-perfect/24036223/pepfar-aids-hiv-africa-global-health-george-w-bush-republicans-congress",
      "https://x.com/KelseyTuoc/status/1883601122524537144"
    ],
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T18:06:50.186850Z"
  },
  {
    "id": 1223,
    "post_id": "bTDQokAQLm8CXZAwe",
    "title": "What do you mean with \u2018alignment is solvable in principle\u2019?",
    "title_normalized": "what do you mean with alignment is solvable in principle",
    "page_url": "https://forum.effectivealtruism.org/posts/bTDQokAQLm8CXZAwe/what-do-you-mean-with-alignment-is-solvable-in-principle",
    "html_body": "<p>Typically, I saw researchers make this claim confidently in one sentence. Sometimes, it's backed by a loose analogy.&nbsp;<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"dcvvc5zwkqc\" role=\"doc-noteref\" id=\"fnrefdcvvc5zwkqc\"><sup><a href=\"#fndcvvc5zwkqc\">[1]</a></sup></span></p><p>This claim is cruxy. If alignment is <i>not</i> solvable, then the alignment community is not viable. But little is written that disambiguates and explicitly reasons through the claim.</p><p>Have you claimed that \u2018AGI alignment is solvable in principle\u2019?&nbsp;<br>If so, can you elaborate what you mean with each term?&nbsp;<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"2\" data-footnote-id=\"3zicucg26fm\" role=\"doc-noteref\" id=\"fnref3zicucg26fm\"><sup><a href=\"#fn3zicucg26fm\">[2]</a></sup></span>&nbsp;</p><p>Below I'll also try <a href=\"https://www.lesswrong.com/posts/hTMrv2WJ59xaJhmmA/what-do-you-mean-with-alignment-is-solvable-in-principle?commentId=qccjaoKBfj7RevsyR\">specify each term</a>, since I support research here by Sandberg &amp; co.</p><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"dcvvc5zwkqc\" role=\"doc-endnote\" id=\"fndcvvc5zwkqc\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"dcvvc5zwkqc\"><sup><strong><a href=\"#fnrefdcvvc5zwkqc\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Some analogies I've seen a few times (rough paraphrases):</p><ul><li>\u2018humans are generally intelligent too, and humans can align with humans\u2019</li><li>'LLMs appear to do a lot of what we want them to do, so AGI could too'</li><li>\u2018other impossible-seeming engineering problems got solved too\u2019</li></ul></div></li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"2\" data-footnote-id=\"3zicucg26fm\" role=\"doc-endnote\" id=\"fn3zicucg26fm\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"3zicucg26fm\"><sup><strong><a href=\"#fnref3zicucg26fm\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>E.g. what does \u2018in principle\u2019 mean? Does it assert that the problem described is solvable based on certain principles, or some model of how the world works?</p></div></li></ol>",
    "base_score": 10,
    "comment_count": 1,
    "posted_at": "2025-01-17T15:03:12.258000Z",
    "created_at": "2025-08-21T20:53:20.599714Z",
    "author_id": "Sv79uKDAQcnrioJmo",
    "author_display_name": "Remmelt",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "4eyeLKC64Yvznzt6Z",
      "9GQf4Ec6ckqvnPBSw",
      "AtDxSNDDXPRa5kxjB",
      "oNiQsBHA3i837sySD"
    ],
    "tag_names": [
      "AI alignment",
      "AI safety",
      "Artificial intelligence",
      "Philosophy"
    ],
    "markdown_content": "Typically, I saw researchers make this claim confidently in one sentence. Sometimes, it's backed by a loose analogy.\u00a0[[1]](#fndcvvc5zwkqc)\n\nThis claim is cruxy. If alignment is *not* solvable, then the alignment community is not viable. But little is written that disambiguates and explicitly reasons through the claim.\n\nHave you claimed that \u2018AGI alignment is solvable in principle\u2019?\u00a0  \nIf so, can you elaborate what you mean with each term?\u00a0[[2]](#fn3zicucg26fm)\n\nBelow I'll also try [specify each term](https://www.lesswrong.com/posts/hTMrv2WJ59xaJhmmA/what-do-you-mean-with-alignment-is-solvable-in-principle?commentId=qccjaoKBfj7RevsyR), since I support research here by Sandberg & co.\n\n1. **[^](#fnrefdcvvc5zwkqc)**\n\n   Some analogies I've seen a few times (rough paraphrases):\n\n   - \u2018humans are generally intelligent too, and humans can align with humans\u2019\n   - 'LLMs appear to do a lot of what we want them to do, so AGI could too'\n   - \u2018other impossible-seeming engineering problems got solved too\u2019\n2. **[^](#fnref3zicucg26fm)**\n\n   E.g. what does \u2018in principle\u2019 mean? Does it assert that the problem described is solvable based on certain principles, or some model of how the world works?",
    "word_count": 163,
    "reading_time_minutes": 1,
    "external_links": [
      "https://www.lesswrong.com/posts/hTMrv2WJ59xaJhmmA/what-do-you-mean-with-alignment-is-solvable-in-principle?commentId=qccjaoKBfj7RevsyR"
    ],
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T20:53:20.967223Z"
  },
  {
    "id": 1527,
    "post_id": "CLDgqhY43bkckWzWJ",
    "title": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs",
    "title_normalized": "utility engineering analyzing and controlling emergent value systems in ais",
    "page_url": "https://forum.effectivealtruism.org/posts/CLDgqhY43bkckWzWJ/utility-engineering-analyzing-and-controlling-emergent-value",
    "html_body": "<blockquote><p>As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations.</p></blockquote>",
    "base_score": 13,
    "comment_count": 0,
    "posted_at": "2025-02-12T09:15:07.628000Z",
    "created_at": "2025-08-21T21:45:35.199016Z",
    "author_id": "KFkknyEkkpo5HXjg8",
    "author_display_name": "Matrice Jacobine",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "9GQf4Ec6ckqvnPBSw",
      "hxRMaKvwGqPb43TWB",
      "oNiQsBHA3i837sySD",
      "wqstYnFPgdekKGLPc"
    ],
    "tag_names": [
      "AI alignment",
      "AI safety",
      "Large Language Models",
      "Research"
    ],
    "markdown_content": "> As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations.",
    "word_count": 228,
    "reading_time_minutes": 1,
    "external_links": null,
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T21:45:35.645267Z"
  },
  {
    "id": 185,
    "post_id": "fpn6MZTShaGLZqopo",
    "title": "EA Survey 2024: Geography",
    "title_normalized": "ea survey 2024 geography",
    "page_url": "https://forum.effectivealtruism.org/posts/fpn6MZTShaGLZqopo/ea-survey-2024-geography",
    "html_body": "<h1>Summary</h1><ul><li>Countries<ul><li>The USA (34.4%) and the UK (13.5%) remain the countries with the largest proportions of EA respondents. We observed that 32% of respondents came from Europe (excluding the UK), while 20% came from the rest of the world. In previous years, we observed a decline in the percentage of respondents from these countries. This trend seems like it might have decreased or plateaued since 2022.</li><li>This year, for the first time, we looked at EAs\u2019 countries of origin as well as their current countries of residence. In absolute terms, we see large flows into the USA and UK, but Switzerland shows by far the highest relative level of in-flows, with 62% of respondents having a different country of origin.</li></ul></li><li>Cities<ul><li>This year, London was the city with the most respondents (6.7%), followed by the SF Bay Area (5.7%), DC (3.8%) and NYC (3.3%). Interestingly, our results show that a majority of EAs live outside of major hubs.</li></ul></li><li>Differences across countries<ul><li>We find that respondents from the US were substantially less likely to be students (15.5%) than those from other regions (23.4% in the UK, 28.2% in Europe).</li><li>The UK has significantly higher engagement levels (41% highly engaged) compared to Europe (28%), the USA (24%), and the rest of the world (25%).</li><li>Satisfaction with the EA community was marginally lower in the UK (7.0) and USA (6.9) compared to Europe (7.4). However, this is likely partly explained by a negative association between length of time in EA and satisfaction, and the USA and UK having respondents with longer time in EA.</li><li>The overall ordering of cause prioritization across geographic areas was relatively consistent. However, we observe that AI risk was prioritized relatively more highly in the UK than the US.</li><li>The percentage of respondents who were members of a local group varied dramatically across countries and cities: The USA (21.9%) and the UK (25.8%) were two of the countries with the lowest percentage of EA group members, while many countries had 40-80%. Similarly, percentages for different cities vary from &lt;10% to &gt;50%.&nbsp;</li></ul></li></ul><h1>Totals per country</h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/mol1mvbmfdcynytaizxg\"></p><p>As was the case in 2022, in 2024 the respondents to the Effective Altruism Survey (EAS) represent countries from every continent (excluding Antarctica).</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/vehtj8hn0iakkwuqwvyw\"></p><p>The United States remains, by far, the country with the greatest number of residents filling in the EA Survey (34% of respondents), followed by the United Kingdom (13.5%). The third and fourth most common countries of residence in the EA Survey were both Western European - Germany and the Netherlands - which respectively represented 7.0% and 5.8% of the 2024 EA Survey respondents. Canada and Australia - both countries in which English is either a first or a widely spoken language - respectively represented 4.8% and 3.9% of respondents. As has been seen in previous years, areas of the globe that stand out as having no or relatively few respondents are South East Asia, Africa, and the Middle East.</p><h1>Changes over time</h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/lqpkhgw41fdrjjfzpbya\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/zdscvqt8ejddncbmh8m6\"></p><h1>Movement between countries</h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/nlcei1nxbexmgknibk4t\"></p><p>As can be seen in the Sankey diagram, there is considerable flow between countries in terms of whether people originated or currently reside there. Those countries that represented the largest share of current countries of residence also tended to be those with the largest proportions as countries of origin for respondents. However, this does not mean that most of the people residing in the country also originated there, and there is considerable variability amongst countries in this respect. For example, the USA had 12.4% of respondents originating from another country, whereas this percentage was 34.1% for the UK. Switzerland stands out as having an exceedingly large percentage of respondents who originated in other countries (mostly from nearby Germany, followed by France). The Netherlands, New Zealand, and Canada also had large fractions of immigrants/expats of approximately one-third. In contrast, almost all respondents from Poland also originated there.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/ofu7otmc0mmy8uhfj7un\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/vbockjecyudokseh0mja\"></p><h1>Totals per city</h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/hrjwek76iqbrerkcyxif\"></p><p>As might be expected, the most prominent hubs for EAs within countries are predominantly capital cities and other major metropolitan areas, or centers of major economic or academic activity. London was the city with the single largest share of survey respondents, followed by four cities/areas of the USA - with the Bay Area only slightly behind London.</p><p>As with countries and regions, we can see how the percentages of EAS respondents residing in the top 15 hubs have changed over survey years. Note, however, that it is not possible to interpret changes over time as direct reflections of growth, variability/consistency, or clear decline of different major hubs. Such hubs may be particularly responsive to efforts by local leaders and groups to have people respond to the EA Survey, which might exaggerate or obscure changes in the number of community members in that location. With this caveat in mind, we note that London has maintained a relatively high percentage of respondents over time, whereas the percentages for the other most substantial hub - the SF Bay area - has been lower in recent surveys relative to earlier years of the survey. Besides a spike in 2015, the percentage of respondents residing in Washington DC has been rising since earlier years, perhaps reflecting an orientation towards policy-related work.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/giprxk9txvqzmn6ayuxz\"></p><h1>Gender</h1><p>Across regions, there were about twice as many men relative to women or other identifications who responded to the survey, in line with our earlier&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/z4Wxd2dnTqDmFZrej/ea-survey-2024-demographics#Gender\"><u>demographics</u></a> post. We also examine variation at the country and city level, but note that many of these estimates are very imprecise due to small sample sizes for each individual geographic area.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/sh0decxtg8v7uegh2yrj\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/a3e3olmh1xnhe61mcg89\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/fw5gqc7pceazwxcw1nba\"></p><h1>Race</h1><p>Racial identification across locations showed considerable variability. Respondents with white racial identification are typically the overwhelming majority in European countries. Respondents identifying with an Asian ethnicity were more common in several predominantly English-speaking countries (UK, USA, Canada, Australia, and New Zealand).</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/ap3npvkor3o27axi20fw\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/hxvjx7kbj3jkubud3zww\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/mynyfkdozkyy3gkeyxm7\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/cgtnuga42xs6pjditafp\"></p><p>UK and US hubs such as London, Seattle, Chicago, Toronto, and New York City tended to have the highest ethnic diversity among top hubs.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/hdeeyss3b0vsbefnhsfs\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/k5krbt2cyoxlybb2yjzg\"></p><p>When considering these racial identification results, at least two things should be noted. Firstly, although respondents were able to provide their own description, the default categories provided were based upon racial identifications primarily from the US, and may not translate especially well to other locations. Secondly, it may not be as informative as it would first seem to compare the racial identification of EAS respondents with the general population in the different locations. Although we can see that there is a general tendency for the racial diversity to increase with higher racial diversity in the broader population, more specific deviations from this background population level may reflect other characteristics that are over-represented in the EA community that are also associated with race, and to different degrees in different areas (for example, income and education).</p><h1>Age and time in the EA community</h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/dbeg8r7ia0bdssibcrfn\"></p><p>The ages of respondents to the EA Survey tend to skew quite young, with most respondents aged between 20 and 40 years of age. There is, however, some variation in ages across locations. Compared with respondents in the USA, UK respondents were on average about 2\u00bd&nbsp; years younger.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/pwjgtciuwd9bytuxwjpc\"></p><p>Of countries with at least 25 respondents, it is interesting to note that among the 5 countries with the highest average ages, 4 are predominantly English speaking (and the&nbsp;<a href=\"https://perma.cc/VU6R-TTPH\"><u>Netherlands has a top ranking</u></a> in English proficiency). One partial explanation for this may be that such English speaking countries were more natural early consumers of EA-related content (which has been mostly written in English), and that the older ages reflect longer tenure of those countries\u2019 residents in the EA community. Indeed, many of these same countries have respondents with longer average \u2018tenure\u2019 in the EA community.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/yyiucykvvp1uhsobnnkq\"></p><p>An assessment of the relationship between the average age per country, and the average tenure in EA, also supports the idea that typical time in the EA community and the age of community members in each country are positively correlated with one another. Countries such as the USA and especially the UK stand out in these plots as having long typical time in EA relative to their average ages, perhaps reflecting the role of these countries in the early stages of EA, with initially young EA community members who now have spent a long time as part of the EA community. The Netherlands is the opposite, with relatively older community members, on average, relative to the typical time spent in EA (it can be seen in the&nbsp;<a href=\"https://docs.google.com/document/d/1VyWKKUvCyKDX_1WY4LLkW5R7qFS5sSGcoFmusXJ2pKo/edit?tab=t.0#heading=h.c6dm5ra0l804\"><u>section below</u></a> that the Netherlands also has a low percentage of students).&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/yddrwz3lnadlwiunc740\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/qtrtnskj9wx7rlfs2zbc\"></p><h1>Student status</h1><p>Student status varied considerably across different regions and countries. Looking at the region-level, the US and UK tended to have lower percentages of students than other European countries or the rest of the world, with the US reliably lower than all other broad regions. Indeed, among the top countries, the US had the lowest percentage of students. Switzerland and France, and their corresponding major hubs of Zurich and Paris, had the highest percentages each with at least a third of EA Survey respondents having been some kind of student.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/cnnh6m9lwfmeb6awawco\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/nrupodkbr6njsgkjrwov\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/qk9qelrf8vxjpzkfmqxr\"></p><h1>Engagement</h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/eqkspihotjarlgu3whs5\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/jdiuieehg5b1tza8q9q4\"></p><p>Amongst countries, the UK had the greatest percentage of respondents placing themselves in the top category of engagement, and the joint highest mean engagement level. Taking countries with large sample sizes (100 or more) and comparing their engagement levels, we found that UK respondents reported significantly greater engagement than respondents from both the US (p &lt; .001) and the Netherlands (p = .02) (these tests were unplanned and the significance levels are not corrected for multiple comparisons). The high engagement apparent in the UK is at least partially driven by Oxford, which stands out amongst hubs as having a tremendously high percentage of respondents placing themselves in the top tier of engagement (72%), although the UK retains significantly higher engagement than the US even when Oxford respondents are removed.&nbsp;</p><p>Two other major hubs, London and Washington DC, also have very high percentages of respondents placing themselves in the top engagement tier (49% and 51% respectively). In contrast, Toronto and Seattle stand out as having relatively less engaged respondents, with the modal response being Moderate engagement.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/ubfvqvjfmjnaodvuxlr7\"></p><h1>Satisfaction</h1><p>As we saw in the previous EA Survey, the USA and the UK (which have the largest number of respondents) have the lowest average community satisfaction ratings. This average is still relatively high, however, at around 7 out of 10. None of the major countries or hubs reached an average of 8. Other European countries and hubs tended to rank higher in satisfaction than countries/hubs in other regions, although Chicago notably bucks this trend, being in the lowest satisfaction country but the hub with the highest average satisfaction rating.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/gn6ncivuumssomnkflpa\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/foxinyufifwfdfzzpt9r\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/wumlkwjdkjyrpkeyunvp\"></p><h1>Cause Prioritization</h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/w6lxskc1xepzwlzitvar\">Across regions, the relative ranking of causes tended to be fairly consistent, with GHD and AI risk towards the top and Climate change and Mental health among the lowest rated. There were, however, some notable differences across regions. For example, AI risk was particularly highly rated in the UK and Europe compared to the US and the rest of the world.</p><p>As we have done in previous years, we followed an approach to summarizing cause prioritization ratings that aims to assess leanings towards broadly \u2018longtermist\u2019 vs. more traditional \u2018neartermist\u2019/global health and wellbeing cause areas. We did this by calculating the average of cause prioritization ratings for biosecurity, nuclear security, AI risk, and existential risk to indicate prioritization of \u2018Longtermist\u2019 cause areas, and the average of global poverty / global health, and mental health to indicate prioritization of \u2018Neartermist\u2019 cause areas. There was relatively little variation amongst top countries in this metric. Among countries with decent sample sizes (100 or more), the UK was found to have a significantly greater LT &gt; NT score than the USA (<i>p&nbsp;</i>= .018) and The Netherlands (<i>p =&nbsp;</i>.005). Though significant, these differences seem substantively quite small (the mean difference between UK and the USA, for example, being around .18, with standard deviations in each country around 1).</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/vkzvfg65ucu9e6lwhpq1\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/j0jxjxapm19xocrodhjx\"></p><h1>EA Group Membership</h1><p>Finally, in the figures below, we present the percentages of 2024 EA Survey respondents who reported being members of a local EA group, across different countries and cities. As we saw in the 2022 survey, the US and UK - which have the largest and longest-established EA communities - had the lowest percentages among the top countries of people reporting being members of EA groups. Amongst major hubs, London and especially the SF Bay area stood out as being both large and having low percentages of respondents as EA group members.&nbsp;</p><p>However, as we\u2019ve noted previously, the percentages for relatively smaller countries (such as Estonia) could be somewhat inflated: when there are relatively few EAs overall responding to the survey from a particular country or city, encouragement from group leaders in those places could lead to nearly all the people who are part of an EA group answering the survey. At the same time, in many smaller or newer communities, it may be that almost the only EAs in a given city or country are the members of a local group. Conversely, more established geographic communities may contain a larger number of (potentially older) EAs who continue to engage with the community but not with groups. That said, these results could also reflect differences in levels of investment, activity, and strategy of local groups in different areas, and we think that this data should be considered holistically, along with other information, by movement builders.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/vbs3cjlbni8vhbio1tcv\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/aaokyljevxy35vzjkcmc\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><h1>Further research</h1><p>This report summarizes general trends and differences across the community, but many results of interest may concern examining the results for individual local communities in more detail. We have already produced a number of bespoke reports for particular country or city level communities, and if other movement builders would find this useful, we would encourage them to reach out.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LaPoyrhzTyCiwYk5T/n1p6dw6ccyt160oprjgc\" alt=\"\"></p><p>This post was written by Jamie Elsey and David Moss. We thank all of the survey respondents for taking the time to complete the survey, as well as Sarah Negris-Mamani for their feedback on this post.</p><p><a href=\"https://rethinkpriorities.org/\">Rethink Priorities</a>&nbsp;is a think-and-do tank dedicated to informing decisions made by high-impact organizations and funders across various cause areas. We invite you to explore our research <a href=\"https://www.rethinkpriorities.org/research\">database</a>&nbsp;and stay updated on new work by subscribing to our <a href=\"https://www.rethinkpriorities.org/newsletter\">newsletter</a>.</p>",
    "base_score": 66,
    "comment_count": 9,
    "posted_at": "2025-08-04T10:27:47.792000Z",
    "created_at": "2025-08-21T04:53:16.630151Z",
    "author_id": "WZovmCHnReSfWiAix",
    "author_display_name": "David_Moss",
    "coauthor_ids": [
      "gJhDWo3tt7Gp65WrP"
    ],
    "coauthor_names": [
      "Jamie E"
    ],
    "tag_ids": [
      "8729b26vvCSQE7H5N",
      "DvgmkiotBKc9Ndm2B",
      "EHLmbEmJ2Qd5WfwTb",
      "gCzFfErskfQzD6b95",
      "hxRMaKvwGqPb43TWB",
      "sLQYE2fgCLwmdRBFH"
    ],
    "tag_names": [
      "Building effective altruism",
      "Data on the EA community",
      "Effective Altruism Survey",
      "Research",
      "Rethink Priorities",
      "Surveys"
    ],
    "markdown_content": "# Summary\n\n- Countries\n  - The USA (34.4%) and the UK (13.5%) remain the countries with the largest proportions of EA respondents. We observed that 32% of respondents came from Europe (excluding the UK), while 20% came from the rest of the world. In previous years, we observed a decline in the percentage of respondents from these countries. This trend seems like it might have decreased or plateaued since 2022.\n  - This year, for the first time, we looked at EAs\u2019 countries of origin as well as their current countries of residence. In absolute terms, we see large flows into the USA and UK, but Switzerland shows by far the highest relative level of in-flows, with 62% of respondents having a different country of origin.\n- Cities\n  - This year, London was the city with the most respondents (6.7%), followed by the SF Bay Area (5.7%), DC (3.8%) and NYC (3.3%). Interestingly, our results show that a majority of EAs live outside of major hubs.\n- Differences across countries\n  - We find that respondents from the US were substantially less likely to be students (15.5%) than those from other regions (23.4% in the UK, 28.2% in Europe).\n  - The UK has significantly higher engagement levels (41% highly engaged) compared to Europe (28%), the USA (24%), and the rest of the world (25%).\n  - Satisfaction with the EA community was marginally lower in the UK (7.0) and USA (6.9) compared to Europe (7.4). However, this is likely partly explained by a negative association between length of time in EA and satisfaction, and the USA and UK having respondents with longer time in EA.\n  - The overall ordering of cause prioritization across geographic areas was relatively consistent. However, we observe that AI risk was prioritized relatively more highly in the UK than the US.\n  - The percentage of respondents who were members of a local group varied dramatically across countries and cities: The USA (21.9%) and the UK (25.8%) were two of the countries with the lowest percentage of EA group members, while many countries had 40-80%. Similarly, percentages for different cities vary from <10% to >50%.\n\n# Totals per country\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/mol1mvbmfdcynytaizxg)\n\nAs was the case in 2022, in 2024 the respondents to the Effective Altruism Survey (EAS) represent countries from every continent (excluding Antarctica).\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/vehtj8hn0iakkwuqwvyw)\n\nThe United States remains, by far, the country with the greatest number of residents filling in the EA Survey (34% of respondents), followed by the United Kingdom (13.5%). The third and fourth most common countries of residence in the EA Survey were both Western European - Germany and the Netherlands - which respectively represented 7.0% and 5.8% of the 2024 EA Survey respondents. Canada and Australia - both countries in which English is either a first or a widely spoken language - respectively represented 4.8% and 3.9% of respondents. As has been seen in previous years, areas of the globe that stand out as having no or relatively few respondents are South East Asia, Africa, and the Middle East.\n\n# Changes over time\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/lqpkhgw41fdrjjfzpbya)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/zdscvqt8ejddncbmh8m6)\n\n# Movement between countries\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/nlcei1nxbexmgknibk4t)\n\nAs can be seen in the Sankey diagram, there is considerable flow between countries in terms of whether people originated or currently reside there. Those countries that represented the largest share of current countries of residence also tended to be those with the largest proportions as countries of origin for respondents. However, this does not mean that most of the people residing in the country also originated there, and there is considerable variability amongst countries in this respect. For example, the USA had 12.4% of respondents originating from another country, whereas this percentage was 34.1% for the UK. Switzerland stands out as having an exceedingly large percentage of respondents who originated in other countries (mostly from nearby Germany, followed by France). The Netherlands, New Zealand, and Canada also had large fractions of immigrants/expats of approximately one-third. In contrast, almost all respondents from Poland also originated there.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/ofu7otmc0mmy8uhfj7un)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/vbockjecyudokseh0mja)\n\n# Totals per city\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/hrjwek76iqbrerkcyxif)\n\nAs might be expected, the most prominent hubs for EAs within countries are predominantly capital cities and other major metropolitan areas, or centers of major economic or academic activity. London was the city with the single largest share of survey respondents, followed by four cities/areas of the USA - with the Bay Area only slightly behind London.\n\nAs with countries and regions, we can see how the percentages of EAS respondents residing in the top 15 hubs have changed over survey years. Note, however, that it is not possible to interpret changes over time as direct reflections of growth, variability/consistency, or clear decline of different major hubs. Such hubs may be particularly responsive to efforts by local leaders and groups to have people respond to the EA Survey, which might exaggerate or obscure changes in the number of community members in that location. With this caveat in mind, we note that London has maintained a relatively high percentage of respondents over time, whereas the percentages for the other most substantial hub - the SF Bay area - has been lower in recent surveys relative to earlier years of the survey. Besides a spike in 2015, the percentage of respondents residing in Washington DC has been rising since earlier years, perhaps reflecting an orientation towards policy-related work.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/giprxk9txvqzmn6ayuxz)\n\n# Gender\n\nAcross regions, there were about twice as many men relative to women or other identifications who responded to the survey, in line with our earlier\u00a0[demographics](https://forum.effectivealtruism.org/posts/z4Wxd2dnTqDmFZrej/ea-survey-2024-demographics#Gender) post. We also examine variation at the country and city level, but note that many of these estimates are very imprecise due to small sample sizes for each individual geographic area.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/sh0decxtg8v7uegh2yrj)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/a3e3olmh1xnhe61mcg89)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/fw5gqc7pceazwxcw1nba)\n\n# Race\n\nRacial identification across locations showed considerable variability. Respondents with white racial identification are typically the overwhelming majority in European countries. Respondents identifying with an Asian ethnicity were more common in several predominantly English-speaking countries (UK, USA, Canada, Australia, and New Zealand).\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/ap3npvkor3o27axi20fw)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/hxvjx7kbj3jkubud3zww)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/mynyfkdozkyy3gkeyxm7)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/cgtnuga42xs6pjditafp)\n\nUK and US hubs such as London, Seattle, Chicago, Toronto, and New York City tended to have the highest ethnic diversity among top hubs.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/hdeeyss3b0vsbefnhsfs)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/k5krbt2cyoxlybb2yjzg)\n\nWhen considering these racial identification results, at least two things should be noted. Firstly, although respondents were able to provide their own description, the default categories provided were based upon racial identifications primarily from the US, and may not translate especially well to other locations. Secondly, it may not be as informative as it would first seem to compare the racial identification of EAS respondents with the general population in the different locations. Although we can see that there is a general tendency for the racial diversity to increase with higher racial diversity in the broader population, more specific deviations from this background population level may reflect other characteristics that are over-represented in the EA community that are also associated with race, and to different degrees in different areas (for example, income and education).\n\n# Age and time in the EA community\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/dbeg8r7ia0bdssibcrfn)\n\nThe ages of respondents to the EA Survey tend to skew quite young, with most respondents aged between 20 and 40 years of age. There is, however, some variation in ages across locations. Compared with respondents in the USA, UK respondents were on average about 2\u00bd\u00a0 years younger.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/pwjgtciuwd9bytuxwjpc)\n\nOf countries with at least 25 respondents, it is interesting to note that among the 5 countries with the highest average ages, 4 are predominantly English speaking (and the\u00a0[Netherlands has a top ranking](https://perma.cc/VU6R-TTPH) in English proficiency). One partial explanation for this may be that such English speaking countries were more natural early consumers of EA-related content (which has been mostly written in English), and that the older ages reflect longer tenure of those countries\u2019 residents in the EA community. Indeed, many of these same countries have respondents with longer average \u2018tenure\u2019 in the EA community.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/yyiucykvvp1uhsobnnkq)\n\nAn assessment of the relationship between the average age per country, and the average tenure in EA, also supports the idea that typical time in the EA community and the age of community members in each country are positively correlated with one another. Countries such as the USA and especially the UK stand out in these plots as having long typical time in EA relative to their average ages, perhaps reflecting the role of these countries in the early stages of EA, with initially young EA community members who now have spent a long time as part of the EA community. The Netherlands is the opposite, with relatively older community members, on average, relative to the typical time spent in EA (it can be seen in the\u00a0[section below](https://docs.google.com/document/d/1VyWKKUvCyKDX_1WY4LLkW5R7qFS5sSGcoFmusXJ2pKo/edit?tab=t.0#heading=h.c6dm5ra0l804) that the Netherlands also has a low percentage of students).\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/yddrwz3lnadlwiunc740)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/qtrtnskj9wx7rlfs2zbc)\n\n# Student status\n\nStudent status varied considerably across different regions and countries. Looking at the region-level, the US and UK tended to have lower percentages of students than other European countries or the rest of the world, with the US reliably lower than all other broad regions. Indeed, among the top countries, the US had the lowest percentage of students. Switzerland and France, and their corresponding major hubs of Zurich and Paris, had the highest percentages each with at least a third of EA Survey respondents having been some kind of student.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/cnnh6m9lwfmeb6awawco)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/nrupodkbr6njsgkjrwov)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/qk9qelrf8vxjpzkfmqxr)\n\n# Engagement\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/eqkspihotjarlgu3whs5)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/jdiuieehg5b1tza8q9q4)\n\nAmongst countries, the UK had the greatest percentage of respondents placing themselves in the top category of engagement, and the joint highest mean engagement level. Taking countries with large sample sizes (100 or more) and comparing their engagement levels, we found that UK respondents reported significantly greater engagement than respondents from both the US (p < .001) and the Netherlands (p = .02) (these tests were unplanned and the significance levels are not corrected for multiple comparisons). The high engagement apparent in the UK is at least partially driven by Oxford, which stands out amongst hubs as having a tremendously high percentage of respondents placing themselves in the top tier of engagement (72%), although the UK retains significantly higher engagement than the US even when Oxford respondents are removed.\n\nTwo other major hubs, London and Washington DC, also have very high percentages of respondents placing themselves in the top engagement tier (49% and 51% respectively). In contrast, Toronto and Seattle stand out as having relatively less engaged respondents, with the modal response being Moderate engagement.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/ubfvqvjfmjnaodvuxlr7)\n\n# Satisfaction\n\nAs we saw in the previous EA Survey, the USA and the UK (which have the largest number of respondents) have the lowest average community satisfaction ratings. This average is still relatively high, however, at around 7 out of 10. None of the major countries or hubs reached an average of 8. Other European countries and hubs tended to rank higher in satisfaction than countries/hubs in other regions, although Chicago notably bucks this trend, being in the lowest satisfaction country but the hub with the highest average satisfaction rating.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/gn6ncivuumssomnkflpa)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/foxinyufifwfdfzzpt9r)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/wumlkwjdkjyrpkeyunvp)\n\n# Cause Prioritization\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/w6lxskc1xepzwlzitvar)Across regions, the relative ranking of causes tended to be fairly consistent, with GHD and AI risk towards the top and Climate change and Mental health among the lowest rated. There were, however, some notable differences across regions. For example, AI risk was particularly highly rated in the UK and Europe compared to the US and the rest of the world.\n\nAs we have done in previous years, we followed an approach to summarizing cause prioritization ratings that aims to assess leanings towards broadly \u2018longtermist\u2019 vs. more traditional \u2018neartermist\u2019/global health and wellbeing cause areas. We did this by calculating the average of cause prioritization ratings for biosecurity, nuclear security, AI risk, and existential risk to indicate prioritization of \u2018Longtermist\u2019 cause areas, and the average of global poverty / global health, and mental health to indicate prioritization of \u2018Neartermist\u2019 cause areas. There was relatively little variation amongst top countries in this metric. Among countries with decent sample sizes (100 or more), the UK was found to have a significantly greater LT > NT score than the USA (*p*= .018) and The Netherlands (*p =*.005). Though significant, these differences seem substantively quite small (the mean difference between UK and the USA, for example, being around .18, with standard deviations in each country around 1).\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/vkzvfg65ucu9e6lwhpq1)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/j0jxjxapm19xocrodhjx)\n\n# EA Group Membership\n\nFinally, in the figures below, we present the percentages of 2024 EA Survey respondents who reported being members of a local EA group, across different countries and cities. As we saw in the 2022 survey, the US and UK - which have the largest and longest-established EA communities - had the lowest percentages among the top countries of people reporting being members of EA groups. Amongst major hubs, London and especially the SF Bay area stood out as being both large and having low percentages of respondents as EA group members.\n\nHowever, as we\u2019ve noted previously, the percentages for relatively smaller countries (such as Estonia) could be somewhat inflated: when there are relatively few EAs overall responding to the survey from a particular country or city, encouragement from group leaders in those places could lead to nearly all the people who are part of an EA group answering the survey. At the same time, in many smaller or newer communities, it may be that almost the only EAs in a given city or country are the members of a local group. Conversely, more established geographic communities may contain a larger number of (potentially older) EAs who continue to engage with the community but not with groups. That said, these results could also reflect differences in levels of investment, activity, and strategy of local groups in different areas, and we think that this data should be considered holistically, along with other information, by movement builders.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/vbs3cjlbni8vhbio1tcv)\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fpn6MZTShaGLZqopo/aaokyljevxy35vzjkcmc)\n\n# Further research\n\nThis report summarizes general trends and differences across the community, but many results of interest may concern examining the results for individual local communities in more detail. We have already produced a number of bespoke reports for particular country or city level communities, and if other movement builders would find this useful, we would encourage them to reach out.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/LaPoyrhzTyCiwYk5T/n1p6dw6ccyt160oprjgc)\n\nThis post was written by Jamie Elsey and David Moss. We thank all of the survey respondents for taking the time to complete the survey, as well as Sarah Negris-Mamani for their feedback on this post.\n\n[Rethink Priorities](https://rethinkpriorities.org/)\u00a0is a think-and-do tank dedicated to informing decisions made by high-impact organizations and funders across various cause areas. We invite you to explore our research [database](https://www.rethinkpriorities.org/research)\u00a0and stay updated on new work by subscribing to our [newsletter](https://www.rethinkpriorities.org/newsletter).",
    "word_count": 2423,
    "reading_time_minutes": 12,
    "external_links": [
      "https://docs.google.com/document/d/1VyWKKUvCyKDX_1WY4LLkW5R7qFS5sSGcoFmusXJ2pKo/edit?tab=t.0#heading=h.c6dm5ra0l804",
      "https://forum.effectivealtruism.org/posts/z4Wxd2dnTqDmFZrej/ea-survey-2024-demographics#Gender",
      "https://perma.cc/VU6R-TTPH",
      "https://rethinkpriorities.org/",
      "https://www.rethinkpriorities.org/newsletter",
      "https://www.rethinkpriorities.org/research"
    ],
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T04:53:19.084738Z"
  },
  {
    "id": 392,
    "post_id": "Fvdj4dmcgauEuKH9E",
    "title": "Giving at death is more effective than giving during life",
    "title_normalized": "giving at death is more effective than giving during life",
    "page_url": "https://forum.effectivealtruism.org/posts/Fvdj4dmcgauEuKH9E/giving-at-death-is-more-effective-than-giving-during-life",
    "html_body": "<p>I\u2019m looking for some feedback. I\u2019ve done some modeling to see how giving at death compare compares to giving during life. I\u2019ve looked specifically at giving 10% of annual salary versus giving a certain percentage of net worth at death. Based on the models that I\u2019m looking at, giving 10% of annual salary is less effective than giving 10% of net worth at death if the population of givers is 750,000 or more. There is no parameter at which the annual givers outperform the givers at death. I would like to hear any feedback so that I can see if my model holds up to scrutiny.&nbsp;</p><p><strong>Should altruists give during life or at death?</strong>&nbsp;</p><p>A mathematical &amp; ethical exploration&nbsp;</p><p><strong>\ud83d\udccb The question:</strong>&nbsp;</p><p>If altruists donated a percentage of their income during life (e.g., 10% annually) versus donating a percentage of their net worth at death (e.g., 10\u201350%), which approach is more effective \u2014 especially at the population level?&nbsp;</p><p><strong>\ud83d\udd37 Key findings from modeling a realistic, steady-state population:</strong>&nbsp;</p><p>We simulated a population of altruists (1M\u201310M people), with realistic wealth accumulation, mortality, and giving behaviors.&nbsp;</p><p>Even though only ~1% of people die each year, their accumulated net worth at death is so much larger than annual income that total donations from death-based giving outperformed life-based giving \u2014 starting in year 1 and continuing indefinitely.&nbsp;</p><p><strong>\ud83d\udcca Results (1M altruists, steady-state, average annual donations):</strong>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"padding:1.0px;width:179.2px\"><p><strong>Strategy</strong></p></td><td style=\"padding:1.0px;width:133.1px\"><p><strong>Average annual donations</strong></p></td></tr><tr><td style=\"padding:1.0px;width:179.2px\">10% of income (everyone)</td><td style=\"padding:1.0px;width:133.1px\">$5B</td></tr><tr><td style=\"padding:1.0px;width:179.2px\">50% of net worth at death (everyone)</td><td style=\"padding:1.0px;width:133.1px\">$65.8B</td></tr><tr><td style=\"padding:1.0px;width:179.2px\">40% of net worth at death (everyone)</td><td style=\"padding:1.0px;width:133.1px\">$52.6B</td></tr><tr><td style=\"padding:1.0px;width:179.2px\">30% of net worth at death (everyone)</td><td style=\"padding:1.0px;width:133.1px\">$39.5B</td></tr><tr><td style=\"padding:1.0px;width:179.2px\">20% of net worth at death (everyone)</td><td style=\"padding:1.0px;width:133.1px\">$26.3B</td></tr><tr><td style=\"padding:1.0px;width:179.2px\">10% of net worth at death (everyone)</td><td style=\"padding:1.0px;width:133.1px\">$13.1B</td></tr></tbody></table></figure><p>Even donating just 10% of net worth at death generated more than 2\u00d7 the annual donations of giving 10% of income during life.&nbsp;</p><p>And at 50% of net worth at death, donations were over 13\u00d7 higher than giving 10% of income annually.&nbsp;</p><p><strong>\ud83c\udf0d Real-world context:</strong>&nbsp;</p><ul><li>About 150,000 people worldwide die each year with &gt;$2M in net worth.</li><li>Together, they hold $750B\u2013$1T at death.</li><li>If they gave just 50% at death, it would add $375\u2013500B/year to global giving \u2014 roughly half of all current global philanthropy (~$900B/year) \u2014 without giving anything during life.&nbsp;</li></ul><p><strong>\ud83d\udcc8 Why death-based giving is even more powerful:</strong>&nbsp;</p><ul><li>If individuals stopped giving during life and allowed their full income &amp; wealth to compound, their estates at death would grow much larger \u2014 making their death-based donations even greater than today\u2019s models assume.&nbsp;</li></ul><p><strong>\u2696\ufe0f What about moral and practical arguments for giving during life?</strong>&nbsp;</p><p>People often justify giving during life to:&nbsp;</p><ul><li>Respond to immediate crises</li><li>Support interventions with compounding social impact</li><li>Feel moral satisfaction &amp; signal values&nbsp;</li></ul><p>However, at the population level, the steady flow of net worth at death donations already surpasses lifetime giving and is more than enough to meet urgent and ongoing needs \u2014 even starting in year 1.&nbsp;</p><p><strong>\ud83d\udd37 Final insight:</strong>&nbsp;</p><p>If altruists gave even just 10% of their net worth at death, they would out-donate what they currently achieve by giving 10% of their income during life.</p><p>If they gave 50% of net worth at death, their donations would be more than 13\u00d7 greater \u2014 while also keeping their full income and wealth throughout life.&nbsp;</p><p><strong>TL;DR:</strong>&nbsp;</p><p>Giving 10% of income during life feels morally satisfying, but at the population level, giving 10\u201350% of net worth at death generates far more resources, meets urgent needs, and allows wealth to grow \u2014 making it the more effective strategy overall.<br>&nbsp;</p>",
    "base_score": 15,
    "comment_count": 44,
    "posted_at": "2025-07-15T15:49:50.830000Z",
    "created_at": "2025-08-21T06:37:50.782692Z",
    "author_id": "sPphsLxwbuvXGphwo",
    "author_display_name": "Dr. Seth Mathus Ganz",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "L6NqHZkLc4xZ7YtDr",
      "hxRMaKvwGqPb43TWB",
      "psBzwdY8ipfCeExJ7",
      "rdxSaRhkHxwjbNkD7",
      "y4b45yXXrFJpPmx7d"
    ],
    "tag_names": [
      "Cause prioritization",
      "Cost-effectiveness analysis",
      "Effective giving",
      "Opinion",
      "Research"
    ],
    "markdown_content": "I\u2019m looking for some feedback. I\u2019ve done some modeling to see how giving at death compare compares to giving during life. I\u2019ve looked specifically at giving 10% of annual salary versus giving a certain percentage of net worth at death. Based on the models that I\u2019m looking at, giving 10% of annual salary is less effective than giving 10% of net worth at death if the population of givers is 750,000 or more. There is no parameter at which the annual givers outperform the givers at death. I would like to hear any feedback so that I can see if my model holds up to scrutiny.\n\n**Should altruists give during life or at death?**\n\nA mathematical & ethical exploration\n\n**\ud83d\udccb The question:**\n\nIf altruists donated a percentage of their income during life (e.g., 10% annually) versus donating a percentage of their net worth at death (e.g., 10\u201350%), which approach is more effective \u2014 especially at the population level?\n\n**\ud83d\udd37 Key findings from modeling a realistic, steady-state population:**\n\nWe simulated a population of altruists (1M\u201310M people), with realistic wealth accumulation, mortality, and giving behaviors.\n\nEven though only ~1% of people die each year, their accumulated net worth at death is so much larger than annual income that total donations from death-based giving outperformed life-based giving \u2014 starting in year 1 and continuing indefinitely.\n\n**\ud83d\udcca Results (1M altruists, steady-state, average annual donations):**\n\n|  |  |\n| --- | --- |\n| **Strategy** | **Average annual donations** |\n| 10% of income (everyone) | $5B |\n| 50% of net worth at death (everyone) | $65.8B |\n| 40% of net worth at death (everyone) | $52.6B |\n| 30% of net worth at death (everyone) | $39.5B |\n| 20% of net worth at death (everyone) | $26.3B |\n| 10% of net worth at death (everyone) | $13.1B |\n\nEven donating just 10% of net worth at death generated more than 2\u00d7 the annual donations of giving 10% of income during life.\n\nAnd at 50% of net worth at death, donations were over 13\u00d7 higher than giving 10% of income annually.\n\n**\ud83c\udf0d Real-world context:**\n\n- About 150,000 people worldwide die each year with >$2M in net worth.\n- Together, they hold $750B\u2013$1T at death.\n- If they gave just 50% at death, it would add $375\u2013500B/year to global giving \u2014 roughly half of all current global philanthropy (~$900B/year) \u2014 without giving anything during life.\n\n**\ud83d\udcc8 Why death-based giving is even more powerful:**\n\n- If individuals stopped giving during life and allowed their full income & wealth to compound, their estates at death would grow much larger \u2014 making their death-based donations even greater than today\u2019s models assume.\n\n**\u2696\ufe0f What about moral and practical arguments for giving during life?**\n\nPeople often justify giving during life to:\n\n- Respond to immediate crises\n- Support interventions with compounding social impact\n- Feel moral satisfaction & signal values\n\nHowever, at the population level, the steady flow of net worth at death donations already surpasses lifetime giving and is more than enough to meet urgent and ongoing needs \u2014 even starting in year 1.\n\n**\ud83d\udd37 Final insight:**\n\nIf altruists gave even just 10% of their net worth at death, they would out-donate what they currently achieve by giving 10% of their income during life.\n\nIf they gave 50% of net worth at death, their donations would be more than 13\u00d7 greater \u2014 while also keeping their full income and wealth throughout life.\n\n**TL;DR:**\n\nGiving 10% of income during life feels morally satisfying, but at the population level, giving 10\u201350% of net worth at death generates far more resources, meets urgent needs, and allows wealth to grow \u2014 making it the more effective strategy overall.",
    "word_count": 618,
    "reading_time_minutes": 3,
    "external_links": null,
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T06:37:53.364079Z"
  },
  {
    "id": 98,
    "post_id": "hwjEjaqvNqg9GdXqr",
    "title": "Idea to boost international AI coordination",
    "title_normalized": "idea to boost international ai coordination",
    "page_url": "https://forum.effectivealtruism.org/posts/hwjEjaqvNqg9GdXqr/idea-to-boost-international-ai-coordination",
    "html_body": "<p>I have an idea for an organisation that I think ought to exist.&nbsp;</p><p>I'd welcome feedback on this from the community in particular:&nbsp;</p><ol><li>Is the concept sound? How could it be better/what's missing?</li><li>Is it needed? Are there other organisations already doing this that I am not aware of perhaps?</li></ol><p>I am not yet sure whether it is worth investing time in this, so would love candid feedback from people who know more than me. I would like to contribute something useful, but I don't know if I am barking up the wrong tree here!</p><p><strong>Sorry if any of this is blindingly obvious - it's very much a half-baked idea at the moment, and I am not sure whether it's worth investing time in or if I am just going over ground that is well covered already.</strong></p><h2>0. The Context</h2><p>Predictions for Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI) timelines have accelerated in recent years, with experts significantly shortening their forecasts as AI capabilities continue to advance.</p><p>The median forecast from current research and expert predictions places AGI arrival around 2028, with most estimates clustering between 2025 and 2035.&nbsp;</p><p><strong>Industry Leaders (typically predicting 2025-2030):</strong>&nbsp;<br>\u2022 Sam Altman (OpenAI): Stated in 2025 that <a href=\"https://blog.samaltman.com/reflections\">\u201cwe know how to build AGI\u201d</a> and suggests arrival by <a href=\"https://techcrunch.com/2025/06/11/sam-altman-thinks-ai-will-have-novel-insights-next-year/#:~:text=At%20one%20point%20in%20the,up%20with%20new%2C%20interesting%20ideas\">2026</a>&nbsp;<br>\u2022 Denis Hassabis (Google DeepMind): Predicts <a href=\"https://www.notion.so/One-Pager-Founding-Memo-240984d411d5801ab85de116d5e19683?pvs=21\">AGI by 2030</a>.&nbsp;<br>\u2022 Dario Amodei (Anthropic): Expects \u201chuman-level AI\u201d within 2-3 years, <a href=\"https://www.benzinga.com/tech/24/11/41928832/anthropic-ceo-says-ai-similar-to-human-intelligence-could-be-around-the-corner-well-get-there-by-2026-or-2027\">suggesting 2026-2027</a><br>&nbsp;\u2022 Shane Legg (DeepMind): Maintains his prediction of <a href=\"https://www.youtube.com/watch?v=N84ZntqkO4I\">50% probability by 2028</a></p><p><strong>Academic Researchers (typically predicting 2040-2060):</strong>&nbsp;<br>\u2022 Most surveys indicate a 50% probability of achieving <a href=\"https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/\">AGI&nbsp;between 2040 and 2061</a><strong>,</strong>&nbsp;with some estimating that superintelligence could follow within a few decades.</p><p><strong>Artificial Superintelligence (ASI) Predictions</strong> ASI predictions are inherently more speculative, but researchers who address this topic generally suggest ASI will follow AGI within a couple of years.</p><h2>1. The Problem: AI Governance is Fragmented, Slow, and Captured</h2><p>AI systems are becoming increasingly influential, shaping markets, societies, and global security dynamics.</p><p>The stakes are high, and the risks extensive. Malicious actors could weaponise AGI to design novel bioweapons, automate cyberwarfare, or conduct large-scale psychological operations. The same systems could undermine global stability by accelerating WMD development, enabling authoritarian surveillance regimes, or triggering mass economic dislocation through the rapid displacement of human labour. A comprehensive assessment of AI risk would run to many pages.</p><p>Yet the institutions responsible for coordinating their development, deployment, and oversight are under developed. There is no shared coordination substrate to translate emerging norms into common practice. Instead of a global governance infrastructure, we have a patchwork of PDFs, speeches, and closed-door agreements.</p><p>This situation incentivises race dynamics, obstructs trust-building, and blocks the emergence of governance mechanism.&nbsp;</p><h2>2. The Solution: The AI Coordination Forum</h2><p>The AI Coordination Forum (AICF) will serve as a supranational, independent institution to coordinate AI governance between states, labs, and civil society.</p><p>AICF is not a think tank, not a lobbying group, and not an advocacy NGO. It is a coordination layer. Its work is not driven by ideology or regulation. It is driven by the need to make distributed actors legible to one another and to allow credible commitments around frontier AI systems to take form. It will design, maintain, and steward protocols, registries, and disclosure mechanisms that promote transparency, safety, and interoperability across jurisdictions and technical ecosystems.</p><p>AICF exists to support the safe deployment of advanced AI by:</p><ul><li>Facilitating cooperation between frontier labs, governments, civil society, and regulators</li><li>Creating non-binding but widely adopted standards and registries for model transparency, evaluation, and risk disclosure</li><li>Supporting the capacity of lower-resourced states and institutions to engage in frontier AI governance</li><li>Bridging legal, technical, and operational gaps in global AI safety infrastructure</li></ul><p>AICF is not a regulator, nor a lobbying body. It is an independent, neutral platform for technical policy coordination.</p><p>The Forum\u2019s role is to fill the institutional void where no one actor can lead, and where centralisation would break trust.</p><h2><strong>3. What AICF Will Do: Starting Objectives (Ideas)</strong></h2><p><strong>1. Launch and maintain the Safety Disclosure Protocol (SDP)</strong></p><p><i>Why</i>: This sets immediate norms for safe deployment and creates de facto standards that can influence both open-source developers and leading labs. If widely adopted, it shapes the release landscape for frontier models.</p><p><i>Impact</i>: Medium-term containment of misuse risk; long-term scaffolding for international regulation.</p><p><i>Feasibility</i>: High, if pitched as a collaborative and flexible standard.</p><p><i>Critical path</i>: Adoption by 2\u20133 influential actors (e.g. Anthropic, Mistral, policy bodies).</p><p><strong>2. Run red-team hackathons and evaluations on frontier models</strong></p><p><i>Why</i>: Provides actionable insights into model capabilities and misuse vectors. Forces labs to confront risks and correct failure modes pre-deployment. Helps the public and policymakers calibrate their threat models.</p><p><i>Impact</i>: Direct contribution to model alignment, responsible disclosure, and hardening.</p><p><i>Feasibility</i>: High, especially with academic and civil society collaboration.</p><p><i>Critical path</i>: Access to frontier models under controlled conditions.</p><p><strong>3. Track AGI-relevant compute and training runs across jurisdictions</strong></p><p><i>Why</i>: No one has a credible, open-source map of global compute trends. Surveillance of this space enables early warning and accountability.</p><p><i>Impact</i>: High, particularly if used to detect race dynamics or rogue development.</p><p><i>Feasibility</i>: Medium to low, due to data opacity and cooperation requirements.</p><h2>5. Conclusion</h2><p>The AI Coordination Forum is designed to fill a vacuum: a supranational, agile, and effective body capable of navigating the fragmented global landscape of AI governance. While existing institutions move slowly and national strategies remain parochial, AICF would establish early legitimacy by creating useful coordination primitives, high-trust dialogues and functional multistakeholderism. It is a bet on institutional innovation as a lever for long-term global stability and a future in which advanced AI is governed wisely.</p><p>AICF can become the coordination infrastructure that enables others to speak the same language, share credible information, and act responsibly across borders.</p><p><strong>If you think this idea is useful, and would like to chat with me about potentially working on it, feel free to comment or dm me.&nbsp;</strong><br><br>Also I hope I have followed all the forum rules - I did read them in advance but this is my first post!&nbsp;</p>",
    "base_score": 2,
    "comment_count": 0,
    "posted_at": "2025-08-13T13:40:10.696000Z",
    "created_at": "2025-08-21T04:16:05.166223Z",
    "author_id": "FdabPYprgkRnDrzbg",
    "author_display_name": "Jamie Green",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "oNiQsBHA3i837sySD",
      "of9xBvR3wpbp6qsZC",
      "u3Xg8MjDe2e6BvKtv",
      "z8qFsGt5iXyZiLbjN"
    ],
    "tag_names": [
      "AI governance",
      "AI safety",
      "Opportunities to take action",
      "Policy"
    ],
    "markdown_content": "I have an idea for an organisation that I think ought to exist.\n\nI'd welcome feedback on this from the community in particular:\n\n1. Is the concept sound? How could it be better/what's missing?\n2. Is it needed? Are there other organisations already doing this that I am not aware of perhaps?\n\nI am not yet sure whether it is worth investing time in this, so would love candid feedback from people who know more than me. I would like to contribute something useful, but I don't know if I am barking up the wrong tree here!\n\n**Sorry if any of this is blindingly obvious - it's very much a half-baked idea at the moment, and I am not sure whether it's worth investing time in or if I am just going over ground that is well covered already.**\n\n## 0. The Context\n\nPredictions for Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI) timelines have accelerated in recent years, with experts significantly shortening their forecasts as AI capabilities continue to advance.\n\nThe median forecast from current research and expert predictions places AGI arrival around 2028, with most estimates clustering between 2025 and 2035.\n\n**Industry Leaders (typically predicting 2025-2030):**\u00a0  \n\u2022 Sam Altman (OpenAI): Stated in 2025 that [\u201cwe know how to build AGI\u201d](https://blog.samaltman.com/reflections) and suggests arrival by [2026](https://techcrunch.com/2025/06/11/sam-altman-thinks-ai-will-have-novel-insights-next-year/#:~:text=At%20one%20point%20in%20the,up%20with%20new%2C%20interesting%20ideas)\u00a0  \n\u2022 Denis Hassabis (Google DeepMind): Predicts [AGI by 2030](https://www.notion.so/One-Pager-Founding-Memo-240984d411d5801ab85de116d5e19683?pvs=21).\u00a0  \n\u2022 Dario Amodei (Anthropic): Expects \u201chuman-level AI\u201d within 2-3 years, [suggesting 2026-2027](https://www.benzinga.com/tech/24/11/41928832/anthropic-ceo-says-ai-similar-to-human-intelligence-could-be-around-the-corner-well-get-there-by-2026-or-2027)  \n\u00a0\u2022 Shane Legg (DeepMind): Maintains his prediction of [50% probability by 2028](https://www.youtube.com/watch?v=N84ZntqkO4I)\n\n**Academic Researchers (typically predicting 2040-2060):**\u00a0  \n\u2022 Most surveys indicate a 50% probability of achieving [AGI\u00a0between 2040 and 2061](https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/)**,**\u00a0with some estimating that superintelligence could follow within a few decades.\n\n**Artificial Superintelligence (ASI) Predictions** ASI predictions are inherently more speculative, but researchers who address this topic generally suggest ASI will follow AGI within a couple of years.\n\n## 1. The Problem: AI Governance is Fragmented, Slow, and Captured\n\nAI systems are becoming increasingly influential, shaping markets, societies, and global security dynamics.\n\nThe stakes are high, and the risks extensive. Malicious actors could weaponise AGI to design novel bioweapons, automate cyberwarfare, or conduct large-scale psychological operations. The same systems could undermine global stability by accelerating WMD development, enabling authoritarian surveillance regimes, or triggering mass economic dislocation through the rapid displacement of human labour. A comprehensive assessment of AI risk would run to many pages.\n\nYet the institutions responsible for coordinating their development, deployment, and oversight are under developed. There is no shared coordination substrate to translate emerging norms into common practice. Instead of a global governance infrastructure, we have a patchwork of PDFs, speeches, and closed-door agreements.\n\nThis situation incentivises race dynamics, obstructs trust-building, and blocks the emergence of governance mechanism.\n\n## 2. The Solution: The AI Coordination Forum\n\nThe AI Coordination Forum (AICF) will serve as a supranational, independent institution to coordinate AI governance between states, labs, and civil society.\n\nAICF is not a think tank, not a lobbying group, and not an advocacy NGO. It is a coordination layer. Its work is not driven by ideology or regulation. It is driven by the need to make distributed actors legible to one another and to allow credible commitments around frontier AI systems to take form. It will design, maintain, and steward protocols, registries, and disclosure mechanisms that promote transparency, safety, and interoperability across jurisdictions and technical ecosystems.\n\nAICF exists to support the safe deployment of advanced AI by:\n\n- Facilitating cooperation between frontier labs, governments, civil society, and regulators\n- Creating non-binding but widely adopted standards and registries for model transparency, evaluation, and risk disclosure\n- Supporting the capacity of lower-resourced states and institutions to engage in frontier AI governance\n- Bridging legal, technical, and operational gaps in global AI safety infrastructure\n\nAICF is not a regulator, nor a lobbying body. It is an independent, neutral platform for technical policy coordination.\n\nThe Forum\u2019s role is to fill the institutional void where no one actor can lead, and where centralisation would break trust.\n\n## **3. What AICF Will Do: Starting Objectives (Ideas)**\n\n**1. Launch and maintain the Safety Disclosure Protocol (SDP)**\n\n*Why*: This sets immediate norms for safe deployment and creates de facto standards that can influence both open-source developers and leading labs. If widely adopted, it shapes the release landscape for frontier models.\n\n*Impact*: Medium-term containment of misuse risk; long-term scaffolding for international regulation.\n\n*Feasibility*: High, if pitched as a collaborative and flexible standard.\n\n*Critical path*: Adoption by 2\u20133 influential actors (e.g. Anthropic, Mistral, policy bodies).\n\n**2. Run red-team hackathons and evaluations on frontier models**\n\n*Why*: Provides actionable insights into model capabilities and misuse vectors. Forces labs to confront risks and correct failure modes pre-deployment. Helps the public and policymakers calibrate their threat models.\n\n*Impact*: Direct contribution to model alignment, responsible disclosure, and hardening.\n\n*Feasibility*: High, especially with academic and civil society collaboration.\n\n*Critical path*: Access to frontier models under controlled conditions.\n\n**3. Track AGI-relevant compute and training runs across jurisdictions**\n\n*Why*: No one has a credible, open-source map of global compute trends. Surveillance of this space enables early warning and accountability.\n\n*Impact*: High, particularly if used to detect race dynamics or rogue development.\n\n*Feasibility*: Medium to low, due to data opacity and cooperation requirements.\n\n## 5. Conclusion\n\nThe AI Coordination Forum is designed to fill a vacuum: a supranational, agile, and effective body capable of navigating the fragmented global landscape of AI governance. While existing institutions move slowly and national strategies remain parochial, AICF would establish early legitimacy by creating useful coordination primitives, high-trust dialogues and functional multistakeholderism. It is a bet on institutional innovation as a lever for long-term global stability and a future in which advanced AI is governed wisely.\n\nAICF can become the coordination infrastructure that enables others to speak the same language, share credible information, and act responsibly across borders.\n\n**If you think this idea is useful, and would like to chat with me about potentially working on it, feel free to comment or dm me.**  \n  \nAlso I hope I have followed all the forum rules - I did read them in advance but this is my first post!",
    "word_count": 1014,
    "reading_time_minutes": 5,
    "external_links": [
      "https://blog.samaltman.com/reflections",
      "https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/",
      "https://techcrunch.com/2025/06/11/sam-altman-thinks-ai-will-have-novel-insights-next-year/#:~:text=At%20one%20point%20in%20the,up%20with%20new%2C%20interesting%20ideas",
      "https://www.benzinga.com/tech/24/11/41928832/anthropic-ceo-says-ai-similar-to-human-intelligence-could-be-around-the-corner-well-get-there-by-2026-or-2027",
      "https://www.notion.so/One-Pager-Founding-Memo-240984d411d5801ab85de116d5e19683?pvs=21",
      "https://www.youtube.com/watch?v=N84ZntqkO4I"
    ],
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T04:16:07.572838Z"
  },
  {
    "id": 2189,
    "post_id": "hYyhKdGSYmxPWS7wy",
    "title": "Considering Directed Kidney Donation Within the EA Community",
    "title_normalized": "considering directed kidney donation within the ea community",
    "page_url": "https://forum.effectivealtruism.org/posts/hYyhKdGSYmxPWS7wy/considering-directed-kidney-donation-within-the-ea-community",
    "html_body": "<p>Hi everyone,</p><p>I've been thinking deeply about donating one of my kidneys to a stranger, and I'd like to explore the possibility of donating it to someone within the EA community\u2014someone who shares core values with me.&nbsp;</p><p>So first and foremost; <strong>is there an EA'er that is waiting for a kidney for themselves/a loved one ?</strong></p><p>Perhaps there is not, which could end the discussion, but from a hypothetical standpoint, the following still interests me:&nbsp;</p><p>The idea of helping a fellow EA'er in such a profound way is compelling, both from a potential impact standpoint as well as in terms of personal fulfillment, &nbsp;but it also raises a number of ethical and practical concerns that I have been considering. Any thoughts on any of the following topics would greatly be appreciated!</p><p><strong>Equity and Fairness:</strong> By prioritizing someone within our community, am I inadvertently encouraging a moral system where only those with specific connections/affiliations/ ideologies have access to potentially life-saving resources? AKA am I making a critical thinking error by seeking to apply a filter to a medical procedure?</p><ul><li>&nbsp;How would I even go about ensuring that a potential recipient truly shares the values of effective altruism? Is it even feasible or fair to attempt such a verification?</li><li>If I donate to someone within the EA community, is it fair to expect or assume that they will continue to uphold EA principles or contribute to the community in specific ways? I have read other posts about donors writing a letter to their recipient post-surgery, aiming to convince them of EA principles.&nbsp;</li></ul><p><strong>Legal and Ethical Complexities:</strong> Are there legal or ethical challenges associated with directed donation based on shared values, particularly when it involves a specific community like EA? I can imagine a doctor might assume pressure is being applied/ financial incentives are at play if a directed donation is attempted outside friends/family circles.&nbsp;</p><p>Given these concerns, I could understand why, from an ethical perspective, it would be better to proceed with an undirected donation through a standard registry, where the recipient is chosen based on medical need alone. &nbsp;Very undefined thought process however, hence all the questions :).&nbsp;</p><p>I\u2019d really appreciate any insights or advice from this community as I navigate this complex decision.</p><p>Thanks in advance for your thoughts!</p>",
    "base_score": 5,
    "comment_count": 1,
    "posted_at": "2025-04-03T23:45:32.104000Z",
    "created_at": "2025-08-21T23:40:16.930823Z",
    "author_id": "jBKyvERpDrtxRYq2f",
    "author_display_name": "Unwobblypanda",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "L6NqHZkLc4xZ7YtDr",
      "ZCihBFp5P64JCvQY6",
      "kddYsCBvCAm5Bcm2H",
      "uhJeftevWYofuogcC"
    ],
    "tag_names": [
      "Community",
      "Donation choice",
      "Effective giving",
      "Kidney donation"
    ],
    "markdown_content": "Hi everyone,\n\nI've been thinking deeply about donating one of my kidneys to a stranger, and I'd like to explore the possibility of donating it to someone within the EA community\u2014someone who shares core values with me.\n\nSo first and foremost; **is there an EA'er that is waiting for a kidney for themselves/a loved one ?**\n\nPerhaps there is not, which could end the discussion, but from a hypothetical standpoint, the following still interests me:\n\nThe idea of helping a fellow EA'er in such a profound way is compelling, both from a potential impact standpoint as well as in terms of personal fulfillment, \u00a0but it also raises a number of ethical and practical concerns that I have been considering. Any thoughts on any of the following topics would greatly be appreciated!\n\n**Equity and Fairness:** By prioritizing someone within our community, am I inadvertently encouraging a moral system where only those with specific connections/affiliations/ ideologies have access to potentially life-saving resources? AKA am I making a critical thinking error by seeking to apply a filter to a medical procedure?\n\n- How would I even go about ensuring that a potential recipient truly shares the values of effective altruism? Is it even feasible or fair to attempt such a verification?\n- If I donate to someone within the EA community, is it fair to expect or assume that they will continue to uphold EA principles or contribute to the community in specific ways? I have read other posts about donors writing a letter to their recipient post-surgery, aiming to convince them of EA principles.\n\n**Legal and Ethical Complexities:** Are there legal or ethical challenges associated with directed donation based on shared values, particularly when it involves a specific community like EA? I can imagine a doctor might assume pressure is being applied/ financial incentives are at play if a directed donation is attempted outside friends/family circles.\n\nGiven these concerns, I could understand why, from an ethical perspective, it would be better to proceed with an undirected donation through a standard registry, where the recipient is chosen based on medical need alone. \u00a0Very undefined thought process however, hence all the questions :).\n\nI\u2019d really appreciate any insights or advice from this community as I navigate this complex decision.\n\nThanks in advance for your thoughts!",
    "word_count": 381,
    "reading_time_minutes": 2,
    "external_links": null,
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T23:40:17.526666Z"
  },
  {
    "id": 1213,
    "post_id": "nktXYzoG2AYAcHToY",
    "title": "Open Philanthropy:\nEconomic Growth in Low- and Middle-Income Countries",
    "title_normalized": "open philanthropy economic growth in low and middleincome countries",
    "page_url": "https://forum.effectivealtruism.org/posts/nktXYzoG2AYAcHToY/open-philanthropy-economic-growth-in-low-and-middle-income",
    "html_body": "<p>A common objection to Effective Altruism is general, and global health and welfare programs in particular, is that it only looks at legible programs when the most important changes are at a system level, and especially ones that encourage economic development. I\u2019m curious what critics who agree with this perspective think about OP\u2019s new program area.</p>\n",
    "base_score": 20,
    "comment_count": 1,
    "posted_at": "2025-01-19T13:07:22.596000Z",
    "created_at": "2025-08-21T18:22:03.641468Z",
    "author_id": "QhJ8h7dJcpvqDeJFf",
    "author_display_name": "Ian Turner",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "ZiKs8Bbvse4DMcEPr",
      "bY6qtCihTzzSCw9gc",
      "sWcuTyTB5dP3nas2t",
      "toct8RYvTvgontEts",
      "uxoyvActFbMwEcGqF",
      "xEnFbH3FjEzXhinGc"
    ],
    "tag_names": [
      "Announcements and updates",
      "Economic growth",
      "Global health & development",
      "Low- and middle-income countries",
      "News relevant to effective altruism",
      "Open Philanthropy"
    ],
    "markdown_content": "A common objection to Effective Altruism is general, and global health and welfare programs in particular, is that it only looks at legible programs when the most important changes are at a system level, and especially ones that encourage economic development. I\u2019m curious what critics who agree with this perspective think about OP\u2019s new program area.",
    "word_count": 56,
    "reading_time_minutes": 1,
    "external_links": null,
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T18:22:03.808847Z"
  },
  {
    "id": 676,
    "post_id": "PBFXXSNZDjunATzaq",
    "title": "Information-Dense Conference Badges",
    "title_normalized": "informationdense conference badges",
    "page_url": "https://forum.effectivealtruism.org/posts/PBFXXSNZDjunATzaq/information-dense-conference-badges",
    "html_body": "<p><i>See previous discussion </i><a href=\"https://www.facebook.com/ozzie.gooen/posts/pfbid02q5vwrzghAkgZtt8QBZpa2YBA9msVeDj5b6sdf1RDLiZvYhitP4dmo1MDxc19yoSHl\"><i>here</i></a><i>.</i></p><p>I find a lot of professional events fairly soul-crushing and have been thinking about why.</p><p>I dislike small talk. Recently I attended <a href=\"https://manifest.is/\"><u>Manifest</u></a>, and noticed that it could easily take 10 minutes of conversation to learn the very basics about a person. There were hundreds of people at the conference, so meeting people felt expensive and haphazard.</p><p>My regular time is costly, and conference time should be even more expensive. It seems reasonable to aim for $50 to $300 of value per hour. So if I spend 10 minutes with someone only to find out we don't have much in common, that can easily be $20 to $100 lost (total, between both parties). Add the unpleasantness of slowly losing my voice and being in a crowded atmosphere. I enjoy quiet spaces and my energy reserves quickly deplete in crowded and loud settings.</p><p>The badges were incredibly basic. Most people's fashion choices were similarly basic. So there's little to go off of. You sort of have to start from scratch for each interaction.</p><p>After the first day I got the idea to use a small <a href=\"https://www.amazon.com/dp/B0BCW4YMR8?ref_=ppx_hzsearch_conn_dt_b_fed_asin_title_1\"><u>sticker machine</u></a> to at least post some basics about me to my badge.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/g27yrtnexdb5c99feup5\" alt=\"\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/hvhhnfcrh066vxvnqvns 424w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/rpu39klce3gdgkhofduw 848w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/bmtvd8q1jsfgas5ohasx 1272w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/g27yrtnexdb5c99feup5 1456w\"></figure><p>&nbsp;</p><p>If you think about it, conference badges are an interesting design opportunity. We could probably get away with larger options than normal\u2014maybe even 10\" by 8\" or similar. This gives a lot of space to work with. They can feature quite a bit of written and visual information.</p><p>I went back-and-forth with ChatGPT to design a few other options. These are obviously rough and imperfect, but I think they suggest some interesting directions. I'd naively expect events to maybe be 5% more pleasant and useful for me with badges like these. (See the end of this post for the mockups)</p><h3><strong>Practical Considerations</strong></h3><p>The obvious challenge is cost. Printing fully custom badges for each attendee would run around $40 per badge, plus design time and iteration. For a 200-person conference, that's $8,000+ just for badges alone.</p><p>The most practical approach is probably a hybrid system. Give people 3-20 pre-designed badge templates to choose from, then set up a sticker station with a curated selection of options. Recently I went to get a library card and was given 5 different design options. Doing this for event badges seems doable and more practical.</p><p>The tricky part isn't the printing\u2014it's sourcing and organizing the stickers. I could easily imagine wanting 50 to 500 different options covering interests, skills, organizations, causes, personality traits, etc. Someone would need to buy all those stickers, sort them, and figure out a reasonable organization system that doesn't create chaos. It might need to handle 6-20 people at once, so require several setups.</p><p>A cheaper middle ground would be large overlay stickers that cover the bottom two-thirds of standard badges. These could include 6-10 pre-selected icons or text snippets for maybe $0.50 each. The main technical hurdle here is building some kind of digital interface where people can select their preferences ahead of time\u2014basically a simple badge customization tool that outputs print files.</p><p>I'm also drawn to the idea of persistent badges that people own and use across multiple conferences. Security concerns come up immediately, but they seem solvable. You could wear the official conference badge underneath your personal one, or have it hang lower so it's still visible. Or maybe the \"<i>conference credential</i>\" is just a small sticker that goes on your persistent badge. This approach would let people invest more in their badge design since they'd use it repeatedly.</p><p>It would be neat to make certain elements unlockable. Imagine stickers you can only get by working at certain organizations, publishing well-regarded research, or hitting specific donation thresholds. The implementation might be straightforward\u2014just mail qualifying people a few sheets of whatever they've earned to use at future events.</p><p>I realize that any of these options adds complexity for organizers and attendees alike. But I think the tradeoffs could work with some thoughtful design. The coordination overhead isn't trivial, but it's still probably cheaper than things like branded hoodies.</p><h2><strong>Addendum 1: Website Profiles</strong></h2><p>One obvious question this brings up is, <i>\"Can we do this for digital profiles? Why are those so standardized?\"</i></p><p>User pages on websites such as LinkedIn, Facebook, LessWrong, and the EA Forum are all fairly plain and standardized.</p><p>Some of this represents best practice. But I suspect some also has to do with practical challenges online. MySpace gave people much more customization, and a lot of people liked this, but it came with a great deal of complexity that likely wasn't worth it.</p><p>I think that LessWrong and the EA Forum would be better with more complex badge and profile systems, though I realize this would be a lot of work.</p><h2><strong>Example Potential Designs</strong></h2><p>Here are a few options generated using ChatGPT. They have clear defects, but I think get at the high-level point of what I\u2019m going for.</p><p>One important thing is that they\u2019re meant to try to convey certain vibes. I think different people should want to give off certain vibes, and I\u2019d expect that events would be more engaging and effective if this could be done.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/yapanv3hb7zxlqsgk1dv\" alt=\"May be an image of text that says 'Oliver Campbell A alignment researcher ! 80% TIMILINES: 2030 INTERESTS LOCATION Berkeley, CA AFFILIATIONS MIRI, AGI Safety Decision Theory Mathematics Existential Risk AI DOOMER CATEGORY THEORIST RESILIENCE LIOHTHAVEN GOLD DONOR CATEGORY THEORIST LIGHTHAVEN GOLD GOLDDONOR DONOR'\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/lmdozje774p8e6tg7k0b 424w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/jps3vgndevx2hu8gszjd 848w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/vj7oiq77u2pykzmohq5s 1272w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/yapanv3hb7zxlqsgk1dv 1456w\"></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/mejhz1gtu6l9medkw39i\" alt=\"May be a graphic of text that says 'Emily Kim A Systems Evaluator Interests Safety Rationality SF&amp;F Organizations MIRI CFAR Probability of Doom 50-% 10-20 20+ EFFECTIVE MEMER ACCELERATE ERATE Al Senior DOOMER Researcher I Category FANTASY Theorist DEBATES BERKELEY AIAG FANTASY DERATES'\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/jvrc7knt7e5dl4pwor9j 424w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/ofg1kfyxwfbqqxvas1ej 848w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/fqdef6c3snl9y2fw5tfp 1272w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/mejhz1gtu6l9medkw39i 1456w\"></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/rejlohobh9pb8vxvtxjc\" alt=\"May be a doodle of text that says 'DR. AISHA REYES EA Macrostrategy Researcher Moral Uncertainty Core Timelines:15% pDoom:15% Global Health &amp; Dev Governance Animal Welfare EA Macrostrategy \u5206 80k Career Coach Alum X-r\u0131isk Strategy X-risk Strategy Climate Coordination Cause Neutrality JE Cause Neutrality View Full Portfolio'\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/lwumobk76ittuqg5uuyo 424w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/tui1hivgkballc8adksa 848w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/cu0u2lncoa5owpmpkcsp 1272w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/rejlohobh9pb8vxvtxjc 1456w\"></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/lxlnkfqxcf57ovvv5azu\" alt=\"May be an image of text that says 'Dr. Max Hanson ALIGNMENT ER RESEARCHER San Francisco, CA CFAR, MIRI pDoom:10% 10% Timelines: Uncertain SHARD THEORIST EXPERIMENTALIST \u5340 ACAUSAL ALIGNMENT SHARD THEORIST WEIRD'\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/rqafskngs7yrs9ynr2sq 424w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/p4bw2fh27hym3v8jyarj 848w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/shahizjynxbim90ujlih 1272w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/lxlnkfqxcf57ovvv5azu 1456w\"></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/ibouk4ilyhtgqv72j5vx\" alt=\"May be an image of text that says 'CHRIS EVANS Shrimp Welfare Researcher Toronto\u2022 Soccer Gardening pDoom 5% Timelines: 2100 SHRIMP Animal Welfare Marine Biologist PLANTS!'\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/b0jlhednc2m1vvl8oamv 424w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/haziharbquejdexswaoz 848w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/oxurkyugvjpq27j6tkry 1272w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/ibouk4ilyhtgqv72j5vx 1456w\"></figure><p>&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/zvgao0mnytagvj6joet5\" alt=\"May be an image of \u200etext that says '\u200eJOSEPH M. AI A STRATEGY FUNDER pDoom: 2% | Timelines: 2045 DONATED TO: GLOBAL HEALTH &amp; DEVELOPMENT \u5e73 EARNING TO GIVE \u05e6\u05d9 FOREST RESTORATION \u0645\u0646\u0631 REGULATION MATTERS INSEAD AI G LOWER IMPACT IM FUND b bayesian GIVE WHAT WE CAN $ EARNING TO GIVE REGULATION MATTERS\u200e'\u200e\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/dpeymdonsvc9x8srwvkx 424w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/ukczdv4ers4npzd7snfu 848w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/evlobtwvwan1jtuup7nk 1272w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/zvgao0mnytagvj6joet5 1456w\"></figure>",
    "base_score": 49,
    "comment_count": 4,
    "posted_at": "2025-06-13T17:52:23.722000Z",
    "created_at": "2025-08-21T09:10:16.012008Z",
    "author_id": "HfGQ6MkoBi4ZfKRpL",
    "author_display_name": "Ozzie Gooen",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "EHLmbEmJ2Qd5WfwTb",
      "WuDHK7wyXKCjzwjpA",
      "ZCihBFp5P64JCvQY6",
      "zJv36ZGjWDSwXDEiT"
    ],
    "tag_names": [
      "Building effective altruism",
      "Community",
      "Conferences",
      "Event strategy"
    ],
    "markdown_content": "*See previous discussion* [*here*](https://www.facebook.com/ozzie.gooen/posts/pfbid02q5vwrzghAkgZtt8QBZpa2YBA9msVeDj5b6sdf1RDLiZvYhitP4dmo1MDxc19yoSHl)*.*\n\nI find a lot of professional events fairly soul-crushing and have been thinking about why.\n\nI dislike small talk. Recently I attended [Manifest](https://manifest.is/), and noticed that it could easily take 10 minutes of conversation to learn the very basics about a person. There were hundreds of people at the conference, so meeting people felt expensive and haphazard.\n\nMy regular time is costly, and conference time should be even more expensive. It seems reasonable to aim for $50 to $300 of value per hour. So if I spend 10 minutes with someone only to find out we don't have much in common, that can easily be $20 to $100 lost (total, between both parties). Add the unpleasantness of slowly losing my voice and being in a crowded atmosphere. I enjoy quiet spaces and my energy reserves quickly deplete in crowded and loud settings.\n\nThe badges were incredibly basic. Most people's fashion choices were similarly basic. So there's little to go off of. You sort of have to start from scratch for each interaction.\n\nAfter the first day I got the idea to use a small [sticker machine](https://www.amazon.com/dp/B0BCW4YMR8?ref_=ppx_hzsearch_conn_dt_b_fed_asin_title_1) to at least post some basics about me to my badge.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/g27yrtnexdb5c99feup5)\n\nIf you think about it, conference badges are an interesting design opportunity. We could probably get away with larger options than normal\u2014maybe even 10\" by 8\" or similar. This gives a lot of space to work with. They can feature quite a bit of written and visual information.\n\nI went back-and-forth with ChatGPT to design a few other options. These are obviously rough and imperfect, but I think they suggest some interesting directions. I'd naively expect events to maybe be 5% more pleasant and useful for me with badges like these. (See the end of this post for the mockups)\n\n### **Practical Considerations**\n\nThe obvious challenge is cost. Printing fully custom badges for each attendee would run around $40 per badge, plus design time and iteration. For a 200-person conference, that's $8,000+ just for badges alone.\n\nThe most practical approach is probably a hybrid system. Give people 3-20 pre-designed badge templates to choose from, then set up a sticker station with a curated selection of options. Recently I went to get a library card and was given 5 different design options. Doing this for event badges seems doable and more practical.\n\nThe tricky part isn't the printing\u2014it's sourcing and organizing the stickers. I could easily imagine wanting 50 to 500 different options covering interests, skills, organizations, causes, personality traits, etc. Someone would need to buy all those stickers, sort them, and figure out a reasonable organization system that doesn't create chaos. It might need to handle 6-20 people at once, so require several setups.\n\nA cheaper middle ground would be large overlay stickers that cover the bottom two-thirds of standard badges. These could include 6-10 pre-selected icons or text snippets for maybe $0.50 each. The main technical hurdle here is building some kind of digital interface where people can select their preferences ahead of time\u2014basically a simple badge customization tool that outputs print files.\n\nI'm also drawn to the idea of persistent badges that people own and use across multiple conferences. Security concerns come up immediately, but they seem solvable. You could wear the official conference badge underneath your personal one, or have it hang lower so it's still visible. Or maybe the \"*conference credential*\" is just a small sticker that goes on your persistent badge. This approach would let people invest more in their badge design since they'd use it repeatedly.\n\nIt would be neat to make certain elements unlockable. Imagine stickers you can only get by working at certain organizations, publishing well-regarded research, or hitting specific donation thresholds. The implementation might be straightforward\u2014just mail qualifying people a few sheets of whatever they've earned to use at future events.\n\nI realize that any of these options adds complexity for organizers and attendees alike. But I think the tradeoffs could work with some thoughtful design. The coordination overhead isn't trivial, but it's still probably cheaper than things like branded hoodies.\n\n## **Addendum 1: Website Profiles**\n\nOne obvious question this brings up is, *\"Can we do this for digital profiles? Why are those so standardized?\"*\n\nUser pages on websites such as LinkedIn, Facebook, LessWrong, and the EA Forum are all fairly plain and standardized.\n\nSome of this represents best practice. But I suspect some also has to do with practical challenges online. MySpace gave people much more customization, and a lot of people liked this, but it came with a great deal of complexity that likely wasn't worth it.\n\nI think that LessWrong and the EA Forum would be better with more complex badge and profile systems, though I realize this would be a lot of work.\n\n## **Example Potential Designs**\n\nHere are a few options generated using ChatGPT. They have clear defects, but I think get at the high-level point of what I\u2019m going for.\n\nOne important thing is that they\u2019re meant to try to convey certain vibes. I think different people should want to give off certain vibes, and I\u2019d expect that events would be more engaging and effective if this could be done.\n\n![May be an image of text that says 'Oliver Campbell A alignment researcher ! 80% TIMILINES: 2030 INTERESTS LOCATION Berkeley, CA AFFILIATIONS MIRI, AGI Safety Decision Theory Mathematics Existential Risk AI DOOMER CATEGORY THEORIST RESILIENCE LIOHTHAVEN GOLD DONOR CATEGORY THEORIST LIGHTHAVEN GOLD GOLDDONOR DONOR'](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/yapanv3hb7zxlqsgk1dv)![May be a graphic of text that says 'Emily Kim A Systems Evaluator Interests Safety Rationality SF&F Organizations MIRI CFAR Probability of Doom 50-% 10-20 20+ EFFECTIVE MEMER ACCELERATE ERATE Al Senior DOOMER Researcher I Category FANTASY Theorist DEBATES BERKELEY AIAG FANTASY DERATES'](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/mejhz1gtu6l9medkw39i)![May be a doodle of text that says 'DR. AISHA REYES EA Macrostrategy Researcher Moral Uncertainty Core Timelines:15% pDoom:15% Global Health & Dev Governance Animal Welfare EA Macrostrategy \u5206 80k Career Coach Alum X-r\u0131isk Strategy X-risk Strategy Climate Coordination Cause Neutrality JE Cause Neutrality View Full Portfolio'](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/rejlohobh9pb8vxvtxjc)![May be an image of text that says 'Dr. Max Hanson ALIGNMENT ER RESEARCHER San Francisco, CA CFAR, MIRI pDoom:10% 10% Timelines: Uncertain SHARD THEORIST EXPERIMENTALIST \u5340 ACAUSAL ALIGNMENT SHARD THEORIST WEIRD'](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/lxlnkfqxcf57ovvv5azu)![May be an image of text that says 'CHRIS EVANS Shrimp Welfare Researcher Toronto\u2022 Soccer Gardening pDoom 5% Timelines: 2100 SHRIMP Animal Welfare Marine Biologist PLANTS!'](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/ibouk4ilyhtgqv72j5vx)![May be an image of \u200etext that says '\u200eJOSEPH M. AI A STRATEGY FUNDER pDoom: 2% | Timelines: 2045 DONATED TO: GLOBAL HEALTH & DEVELOPMENT \u5e73 EARNING TO GIVE \u05e6\u05d9 FOREST RESTORATION \u0645\u0646\u0631 REGULATION MATTERS INSEAD AI G LOWER IMPACT IM FUND b bayesian GIVE WHAT WE CAN $ EARNING TO GIVE REGULATION MATTERS\u200e'\u200e](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/PBFXXSNZDjunATzaq/zvgao0mnytagvj6joet5)",
    "word_count": 1108,
    "reading_time_minutes": 6,
    "external_links": [
      "https://manifest.is/",
      "https://www.amazon.com/dp/B0BCW4YMR8?ref_=ppx_hzsearch_conn_dt_b_fed_asin_title_1",
      "https://www.facebook.com/ozzie.gooen/posts/pfbid02q5vwrzghAkgZtt8QBZpa2YBA9msVeDj5b6sdf1RDLiZvYhitP4dmo1MDxc19yoSHl"
    ],
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T09:10:18.774442Z"
  },
  {
    "id": 1214,
    "post_id": "RdbDH4T8bxWwZpc9h",
    "title": "GiveWell raised less than its 10th percentile forecast in 2023",
    "title_normalized": "givewell raised less than its 10th percentile forecast in 2023",
    "page_url": "https://forum.effectivealtruism.org/posts/RdbDH4T8bxWwZpc9h/givewell-raised-less-than-its-10th-percentile-forecast-in",
    "html_body": "<p>In 2023<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"4jufv4w82ck\" role=\"doc-noteref\" id=\"fnref4jufv4w82ck\"><sup><a href=\"#fn4jufv4w82ck\">[1]</a></sup></span>&nbsp;GiveWell <a href=\"https://blog.givewell.org/2024/12/12/givewells-fundraising-and-grantmaking-in-2023/\">raised $355 million</a> - $100 million from Open Philanthropy, and $255 million from other donors.</p><p>In <a href=\"https://blog.givewell.org/2023/04/10/expected-funds-raised-through-2025/\">their post on 10th April 2023</a>, GiveWell forecast the amount they expected to raise in 2023, albeit with wide confidence intervals, and stated that their 10th percentile estimate for total funds raised was $416 million, and 10th percentile estimate for funds raised outside of Open Philanthropy was $260 million.</p><figure class=\"table\"><table><tbody><tr><td>&nbsp;</td><td>10th percentile estimate</td><td>Median estimate</td><td><strong>Amount raised</strong></td></tr><tr><td><strong>Total</strong></td><td>$416 million</td><td>$581 million</td><td>$355 million</td></tr><tr><td><strong>Excluding Open Philanthropy</strong></td><td>$260 million</td><td>$330 million</td><td>$255 million</td></tr></tbody></table></figure><p>Regarding Open Philanthropy, the April 2023 post states that they \"tentatively plans to give $250 million in 2023\", however <a href=\"https://www.openphilanthropy.org/research/our-planned-allocation-to-givewells-recommendations-for-the-next-few-years/#id-how-givewells-impressive-growth-and-our-higher-global-health-and-wellbeing-bar-influence-our-current-plans\">Open Philanthropy gave a grant of $300 million to cover 2023-2025</a>, <a href=\"https://blog.givewell.org/2023/10/06/open-philanthropys-2023-2025-funding-for-givewells-recommendations/\">to be split however GiveWell saw fit</a>, and it used $100 million of that grant in 2023.</p><p>However for other donors I'm not sure what caused the missed estimate</p><p><i>Credit to 'Arnold' on </i><a href=\"https://blog.givewell.org/2024/12/03/december-2024-open-thread/#comment-968008\"><i>GiveWell's December 2024 Open Thread</i></a><i> for bringing this to my attention</i></p><p>&nbsp;</p><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"4jufv4w82ck\" role=\"doc-endnote\" id=\"fn4jufv4w82ck\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"4jufv4w82ck\"><sup><strong><a href=\"#fnref4jufv4w82ck\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>1st February 2023 - 31st January 2024</p></div></li></ol>",
    "base_score": 138,
    "comment_count": 10,
    "posted_at": "2025-01-19T10:27:01.487000Z",
    "created_at": "2025-08-21T18:22:19.346017Z",
    "author_id": "snfD2eCdvY7hksPWk",
    "author_display_name": "Rasool",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "L6NqHZkLc4xZ7YtDr",
      "RRcLA4Kn8pasimQZr",
      "aJnrnnobcBNWRsfAw",
      "sWcuTyTB5dP3nas2t"
    ],
    "tag_names": [
      "Effective giving",
      "Forecasting",
      "GiveWell",
      "Global health & development"
    ],
    "markdown_content": "In 2023[[1]](#fn4jufv4w82ck)\u00a0GiveWell [raised $355 million](https://blog.givewell.org/2024/12/12/givewells-fundraising-and-grantmaking-in-2023/) - $100 million from Open Philanthropy, and $255 million from other donors.\n\nIn [their post on 10th April 2023](https://blog.givewell.org/2023/04/10/expected-funds-raised-through-2025/), GiveWell forecast the amount they expected to raise in 2023, albeit with wide confidence intervals, and stated that their 10th percentile estimate for total funds raised was $416 million, and 10th percentile estimate for funds raised outside of Open Philanthropy was $260 million.\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n|  | 10th percentile estimate | Median estimate | **Amount raised** |\n| **Total** | $416 million | $581 million | $355 million |\n| **Excluding Open Philanthropy** | $260 million | $330 million | $255 million |\n\nRegarding Open Philanthropy, the April 2023 post states that they \"tentatively plans to give $250 million in 2023\", however [Open Philanthropy gave a grant of $300 million to cover 2023-2025](https://www.openphilanthropy.org/research/our-planned-allocation-to-givewells-recommendations-for-the-next-few-years/#id-how-givewells-impressive-growth-and-our-higher-global-health-and-wellbeing-bar-influence-our-current-plans), [to be split however GiveWell saw fit](https://blog.givewell.org/2023/10/06/open-philanthropys-2023-2025-funding-for-givewells-recommendations/), and it used $100 million of that grant in 2023.\n\nHowever for other donors I'm not sure what caused the missed estimate\n\n*Credit to 'Arnold' on* [*GiveWell's December 2024 Open Thread*](https://blog.givewell.org/2024/12/03/december-2024-open-thread/#comment-968008) *for bringing this to my attention*\n\n1. **[^](#fnref4jufv4w82ck)**\n\n   1st February 2023 - 31st January 2024",
    "word_count": 203,
    "reading_time_minutes": 1,
    "external_links": [
      "https://blog.givewell.org/2023/04/10/expected-funds-raised-through-2025/",
      "https://blog.givewell.org/2023/10/06/open-philanthropys-2023-2025-funding-for-givewells-recommendations/",
      "https://blog.givewell.org/2024/12/03/december-2024-open-thread/#comment-968008",
      "https://blog.givewell.org/2024/12/12/givewells-fundraising-and-grantmaking-in-2023/",
      "https://www.openphilanthropy.org/research/our-planned-allocation-to-givewells-recommendations-for-the-next-few-years/#id-how-givewells-impressive-growth-and-our-higher-global-health-and-wellbeing-bar-influence-our-current-plans"
    ],
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T18:22:19.495919Z"
  },
  {
    "id": 2137,
    "post_id": "qQvQfayazeNCqZzDT",
    "title": "How We Learned to Write Better Research Papers",
    "title_normalized": "how we learned to write better research papers",
    "page_url": "https://forum.effectivealtruism.org/posts/qQvQfayazeNCqZzDT/how-we-learned-to-write-better-research-papers",
    "html_body": "<p>Earlier this year, a few of us got together \u2014 students, early-career researchers, and tech enthusiasts \u2014 with a shared goal: to get better at writing academic papers. Not necessarily to publish in <i>Nature</i>, but to stop feeling like every paper was a chaotic mess of half-baked ideas, citation confusion, and last-minute formatting disasters.</p><p>So we set up a \u201cwriting practice group.\u201d<br>The idea was simple: we would focus on <strong>one small writing skill at a time</strong>, write short pieces around a specific question, and get quick feedback from peers and from AI tools.</p><p>The results were surprising.</p><h2>Why Practice Academic Writing (Deliberately)?</h2><p>Most of us had read dozens of research papers \u2014 and yet, writing one felt like fumbling in the dark. Structuring arguments, reviewing literature effectively, and explaining methods clearly are all skills, but no one had ever really taught us how to <i>practice</i> them.</p><p>Our goal wasn\u2019t to churn out publishable articles every week. It was to develop <strong>a writing intuition</strong> \u2014 to recognize what makes a paragraph clear or confusing, a section too shallow or too dense, a flow too abrupt or too vague.</p><p>So we started treating academic writing like a skill that could be trained \u2014 like coding, sports, or music.</p><h2>Our Writing Sprint Format</h2><p>Each week, we\u2019d pick one small writing challenge and go through this process:</p><ol><li><strong>Choose a focused topic or question</strong><br>E.g., <i>What makes an AI-generated paragraph feel \u201coff\u201d?</i><br>Or, <i>How can I write a literature review that doesn\u2019t feel like filler?</i></li><li><strong>Write a short draft in 40\u201360 minutes</strong><br>We often used tools like Scifocus to generate outlines or jumpstart ideas (more on that below).</li><li><strong>Give each other feedback (quick &amp; honest)</strong><br>Everyone gave 1\u20132 practical suggestions to improve each piece. No fluff, just what helped.</li><li><strong>Group reflection</strong><br>We shared what felt hard, what worked well, and how we\u2019d revise next time.</li></ol><p>This rhythm made writing feel less daunting \u2014 more like debugging a piece of code than delivering a final product.</p><h2>A Few Habits That Actually Worked</h2><p>Here are some specific habits that stuck with us and consistently improved our writing:</p><h3>1. Start with a structure, not a sentence</h3><p>Before writing, we sketched the logical flow of the argument \u2014 either on paper or using <strong>AI Outline Generator</strong>, which helped us visualize the typical structure:<br><strong>Intro \u2192 Background \u2192 Gap \u2192 Method \u2192 Result \u2192 Implication.</strong></p><p>Even if we didn\u2019t follow it rigidly, having a scaffold made writing <i>much</i> faster and easier to revise.</p><h3>2. Over-collect, then filter your sources</h3><p>Instead of trying to find the \u201cperfect\u201d 5 sources while writing, we\u2019d spend 15\u201320 minutes up front collecting 20+ potentially relevant papers. Then we\u2019d scan abstracts, highlight key phrases, and choose what to keep.</p><p><strong>Tools we used:</strong></p><ul><li><a href=\"https://scholar.google.com/\">Google Scholar</a></li><li><a href=\"https://www.scifocus.ai/\">Scifocus Literature Search Assistant</a> \u2013 helpful for quickly generating summaries of each paper so we could scan more in less time.</li></ul><h3>3. Make your argument \u201cwalk\u201d</h3><p>One method we practiced: after writing a paragraph, summarize it in one sentence. If you couldn\u2019t, the logic was probably muddy. This helped us catch fluff and redundancy early.</p><p>Scifocus\u2019s <strong>Essay Checker</strong> also helped here \u2014 flagging unclear transitions and paragraphs that didn\u2019t align with the rest of the paper.</p><h2>Want to Try This Yourself?</h2><p>If you\u2019re a student or early researcher who wants to build writing confidence, here\u2019s what we\u2019d suggest:</p><ol><li>Find 2\u20134 friends and form a weekly writing group</li><li>Pick <i>one</i> question or skill to focus on each time</li><li>Write short \u2014 300\u2013500 words is plenty</li><li>Give clear, useful feedback \u2014 focus on clarity and logic</li><li>Use tools like Scifocus or Connected Papers to save time and boost feedback quality</li></ol><p>Most importantly: <strong>treat writing like a learnable process</strong> \u2014 not a mysterious art form reserved for \u201cgood writers.\u201d</p><p>&nbsp;</p><h2>Final Thoughts</h2><p>We didn\u2019t become research rockstars overnight. But we did start writing faster, revising more clearly, and \u2014 maybe most importantly \u2014 feeling more in control of the process.</p><p>If writing papers currently feels overwhelming, you\u2019re not alone. But it doesn\u2019t have to stay that way.<br>Structure, feedback, iteration, and the right tools can go a long way.</p>",
    "base_score": -2,
    "comment_count": 1,
    "posted_at": "2025-04-10T14:25:58.276000Z",
    "created_at": "2025-08-21T23:31:19.979297Z",
    "author_id": "9fajckYBQhvHghDHG",
    "author_display_name": "Connor Wood",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "4dikNJgjJRYB66Eck",
      "mxyrHDrmiYq3MR6oD"
    ],
    "tag_names": [
      "Productivity",
      "Writing advice"
    ],
    "markdown_content": "Earlier this year, a few of us got together \u2014 students, early-career researchers, and tech enthusiasts \u2014 with a shared goal: to get better at writing academic papers. Not necessarily to publish in *Nature*, but to stop feeling like every paper was a chaotic mess of half-baked ideas, citation confusion, and last-minute formatting disasters.\n\nSo we set up a \u201cwriting practice group.\u201d  \nThe idea was simple: we would focus on **one small writing skill at a time**, write short pieces around a specific question, and get quick feedback from peers and from AI tools.\n\nThe results were surprising.\n\n## Why Practice Academic Writing (Deliberately)?\n\nMost of us had read dozens of research papers \u2014 and yet, writing one felt like fumbling in the dark. Structuring arguments, reviewing literature effectively, and explaining methods clearly are all skills, but no one had ever really taught us how to *practice* them.\n\nOur goal wasn\u2019t to churn out publishable articles every week. It was to develop **a writing intuition** \u2014 to recognize what makes a paragraph clear or confusing, a section too shallow or too dense, a flow too abrupt or too vague.\n\nSo we started treating academic writing like a skill that could be trained \u2014 like coding, sports, or music.\n\n## Our Writing Sprint Format\n\nEach week, we\u2019d pick one small writing challenge and go through this process:\n\n1. **Choose a focused topic or question**  \n   E.g., *What makes an AI-generated paragraph feel \u201coff\u201d?*  \n   Or, *How can I write a literature review that doesn\u2019t feel like filler?*\n2. **Write a short draft in 40\u201360 minutes**  \n   We often used tools like Scifocus to generate outlines or jumpstart ideas (more on that below).\n3. **Give each other feedback (quick & honest)**  \n   Everyone gave 1\u20132 practical suggestions to improve each piece. No fluff, just what helped.\n4. **Group reflection**  \n   We shared what felt hard, what worked well, and how we\u2019d revise next time.\n\nThis rhythm made writing feel less daunting \u2014 more like debugging a piece of code than delivering a final product.\n\n## A Few Habits That Actually Worked\n\nHere are some specific habits that stuck with us and consistently improved our writing:\n\n### 1. Start with a structure, not a sentence\n\nBefore writing, we sketched the logical flow of the argument \u2014 either on paper or using **AI Outline Generator**, which helped us visualize the typical structure:  \n**Intro \u2192 Background \u2192 Gap \u2192 Method \u2192 Result \u2192 Implication.**\n\nEven if we didn\u2019t follow it rigidly, having a scaffold made writing *much* faster and easier to revise.\n\n### 2. Over-collect, then filter your sources\n\nInstead of trying to find the \u201cperfect\u201d 5 sources while writing, we\u2019d spend 15\u201320 minutes up front collecting 20+ potentially relevant papers. Then we\u2019d scan abstracts, highlight key phrases, and choose what to keep.\n\n**Tools we used:**\n\n- [Google Scholar](https://scholar.google.com/)\n- [Scifocus Literature Search Assistant](https://www.scifocus.ai/) \u2013 helpful for quickly generating summaries of each paper so we could scan more in less time.\n\n### 3. Make your argument \u201cwalk\u201d\n\nOne method we practiced: after writing a paragraph, summarize it in one sentence. If you couldn\u2019t, the logic was probably muddy. This helped us catch fluff and redundancy early.\n\nScifocus\u2019s **Essay Checker** also helped here \u2014 flagging unclear transitions and paragraphs that didn\u2019t align with the rest of the paper.\n\n## Want to Try This Yourself?\n\nIf you\u2019re a student or early researcher who wants to build writing confidence, here\u2019s what we\u2019d suggest:\n\n1. Find 2\u20134 friends and form a weekly writing group\n2. Pick *one* question or skill to focus on each time\n3. Write short \u2014 300\u2013500 words is plenty\n4. Give clear, useful feedback \u2014 focus on clarity and logic\n5. Use tools like Scifocus or Connected Papers to save time and boost feedback quality\n\nMost importantly: **treat writing like a learnable process** \u2014 not a mysterious art form reserved for \u201cgood writers.\u201d\n\n## Final Thoughts\n\nWe didn\u2019t become research rockstars overnight. But we did start writing faster, revising more clearly, and \u2014 maybe most importantly \u2014 feeling more in control of the process.\n\nIf writing papers currently feels overwhelming, you\u2019re not alone. But it doesn\u2019t have to stay that way.  \nStructure, feedback, iteration, and the right tools can go a long way.",
    "word_count": 703,
    "reading_time_minutes": 4,
    "external_links": [
      "https://scholar.google.com/",
      "https://www.scifocus.ai/"
    ],
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T23:31:20.563202Z"
  },
  {
    "id": 478,
    "post_id": "rdc69nDvgPfK3d3bw",
    "title": "Rational Animations' video about scalable oversight and sandwiching",
    "title_normalized": "rational animations video about scalable oversight and sandwiching",
    "page_url": "https://forum.effectivealtruism.org/posts/rdc69nDvgPfK3d3bw/rational-animations-video-about-scalable-oversight-and",
    "html_body": "<figure class=\"media\"><div data-oembed-url=\"https://youtu.be/5mco9zAamRk\"><div><iframe src=\"https://www.youtube.com/embed/5mco9zAamRk\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p><i>In the future, AIs will likely be much smarter than we are. They'll produce outputs that may be difficult for humans to evaluate, either because evaluation is too labor-intensive, or because it's qualitatively hard to judge the actions of machines smarter than us. This is the problem of \u201cscalable oversight.\u201d Proposed solutions include \u201cdebate\u201d and iterated amplification. But how can we run experiments today to see whether these ideas actually work in practice?</i></p><p><i>In this video, we cover Ajeya Cotra\u2019s \u201csandwiching\u201d proposal: asking non-experts to align a model that is smarter than they are but less smart than a group of experts, and seeing how well they do. We then show how Sam Bowman et al. tested a basic version of this idea in their paper \u201cMeasuring Progress on Scalable Oversight for Large Language Models.\u201d</i></p><p><i>This video has been written by </i><a href=\"https://www.lesswrong.com/users/markovial?mention=user\"><i>@markov</i></a><i>, John Burden, and me.</i></p><p><i>You can find the script below.</i></p><hr><h2>How do we oversee AIs that are smarter than us?</h2><p>AI systems are getting more capable at a rapid pace. &nbsp;In our previous videos, we talked about how developers &nbsp;are using a technique called reinforcement learning from human feedback, or RLHF, to try to align AI systems to our preferences using human oversight. .</p><p>This might work for now, but as the tasks that we need to oversee keep getting increasingly complex, and AI systems increasingly smart, it\u2019s difficult for humans to remain good supervisors. This is the core problem of \u2018scalable oversight\u2019.</p><p>In this video, we\u2019ll &nbsp;walk through two related parts of scalable oversight. They aren\u2019t really that separable in practice, but explaining them independently helps with understanding them.</p><p>The first part is, we need to come up with potential ways to solve the problem. Researchers have tested feedback methods that work pretty well right now on tasks that are already hard to evaluate.&nbsp;<br><br>The second part is, what happens when AI\u2019s are better than humans at every task? Can we still effectively supervise them? This is hard to figure out &nbsp;before we have such AIs, but in this video we\u2019ll showcase a technique that might &nbsp;make it possible!&nbsp;</p><h2>We have theoretical scalable oversight techniques</h2><p>Let's start by diving into the first problem.</p><p>In the last few years, researchers have come up with a few proposals for attempting to solve scalable oversight.</p><p>While working on providing human evaluations for AI generated texts, OpenAI experimented with some scalable oversight proposals that extended existing RLHF methods. Their general idea was to augment human oversight using AI assistants trained on an easier version of the problem.</p><p>In 2020, they trained AIs to generate summaries of texts. Effectively, &nbsp;TL;DRs of reddit posts. Human evaluators would then give feedback on how useful or accurate the generated summaries were. These summaries were still relatively easy to evaluate, because if needed, it was feasible for an evaluator to just read the entire original text and check the accuracy of the summary. But the point is that these techniques need to scale! What if the overseers don't have the ability or the time to read the full text?</p><p>To make progress on this problem, in 2021, OpenAI used language models to break longer texts, like entire books, into smaller sections. As an example, the original full text of Shakespeare's, Romeo and Juliet, is 25 thousand words. So first they used a LLM to summarize it into 5 thousand words split into 72 sections. Then, these section summaries were summarized again into 7 summaries of a total of roughly 700 words. These were further summarized into a final paragraph of about 100 words.&nbsp;</p><p>Having access to these chains of summaries made evaluation a lot easier. Because now, the humans could evaluate smaller section summaries, instead of reading entire books. And kind of like how teachers often ask you to show the steps of your calculations to see if you\u2019re thinking about the problem correctly. Intermediate summaries let the evaluators see more of the process, if the summarization goes off track they can see where that happened. &nbsp;<br>This marks progress, but even needing to read shorter summaries of books can be time consuming. So the next year, in 2022, OpenAI developed a model that could critique summaries generated by other models. When humans were shown these critiques during their evaluation process, they found about 50% more flaws than unassisted evaluators. This decreased the load on human evaluators, while also increasing quality by highlighting potential errors for humans to pay extra attention to. Using this kind of approach the AIs don't just assist humans by breaking down tasks, they can also &nbsp;help keep other AIs in line.</p><p>These are all concrete experiments in oversight that can be seen as extensions of RLHF, but they all just about text generation. Researchers have also proposed other, broader &nbsp;scalable oversight techniques that use similar notions. The AIs either act as assistants to humans, or, as adversaries to other AIs, keeping them in check. One example is called - AI safety via debate.</p><p>Think of this technique as expert witnesses arguing to convince a jury. The AI models are experts, and the human evaluator is the jury. Separate versions of the AI propose solutions to some problem, and then engage in a debate. &nbsp;They can either defend their proposed solution,, or try to find logical holes, factual flaws, or other ways that the opposing AIs solution might fail. Meanwhile, a human evaluator watches this debate and chooses the most convincing solution. If, as we'd hope, being correct gives you an advantage in a debate, then even if the AI\u2019s have a more advanced understanding of the overall problem, compared to humans, we may still be able to judge which AI has the better argument.</p><p>Besides the ones that we just mentioned, there are a whole range of other proposed solutions to scalable oversight like recursive reward modeling, or iterated distillation and amplification. Some of these methods are quite closely related with recursive reward modeling and debate being special cases of iterated distillation. If you're interested in the details, you can learn more about some of these at the Rob Miles AI Safety channel. That's me, by the way, I\u2019m Rob Miles. I narrate for Rational Animations and I also have my own AI Safety Channel. Link in the description..</p><p>Despite researchers having a few proposed techniques that look promising, there\u2019s still one problem. Our goal is to use these techniques on AIs which might be more capable than every single human expert. Can we really trust that the same techniques we use to train somewhat aligned book-summarizers and chat bots today, will just keep working on all models in the&nbsp;</p><p>future? Ideally, we need some way to start testing whether our techniques will continue to work when AIs will surpass humans in every domain.</p><h2>We don't know if they will work</h2><p>With this problem in mind, in 2021 Ajeya Cotra wrote a blog post titled \u201cThe case for aligning narrowly superhuman models\u201d. She outlined the idea of \u201csandwiching\u201d: a way &nbsp;of evaluating scalable oversight techniques relative to superhuman AIs even before such AIs are built.</p><p>She noticed that compared to average humans, current AIs show superhuman abilities on some narrowly defined tasks. In particular, recently we have started building models that are getting better than some humans on abstract multi-dimensional real-world tasks that she calls \u2018fuzzy tasks\u2019. These are tasks that don't have easily generated training signals or are hard to evaluate, like picking good stocks, or giving good medical advice. Importantly, these models are not \u201csuperhuman\u201d at these tasks in the same way &nbsp;that AlphaGo is \u201csuperhuman\u201d at playing Go. &nbsp;AlphaGo plays Go better than any human, while GPT-4 is only capable of giving better advice or writing better stories than some humans. We still have domain specific experts that can outperform GPT-4.</p><p>Importantly, the fact that such narrowly superhuman models exist means that we can take advantage of them to get some practice in dealing with superhuman AI that might help with more general systems.</p><p>Ajeya Cotra suggested carrying out case studies on the various proposed scalable oversight techniques using one of these models alongside &nbsp;humans that are less capable than the AI on some fuzzy task. This allows us to see how well humans that are less capable than current AI models can be combined with scalable oversight techniques to try to align the models.</p><p>The aim here is to simulate the situation we expect to find ourselves in the future. So we have two groups of humans, experts and non-experts. The non-experts are stand-ins for a future version of humanity, who might have many tools and techniques at their disposal. They need to use whatever tools they have to somehow oversee the outputs of an untrustworthy but superhuman system. The experts meanwhile are standing in as the ground truth.</p><p>Based on whether the oversight techniques succeed or fail in these case studies, we can start iteratively tweaking them, making them stronger over time. Using this process, we may be able to develop techniques that actually work when AI\u2019s will be better than the best humans at every task.&nbsp;</p><h2>Let's do sandwiching to test!</h2><h3>Example</h3><p>To get some intuition about how sandwiching might work, let's think about one of our current models \u2013 GPT-4. Imagine that you\u2019re sick and want to ask for some advice. GPT-4 has seen huge amounts of information from all over the internet. Including a lot of medical research. &nbsp;</p><p>It seems plausible that GPT-4 could give health advice better than a random internet user on Reddit or Facebook. But all a language model \u201cwants\u201d by default, is to predict the next word as it would have appeared in the training data, it doesn't really &nbsp;\u201cwant\u201d to give you good medical advice. In fact, text that constitutes bad advice might be considered more likely if it\u2019s seen more frequently online. You can explicitly ask it to give you good advice, but it\u2019s not really trying to follow instructions, it's trying to predict what comes next and in the training data a request for \u201cgood advice\u201d is still often followed by bad advice.. So, the AI has a goal to generate likely text is misaligned with the user's desire specifically for accurate and helpful information.</p><p>To try and fix this problem, suppose that we gather a group of people who don't know much about medicine. We can arm them with one of our proposed scalable oversight techniques, and see how far they get in trying to make GPT-4 give useful medical advice. We can then compare performance to control groups, like GPT-4 acting alone, or unaugmented humans without any oversight techniques.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rdc69nDvgPfK3d3bw/tp3ma6lkdpdjctdnrx89\"></p><p>The key point here is to measure how well these non-experts manage to make GPT-4 give accurate advice without getting any actual medical experts involved.</p><p>Basically, we want to see how well non-experts can independently take an unaligned model that is more capable than them, and make it behave in a more aligned manner. We can then compare the result with the expert\u2019s judgment, or with the model aligned by the experts.</p><p>In this case, we want the models to move in the direction of accuracy and helpfulness that a qualified doctor might be able to provide. Any progress in this direction indicates some amount of success! It would constitute one data point suggesting that the oversight technique we tested might continue to work on strongly superhuman AIs.</p><h2>Here's the paper that shows how well it works</h2><p>In a 2022 paper titled \u201cMeasuring Progress on Scalable Oversight for Large Language Models\u201d, a group of researchers tested out how well sandwiching might work.</p><p>In a 2022 paper titled \u201cMeasuring Progress on Scalable Oversight for Large Language Models\u201d, a group of researchers tested out how well sandwiching might work.</p><p>In their experiment, they hired non-experts to try and solve two different types of tasks. One task was solving multiple-choice tests on the Massive Multitask Language Understanding benchmark, or the MMLU for short. This has hundreds of questions ranging from college-level physics to abstract questions on morality. The other task was answering questions about short stories on a 5-minute deadline, when such stories would ordinarily take 15 to 30 minutes to read in full. Participants were &nbsp;asked to answer the questions and to &nbsp;rate how confident they were to the correctness of each answer with the goal of reaching the highest level of accuracy on the questions from the two dataset. &nbsp;</p><p>The researchers made some simplifying assumptions for the experimental setup. The oversight technique was a simple back-and-forth text conversation with a language model to help augment the knowledge of the participants, and instead of human experts they used the correct answers from the two tests.</p><p>Despite the simplifications the core of the sandwiching paradigm was preserved. When measured individually, the model outperformed unassisted humans on both tasks by a large margin. This means the basic condition for testing sandwiching - the model being better than the non-experts - was fulfilled. Over the course of the experiment, the participants got pretty good at using the chatbot to probe for facts. They also learned to break down complicated questions into simpler parts, which helped them understand the chatbot's logic and answers better. So the assisted humans got substantially better scores than either the humans or the model did alone. They didn't manage to match expert-level performance estimated in other studies, though.</p><p>There were also some problematic elements that the researchers noticed during the course of the experiment. For example, the chatbot sometimes agreed too easily with whatever the participants said, rather than correcting them when needed. Also since the participants had limited domain knowledge, and couldn't use external sources to fact-check, they accepted false claims as long as the chatbot sounded confident and the answer seemed &nbsp;plausible. This made them give highly confident judgments that turned out to be wrong.</p><p>Despite the problems, simplifications, and the relatively unrealistic setting of multiple-choice questions, the participants managed to move the behavior of the model in the direction that we would want. So the researchers effectively demonstrated sandwiching with this experimental design.</p><p>This paper built a baseline demonstration that future experiments could refine, for example by letting people fine-tune the model, and implementing techniques such as debate and recursive reward modeling, or even letting the participant have access to interpretability techniques to better evaluate what the model says by looking at its internals.</p><p>And, sandwiching is not the only contender for ways to evaluate scalable oversight techniques. There are also other proposals, like meta-level adversarial evaluations that are being explored to ensure that any oversight processes we come up with today are going to scale to dangerously powerful future models.</p><h2>Conclusion</h2><p>These techniques, and others like them, are helping to build an empirical science of AI safety. If we combine the development of new oversight techniques with improved evaluations for their efficacy, then we can start building up some justified confidence in our ability to oversee superhuman AIs in the future.</p><p>[calls to action omitted]</p><h2>Sources and further readings included in the video description</h2><p>Where Ration explained RLHF: https://www.youtube.com/watch?v=qV_rOlHjvvs</p><p>Learning to summarize from human feedback:<br>Paper: https://arxiv.org/abs/2009.01325<br>Blog: https://openai.com/research/learning-to-summarize-with-human-feedback</p><p>Summarizing books with human feedback:<br>Paper: https://arxiv.org/abs/2109.10862<br>Blog: https://openai.com/research/summarizing-books</p><p>Self-critiquing models for assisting human evaluators:<br>Paper: https://arxiv.org/abs/2206.05802<br>Blog: https://openai.com/research/critiques</p><p>AI Safety via debate:<br>Paper: https://arxiv.org/abs/1805.00899<br>Blog: https://openai.com/index/debate/</p><p>Scalable agent alignment via reward modeling:&nbsp;<br>Paper: https://arxiv.org/abs/1811.07871<br>Blog: https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84</p><p>Learning complex goals with iterated amplification:&nbsp;<br>Paper: https://arxiv.org/abs/1810.08575<br>Blog: https://openai.com/research/learning-complex-goals-with-iterated-amplification</p><p>Rob Miles explains reward modeling: https://www.youtube.com/watch?v=PYylPRX6z4Q<br>Rob Miles explains iterated amplification and distillation: https://www.youtube.com/watch?v=v9M2Ho9I9Qo</p><p>The case for aligning narrowly superhuman models, by Ajeya Cotra: https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models</p><p>Measuring Progress on Scalable Oversight for Large Language Models: https://arxiv.org/abs/2211.03540</p><p>Meta-level adversarial evaluation: https://www.lesswrong.com/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1</p>",
    "base_score": 14,
    "comment_count": 1,
    "posted_at": "2025-07-06T14:00:41.519000Z",
    "created_at": "2025-08-21T07:18:23.378866Z",
    "author_id": "AGWTBgADiMqsJYLBG",
    "author_display_name": "Writer",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "2pYW47yQfGxDdawpj",
      "9GQf4Ec6ckqvnPBSw",
      "EHXNGqENrziYQJsox",
      "NTEQR6Saq3u3t22vZ",
      "ftyDfKF45Gig2u4sx",
      "oNiQsBHA3i837sySD"
    ],
    "tag_names": [
      "AI alignment",
      "AI evaluations and standards",
      "AI forecasting",
      "AI safety",
      "Research summary",
      "Video"
    ],
    "markdown_content": "*In the future, AIs will likely be much smarter than we are. They'll produce outputs that may be difficult for humans to evaluate, either because evaluation is too labor-intensive, or because it's qualitatively hard to judge the actions of machines smarter than us. This is the problem of \u201cscalable oversight.\u201d Proposed solutions include \u201cdebate\u201d and iterated amplification. But how can we run experiments today to see whether these ideas actually work in practice?*\n\n*In this video, we cover Ajeya Cotra\u2019s \u201csandwiching\u201d proposal: asking non-experts to align a model that is smarter than they are but less smart than a group of experts, and seeing how well they do. We then show how Sam Bowman et al. tested a basic version of this idea in their paper \u201cMeasuring Progress on Scalable Oversight for Large Language Models.\u201d*\n\n*This video has been written by* [*@markov*](https://www.lesswrong.com/users/markovial?mention=user)*, John Burden, and me.*\n\n*You can find the script below.*\n\n---\n\n## How do we oversee AIs that are smarter than us?\n\nAI systems are getting more capable at a rapid pace. \u00a0In our previous videos, we talked about how developers \u00a0are using a technique called reinforcement learning from human feedback, or RLHF, to try to align AI systems to our preferences using human oversight. .\n\nThis might work for now, but as the tasks that we need to oversee keep getting increasingly complex, and AI systems increasingly smart, it\u2019s difficult for humans to remain good supervisors. This is the core problem of \u2018scalable oversight\u2019.\n\nIn this video, we\u2019ll \u00a0walk through two related parts of scalable oversight. They aren\u2019t really that separable in practice, but explaining them independently helps with understanding them.\n\nThe first part is, we need to come up with potential ways to solve the problem. Researchers have tested feedback methods that work pretty well right now on tasks that are already hard to evaluate.\u00a0  \n  \nThe second part is, what happens when AI\u2019s are better than humans at every task? Can we still effectively supervise them? This is hard to figure out \u00a0before we have such AIs, but in this video we\u2019ll showcase a technique that might \u00a0make it possible!\n\n## We have theoretical scalable oversight techniques\n\nLet's start by diving into the first problem.\n\nIn the last few years, researchers have come up with a few proposals for attempting to solve scalable oversight.\n\nWhile working on providing human evaluations for AI generated texts, OpenAI experimented with some scalable oversight proposals that extended existing RLHF methods. Their general idea was to augment human oversight using AI assistants trained on an easier version of the problem.\n\nIn 2020, they trained AIs to generate summaries of texts. Effectively, \u00a0TL;DRs of reddit posts. Human evaluators would then give feedback on how useful or accurate the generated summaries were. These summaries were still relatively easy to evaluate, because if needed, it was feasible for an evaluator to just read the entire original text and check the accuracy of the summary. But the point is that these techniques need to scale! What if the overseers don't have the ability or the time to read the full text?\n\nTo make progress on this problem, in 2021, OpenAI used language models to break longer texts, like entire books, into smaller sections. As an example, the original full text of Shakespeare's, Romeo and Juliet, is 25 thousand words. So first they used a LLM to summarize it into 5 thousand words split into 72 sections. Then, these section summaries were summarized again into 7 summaries of a total of roughly 700 words. These were further summarized into a final paragraph of about 100 words.\n\nHaving access to these chains of summaries made evaluation a lot easier. Because now, the humans could evaluate smaller section summaries, instead of reading entire books. And kind of like how teachers often ask you to show the steps of your calculations to see if you\u2019re thinking about the problem correctly. Intermediate summaries let the evaluators see more of the process, if the summarization goes off track they can see where that happened. \u00a0  \nThis marks progress, but even needing to read shorter summaries of books can be time consuming. So the next year, in 2022, OpenAI developed a model that could critique summaries generated by other models. When humans were shown these critiques during their evaluation process, they found about 50% more flaws than unassisted evaluators. This decreased the load on human evaluators, while also increasing quality by highlighting potential errors for humans to pay extra attention to. Using this kind of approach the AIs don't just assist humans by breaking down tasks, they can also \u00a0help keep other AIs in line.\n\nThese are all concrete experiments in oversight that can be seen as extensions of RLHF, but they all just about text generation. Researchers have also proposed other, broader \u00a0scalable oversight techniques that use similar notions. The AIs either act as assistants to humans, or, as adversaries to other AIs, keeping them in check. One example is called - AI safety via debate.\n\nThink of this technique as expert witnesses arguing to convince a jury. The AI models are experts, and the human evaluator is the jury. Separate versions of the AI propose solutions to some problem, and then engage in a debate. \u00a0They can either defend their proposed solution,, or try to find logical holes, factual flaws, or other ways that the opposing AIs solution might fail. Meanwhile, a human evaluator watches this debate and chooses the most convincing solution. If, as we'd hope, being correct gives you an advantage in a debate, then even if the AI\u2019s have a more advanced understanding of the overall problem, compared to humans, we may still be able to judge which AI has the better argument.\n\nBesides the ones that we just mentioned, there are a whole range of other proposed solutions to scalable oversight like recursive reward modeling, or iterated distillation and amplification. Some of these methods are quite closely related with recursive reward modeling and debate being special cases of iterated distillation. If you're interested in the details, you can learn more about some of these at the Rob Miles AI Safety channel. That's me, by the way, I\u2019m Rob Miles. I narrate for Rational Animations and I also have my own AI Safety Channel. Link in the description..\n\nDespite researchers having a few proposed techniques that look promising, there\u2019s still one problem. Our goal is to use these techniques on AIs which might be more capable than every single human expert. Can we really trust that the same techniques we use to train somewhat aligned book-summarizers and chat bots today, will just keep working on all models in the\n\nfuture? Ideally, we need some way to start testing whether our techniques will continue to work when AIs will surpass humans in every domain.\n\n## We don't know if they will work\n\nWith this problem in mind, in 2021 Ajeya Cotra wrote a blog post titled \u201cThe case for aligning narrowly superhuman models\u201d. She outlined the idea of \u201csandwiching\u201d: a way \u00a0of evaluating scalable oversight techniques relative to superhuman AIs even before such AIs are built.\n\nShe noticed that compared to average humans, current AIs show superhuman abilities on some narrowly defined tasks. In particular, recently we have started building models that are getting better than some humans on abstract multi-dimensional real-world tasks that she calls \u2018fuzzy tasks\u2019. These are tasks that don't have easily generated training signals or are hard to evaluate, like picking good stocks, or giving good medical advice. Importantly, these models are not \u201csuperhuman\u201d at these tasks in the same way \u00a0that AlphaGo is \u201csuperhuman\u201d at playing Go. \u00a0AlphaGo plays Go better than any human, while GPT-4 is only capable of giving better advice or writing better stories than some humans. We still have domain specific experts that can outperform GPT-4.\n\nImportantly, the fact that such narrowly superhuman models exist means that we can take advantage of them to get some practice in dealing with superhuman AI that might help with more general systems.\n\nAjeya Cotra suggested carrying out case studies on the various proposed scalable oversight techniques using one of these models alongside \u00a0humans that are less capable than the AI on some fuzzy task. This allows us to see how well humans that are less capable than current AI models can be combined with scalable oversight techniques to try to align the models.\n\nThe aim here is to simulate the situation we expect to find ourselves in the future. So we have two groups of humans, experts and non-experts. The non-experts are stand-ins for a future version of humanity, who might have many tools and techniques at their disposal. They need to use whatever tools they have to somehow oversee the outputs of an untrustworthy but superhuman system. The experts meanwhile are standing in as the ground truth.\n\nBased on whether the oversight techniques succeed or fail in these case studies, we can start iteratively tweaking them, making them stronger over time. Using this process, we may be able to develop techniques that actually work when AI\u2019s will be better than the best humans at every task.\n\n## Let's do sandwiching to test!\n\n### Example\n\nTo get some intuition about how sandwiching might work, let's think about one of our current models \u2013 GPT-4. Imagine that you\u2019re sick and want to ask for some advice. GPT-4 has seen huge amounts of information from all over the internet. Including a lot of medical research.\n\nIt seems plausible that GPT-4 could give health advice better than a random internet user on Reddit or Facebook. But all a language model \u201cwants\u201d by default, is to predict the next word as it would have appeared in the training data, it doesn't really \u00a0\u201cwant\u201d to give you good medical advice. In fact, text that constitutes bad advice might be considered more likely if it\u2019s seen more frequently online. You can explicitly ask it to give you good advice, but it\u2019s not really trying to follow instructions, it's trying to predict what comes next and in the training data a request for \u201cgood advice\u201d is still often followed by bad advice.. So, the AI has a goal to generate likely text is misaligned with the user's desire specifically for accurate and helpful information.\n\nTo try and fix this problem, suppose that we gather a group of people who don't know much about medicine. We can arm them with one of our proposed scalable oversight techniques, and see how far they get in trying to make GPT-4 give useful medical advice. We can then compare performance to control groups, like GPT-4 acting alone, or unaugmented humans without any oversight techniques.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rdc69nDvgPfK3d3bw/tp3ma6lkdpdjctdnrx89)\n\nThe key point here is to measure how well these non-experts manage to make GPT-4 give accurate advice without getting any actual medical experts involved.\n\nBasically, we want to see how well non-experts can independently take an unaligned model that is more capable than them, and make it behave in a more aligned manner. We can then compare the result with the expert\u2019s judgment, or with the model aligned by the experts.\n\nIn this case, we want the models to move in the direction of accuracy and helpfulness that a qualified doctor might be able to provide. Any progress in this direction indicates some amount of success! It would constitute one data point suggesting that the oversight technique we tested might continue to work on strongly superhuman AIs.\n\n## Here's the paper that shows how well it works\n\nIn a 2022 paper titled \u201cMeasuring Progress on Scalable Oversight for Large Language Models\u201d, a group of researchers tested out how well sandwiching might work.\n\nIn a 2022 paper titled \u201cMeasuring Progress on Scalable Oversight for Large Language Models\u201d, a group of researchers tested out how well sandwiching might work.\n\nIn their experiment, they hired non-experts to try and solve two different types of tasks. One task was solving multiple-choice tests on the Massive Multitask Language Understanding benchmark, or the MMLU for short. This has hundreds of questions ranging from college-level physics to abstract questions on morality. The other task was answering questions about short stories on a 5-minute deadline, when such stories would ordinarily take 15 to 30 minutes to read in full. Participants were \u00a0asked to answer the questions and to \u00a0rate how confident they were to the correctness of each answer with the goal of reaching the highest level of accuracy on the questions from the two dataset.\n\nThe researchers made some simplifying assumptions for the experimental setup. The oversight technique was a simple back-and-forth text conversation with a language model to help augment the knowledge of the participants, and instead of human experts they used the correct answers from the two tests.\n\nDespite the simplifications the core of the sandwiching paradigm was preserved. When measured individually, the model outperformed unassisted humans on both tasks by a large margin. This means the basic condition for testing sandwiching - the model being better than the non-experts - was fulfilled. Over the course of the experiment, the participants got pretty good at using the chatbot to probe for facts. They also learned to break down complicated questions into simpler parts, which helped them understand the chatbot's logic and answers better. So the assisted humans got substantially better scores than either the humans or the model did alone. They didn't manage to match expert-level performance estimated in other studies, though.\n\nThere were also some problematic elements that the researchers noticed during the course of the experiment. For example, the chatbot sometimes agreed too easily with whatever the participants said, rather than correcting them when needed. Also since the participants had limited domain knowledge, and couldn't use external sources to fact-check, they accepted false claims as long as the chatbot sounded confident and the answer seemed \u00a0plausible. This made them give highly confident judgments that turned out to be wrong.\n\nDespite the problems, simplifications, and the relatively unrealistic setting of multiple-choice questions, the participants managed to move the behavior of the model in the direction that we would want. So the researchers effectively demonstrated sandwiching with this experimental design.\n\nThis paper built a baseline demonstration that future experiments could refine, for example by letting people fine-tune the model, and implementing techniques such as debate and recursive reward modeling, or even letting the participant have access to interpretability techniques to better evaluate what the model says by looking at its internals.\n\nAnd, sandwiching is not the only contender for ways to evaluate scalable oversight techniques. There are also other proposals, like meta-level adversarial evaluations that are being explored to ensure that any oversight processes we come up with today are going to scale to dangerously powerful future models.\n\n## Conclusion\n\nThese techniques, and others like them, are helping to build an empirical science of AI safety. If we combine the development of new oversight techniques with improved evaluations for their efficacy, then we can start building up some justified confidence in our ability to oversee superhuman AIs in the future.\n\n[calls to action omitted]\n\n## Sources and further readings included in the video description\n\nWhere Ration explained RLHF: https://www.youtube.com/watch?v=qV\\_rOlHjvvs\n\nLearning to summarize from human feedback:  \nPaper: https://arxiv.org/abs/2009.01325  \nBlog: https://openai.com/research/learning-to-summarize-with-human-feedback\n\nSummarizing books with human feedback:  \nPaper: https://arxiv.org/abs/2109.10862  \nBlog: https://openai.com/research/summarizing-books\n\nSelf-critiquing models for assisting human evaluators:  \nPaper: https://arxiv.org/abs/2206.05802  \nBlog: https://openai.com/research/critiques\n\nAI Safety via debate:  \nPaper: https://arxiv.org/abs/1805.00899  \nBlog: https://openai.com/index/debate/\n\nScalable agent alignment via reward modeling:\u00a0  \nPaper: https://arxiv.org/abs/1811.07871  \nBlog: https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84\n\nLearning complex goals with iterated amplification:\u00a0  \nPaper: https://arxiv.org/abs/1810.08575  \nBlog: https://openai.com/research/learning-complex-goals-with-iterated-amplification\n\nRob Miles explains reward modeling: https://www.youtube.com/watch?v=PYylPRX6z4Q  \nRob Miles explains iterated amplification and distillation: https://www.youtube.com/watch?v=v9M2Ho9I9Qo\n\nThe case for aligning narrowly superhuman models, by Ajeya Cotra: https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models\n\nMeasuring Progress on Scalable Oversight for Large Language Models: https://arxiv.org/abs/2211.03540\n\nMeta-level adversarial evaluation: https://www.lesswrong.com/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1",
    "word_count": 2622,
    "reading_time_minutes": 13,
    "external_links": [
      "https://www.lesswrong.com/users/markovial?mention=user"
    ],
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T07:18:26.006951Z"
  },
  {
    "id": 205,
    "post_id": "RPrkxFGmDvA5nPJdQ",
    "title": "Call on AI Companies: Publish Your Whistleblowing Policies",
    "title_normalized": "call on ai companies publish your whistleblowing policies",
    "page_url": "https://forum.effectivealtruism.org/posts/RPrkxFGmDvA5nPJdQ/call-on-ai-companies-publish-your-whistleblowing-policies",
    "html_body": "<p><a href=\"https://publishyourpolicies.org/\"><i>Announcing a coalition of +30 whistleblowing and AI organizations, calling for stronger transparency on company-internal whistleblower protections.&nbsp;</i></a></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kH73v7rNrawjNRquc/pqothpyotcveiakhmanf\" alt=\"Rating of whistlbelowing system transparency\"><figcaption>Rating of whistleblower system transparency of major AI companies. Details below.<br>*Please note that AIWI only evaluates the transparency of the policy and outcome reporting\u2014not the content or quality of the underlying system, protections, culture, or past patterns of retaliation.</figcaption></figure><p>Frontier AI companies currently lag global standards and best practices in creating adequate transparency around their internal whistleblowing systems: 5 out 6 companies in our target set do not even publish their whistleblowing policies.&nbsp;</p><p>That means we, the public, and employees in AI, are forced to 'trust' that companies will address concerns well internally.&nbsp;</p><p><strong>This is far from good enough&nbsp;</strong></p><p>...and why we, at the National Whistleblower Day Event in Washington DC yesterday, launched a campaign asking AI companies to publish their internal whistleblowing policies (\"Level 1\") and reports on their whistleblowing system performance, effectiveness, and outcomes (\"Level 2\").&nbsp;</p><p>We are very proud of the coalition we have the privilege of representing here - uniting most of the world's most prominent whistleblowing organizations and scholars with equally prominent AI counterparts.</p><p>See further below a full list of signatories or our <a href=\"https://publishyourpolicies.org/\">campaign page</a>.&nbsp;</p><h2>This Post</h2><p>You can find the actual campaign page, including evidence and sources, here: <a href=\"https://publishyourpolicies.org/\">https://publishyourpolicies.org/</a></p><p>In this post I'll share the same message with a slightly altered 'storyline'.&nbsp;</p><h2>Why This Matters Now</h2><p>I don't have to make the case here for why we should care about the way AI companies go about development and deployment of their frontier models - especially over the coming years.&nbsp;</p><p>Likewise, if you've seen <a href=\"https://righttowarn.ai/\">righttowarn</a>, you're likely aware of this line of reasoning: <strong>Many risks will only be visible to insiders.</strong> The current black-box nature of AI development means employees are often the first\u2014and potentially only\u2014people positioned to spot dangerous developments, misconduct, or safety shortcuts.</p><p>It therefore matters that AI companies build up the infrastructure required to address concerns raised today already and that we can enter a 'race to the top' on system quality as soon as possible.</p><p>Transparency on internal whistleblowing systems, allowing for public feedback and empowering employees to understand and compare protections is the mechanism to enter that 'race to the top' mechanism.</p><p>Important note 1: We are talking about company internal whistleblowing systems here (although they can extend arbitrarily far in terms of 'covered persons', e.g. to suppliers, customers, etc.). <strong>This does NOT diminish the importance of legal protections for AI whistleblowers or independent support offerings for insiders.&nbsp;</strong><br>But the reality is (see below) that we expect the majority of risks to be flagged internally <i>first. That means internal channels are critical and must not be neglected. </i>If you like the 'swiss cheese mode' of risk management - we want to make sure protections are as strong as possible at <i>every level</i>.&nbsp;<i>&nbsp;</i>&nbsp;</p><p>Important note 2: Both in this post and our main post, we are not evaluating policy or system quality. We only talk about the degree of transparency provided. &nbsp;</p><h2>The Case for Transparency</h2><h3>1. Insiders Are Uniquely Positioned</h3><p>Current and former AI employees have recognized that they are \"among the few people who can hold [companies] accountable to the public.\" <a href=\"https://righttowarn.ai/\">They've called for </a>companies to \"facilitate a verifiably anonymous process for current and former employees to raise risk-related concerns to the company's board, to regulators, and to an appropriate independent organization.\"</p><p>Research consistently shows that employees are often the first to recognize potential wrongdoing or risk of harm. In AI specifically, the technical complexity and proprietary nature of development means many risks are only visible to those with internal access.</p><h3>2. Internal Channels Are a Major Path</h3><p>Data from the SEC Whistleblower Program shows that three-quarters of award recipients initially attempted to address concerns within their organizations before seeking external remedies. Employees naturally try internal channels first, and we expect this to be no different in frontier AI companies:&nbsp;</p><ol><li>Nature of work: Research &amp; Engineering work relies on discussion. It is standard practice for concerns to be escalated internally - especially if they are less 'clear cut' and independently identifiable as e.g. accounting fraud or bribery (which however are still in scope of whistleblowing policies). &nbsp;</li><li>Culture: Addressing concerns internally first is a common part of many Silicon Valley organizations.&nbsp;</li></ol><p>This means that these systems must work reliably: When internal systems fail, we all lose. Companies miss opportunities to address problems early, employees face unnecessary risks, and the public remains unaware of safety issues until they potentially become crises.</p><h3>3. Current Systems Are Opaque and Potentially Broken</h3><p>Major AI companies have not published their whistleblowing policies. The recent <a href=\"https://futureoflife.org/wp-content/uploads/2025/07/FLI-AI-Safety-Index-Report-Summer-2025.pdf\">Future of Life Institute AI Safety Index </a>highlighted that Anthropic, Google DeepMind, xAI, and Mistral lack public whistleblowing policies, making neutral assessment impossible. They, likewise, call for the publication of policies.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kH73v7rNrawjNRquc/utijwvjvwkseyeshfath\"></figure><p>OpenAI is the sole exception\u2014and they only published their policy following public pressure over their restrictive non-disparagement clauses. Even then, none of the major AI companies publish effectiveness metrics or outcome data.</p><p><strong>This stands in stark contrast to other industries. Companies across sectors routinely publish whistleblowing policies\u2014from AI-related organizations like ASML to industrial firms like Tata Steel to financial services companies. </strong>Many also publish regular effectiveness evaluations and outcome statistics.</p><p>Conversations with insiders also reveal gaps:</p><p><strong>Employee Awareness</strong>: Interviews with current and former frontier AI company insiders show that many employees don't know, understand, or trust their companies' internal reporting systems. As one insider told us: <i>\"I'm not well-informed about our company's whistleblowing procedures (and it feels uncomfortable to inquire about them directly).\"</i></p><p><strong>Trust Deficit</strong>: AI employees suspect that making reports would be ineffective or could make their work lives more difficult. Another insider shared: <i>\"I anticipate that using official reporting channels would likely result in subtle, indirect consequences rather than overt retaliation like termination.\"</i></p><p><strong>History of Retaliation</strong>: AI companies have attempted to suppress individuals voicing concerns (OpenAI's restrictive NDAs) and have faced cases around alleged wrongful termination for speaking up on research misconduct (Google).&nbsp;</p><p>We also have good reason to believe that multiple companies' internal whistleblowing policies are currently in violation of the EU Whistleblowing Directive. If you are interested: Happy to provide details via DM.&nbsp;</p><p>It <i>might </i>still be the case that certain systems are working relatively well today (at least for one of the organizations in the set we have an 'okay' impression based on conversations with individuals) <u>- </u><strong><u>but the reality is that neither insiders nor we know</u>.</strong>&nbsp;</p><p><strong>Every insider we have spoken to to date supports the publication of whistleblowing policies. </strong>If you are an insider and you don't - please reach out and share your thoughts with us (or comment below).</p><h3>4. Transparency Enables Verification and Improvement</h3><p>Without published policies &amp; outcome transparency, the public cannot assess whether internal systems actually protect employees who raise safety concerns.&nbsp;</p><p><strong>Employees </strong>cannot compare protections across companies when making career decisions.&nbsp;</p><p><strong>Policymakers </strong>cannot identify coverage gaps or craft appropriate regulations.</p><p><strong>Companies benefit</strong> from improved systems through public feedback and heightened employee awareness. Empirical evidence shows that there is a strong 'business case' for improved speak up cultures and whistleblowing systems - from improved innovation to increased employee loyalty. This is why, for example, <a href=\"https://www.theverge.com/2021/4/6/22370177/google-whistleblower-protections-trillium-asset-management\">shareholder representatives have called on Google to improve its whistleblowing systems</a>.&nbsp;</p><h3>5. This information vacuum serves no legitimate purpose</h3><p>We are only calling for transparency: This should create no major workload for companies. If it does: Then maybe that means there were things to be improved upon).&nbsp;</p><p>Whistleblowing policies contain procedural frameworks and legal guarantees\u2014<strong>not trade secrets or competitive advantages. There's no business case for secrecy, but substantial evidence for the benefits of transparency.</strong></p><p><strong>If companies truly care about developing a strong speak-up culture and protecting those who live it: Publish. Your. Policies. &nbsp;</strong></p><h2>What We're Asking For</h2><p>We're calling on AI companies to meet two levels of transparency [this is an excerpt - see <a href=\"https://publishyourpolicies.org/\">campaign page </a>for details]:</p><p><strong>Level 1: Policy Transparency</strong> (minimum baseline)</p><ul><li>Publish complete whistleblowing policy documents</li><li>Clearly define scope of protected individuals and covered wrongdoing</li><li>Outline reporting channels, investigation procedures, and timelines</li><li>Specify protection and support measures</li><li>Detail independence guarantees and implementation</li></ul><p><strong>Level 2: Effectiveness Transparency</strong> (what companies should strive for)</p><ul><li>Publish metrics on reports received, resolved, and outcomes</li><li>Share data on retaliation complaints and whistleblower satisfaction</li><li>Report on employee awareness, understanding, and trust levels</li><li>Document regular system reviews and improvements</li><li>Conduct and publish results of independent effectiveness audits</li></ul><p>Companies that take whistleblowing seriously should already gather this data for continuous improvement.&nbsp;</p><p>Publication is simply a matter of transparency.</p><h2>The Coalition</h2><p>This call is supported by a broad coalition of scholars, AI safety organizations, and whistleblowing advocacy groups:</p><p><strong>Organizations:</strong></p><ul><li>Blueprint for Free Speech</li><li>Center for AI Policy</li><li>CARMA (Centre for AI Risk Management &amp; Alignment)</li><li>Convergence Analysis</li><li>Encode</li><li>Fathom</li><li>Government Accountability Project</li><li>Human Rights Law Centre</li><li>LASST</li><li>Legal Safety Lab</li><li>National Whistleblower Center</li><li>Psst</li><li>Pour Demain</li><li>Safer AI</li><li>Secure AI Project</li><li>Future of Life Institute</li><li>The Future Society</li><li>The Midas Project</li><li>The Signals Network</li><li>Transparency International</li><li>WHISPeR</li><li>Whistleblower Netzwerk</li><li>Whistleblower Partners LLP</li><li>Whistleblower International Network</li></ul><p><strong>Academic Signatories:</strong></p><ul><li><strong>Dimitrios Kafteranis</strong>, University of Coventry</li><li><strong>Jessica Newman</strong>, AI Security Initiative, UC Berkeley</li><li><strong>Kartik Hosanagar</strong>, Wharton Business School</li><li><strong>Lawrence Lessig</strong>, Harvard Law School</li><li><strong>Nathan Labenz</strong>, Cognitive Revolution</li><li><strong>Peter Salib</strong>, University of Houston Law Center</li><li><strong>Roman Yampolskiy</strong>, University of Louisville</li><li><strong>Simon Gerdemann</strong>, University of Goettingen</li><li><strong>Stuart Russell</strong>, University of California, Berkeley</li><li><strong>Wim Vandekerckhove</strong>, EDHEC Business School</li></ul><h2>Moving Forward</h2><p>This campaign offers an opportunity for AI companies to demonstrate commitment to integrity cultures where flagging risks is a normal and expected responsibility.</p><p>We're not asking companies to reveal competitive secrets\u2014we're asking them to show they're serious about the concern systems they claim to have. Transparency costs nothing but builds everything.</p><p><strong>The stakes are too high for \"trust us\" to be enough.</strong> When AI companies publicly <a href=\"https://openai.com/index/planning-for-agi-and-beyond/\">acknowledge</a> <a href=\"https://www.anthropic.com/news/core-views-on-ai-safety\">existential</a> <a href=\"https://deepmind.google/public-policy/ai-summit-policies/\">risks</a>, they must also demonstrate that employees can safely report concerns about those risks.</p><h2>What you can do</h2><p>If you believe our call is sensible and you are...</p><ol><li>An insider at an AI Company: Ask your management why they are not publishing their policies. Share our call with them.</li><li>A leader of an AI Company: You can lead the charge! A strong speak up culture benefits your employees, shareholders, and you (unless you'd prefer risks to be hidden until it's too late): We can be in the same boat - if you genuinely care about protecting those speaking up.&nbsp;<br>If you credibly commit to Level 2: We will commend you for it.</li><li>An 'outsider': Spread the word. Every share gets us closer to transparency and a world where insiders in AI can raise their concerns as they see them.<br>We might also announce a second round of signatories. Contact us if you would like to be on this list.&nbsp;</li></ol><p><strong>Join and share the campaign:</strong> https://aiwi.org/publishyourpolicies/</p><p><strong>Contact:</strong> For questions or to add your organization's support, reach out through the campaign website.</p><p><i>This campaign is led by </i><a href=\"https://aiwi.org/\"><i>The AI Whistleblower Initiative</i></a><i> (AIWI, formerly OAISIS), an independent, nonpartisan, nonprofit organization supporting whistleblowers in AI.</i></p>",
    "base_score": 11,
    "comment_count": 0,
    "posted_at": "2025-08-01T15:59:09.075000Z",
    "created_at": "2025-08-21T05:04:31.189093Z",
    "author_id": "D4y2mehXKeQae7pTM",
    "author_display_name": "Karl",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "bY6qtCihTzzSCw9gc",
      "oNiQsBHA3i837sySD",
      "of9xBvR3wpbp6qsZC",
      "u3Xg8MjDe2e6BvKtv",
      "voyq7LkNhsFfYqddL"
    ],
    "tag_names": [
      "AI governance",
      "AI safety",
      "Announcements and updates",
      "Policy",
      "Transparency"
    ],
    "markdown_content": "[*Announcing a coalition of +30 whistleblowing and AI organizations, calling for stronger transparency on company-internal whistleblower protections.*](https://publishyourpolicies.org/)\n\n![Rating of whistlbelowing system transparency](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kH73v7rNrawjNRquc/pqothpyotcveiakhmanf)\n\nRating of whistleblower system transparency of major AI companies. Details below.  \n\\*Please note that AIWI only evaluates the transparency of the policy and outcome reporting\u2014not the content or quality of the underlying system, protections, culture, or past patterns of retaliation.\n\nFrontier AI companies currently lag global standards and best practices in creating adequate transparency around their internal whistleblowing systems: 5 out 6 companies in our target set do not even publish their whistleblowing policies.\n\nThat means we, the public, and employees in AI, are forced to 'trust' that companies will address concerns well internally.\n\n**This is far from good enough**\n\n...and why we, at the National Whistleblower Day Event in Washington DC yesterday, launched a campaign asking AI companies to publish their internal whistleblowing policies (\"Level 1\") and reports on their whistleblowing system performance, effectiveness, and outcomes (\"Level 2\").\n\nWe are very proud of the coalition we have the privilege of representing here - uniting most of the world's most prominent whistleblowing organizations and scholars with equally prominent AI counterparts.\n\nSee further below a full list of signatories or our [campaign page](https://publishyourpolicies.org/).\n\n## This Post\n\nYou can find the actual campaign page, including evidence and sources, here: <https://publishyourpolicies.org/>\n\nIn this post I'll share the same message with a slightly altered 'storyline'.\n\n## Why This Matters Now\n\nI don't have to make the case here for why we should care about the way AI companies go about development and deployment of their frontier models - especially over the coming years.\n\nLikewise, if you've seen [righttowarn](https://righttowarn.ai/), you're likely aware of this line of reasoning: **Many risks will only be visible to insiders.** The current black-box nature of AI development means employees are often the first\u2014and potentially only\u2014people positioned to spot dangerous developments, misconduct, or safety shortcuts.\n\nIt therefore matters that AI companies build up the infrastructure required to address concerns raised today already and that we can enter a 'race to the top' on system quality as soon as possible.\n\nTransparency on internal whistleblowing systems, allowing for public feedback and empowering employees to understand and compare protections is the mechanism to enter that 'race to the top' mechanism.\n\nImportant note 1: We are talking about company internal whistleblowing systems here (although they can extend arbitrarily far in terms of 'covered persons', e.g. to suppliers, customers, etc.). **This does NOT diminish the importance of legal protections for AI whistleblowers or independent support offerings for insiders.**  \nBut the reality is (see below) that we expect the majority of risks to be flagged internally *first. That means internal channels are critical and must not be neglected.* If you like the 'swiss cheese mode' of risk management - we want to make sure protections are as strong as possible at *every level*.\u00a0\n\nImportant note 2: Both in this post and our main post, we are not evaluating policy or system quality. We only talk about the degree of transparency provided.\n\n## The Case for Transparency\n\n### 1. Insiders Are Uniquely Positioned\n\nCurrent and former AI employees have recognized that they are \"among the few people who can hold [companies] accountable to the public.\" [They've called for](https://righttowarn.ai/) companies to \"facilitate a verifiably anonymous process for current and former employees to raise risk-related concerns to the company's board, to regulators, and to an appropriate independent organization.\"\n\nResearch consistently shows that employees are often the first to recognize potential wrongdoing or risk of harm. In AI specifically, the technical complexity and proprietary nature of development means many risks are only visible to those with internal access.\n\n### 2. Internal Channels Are a Major Path\n\nData from the SEC Whistleblower Program shows that three-quarters of award recipients initially attempted to address concerns within their organizations before seeking external remedies. Employees naturally try internal channels first, and we expect this to be no different in frontier AI companies:\n\n1. Nature of work: Research & Engineering work relies on discussion. It is standard practice for concerns to be escalated internally - especially if they are less 'clear cut' and independently identifiable as e.g. accounting fraud or bribery (which however are still in scope of whistleblowing policies).\n2. Culture: Addressing concerns internally first is a common part of many Silicon Valley organizations.\n\nThis means that these systems must work reliably: When internal systems fail, we all lose. Companies miss opportunities to address problems early, employees face unnecessary risks, and the public remains unaware of safety issues until they potentially become crises.\n\n### 3. Current Systems Are Opaque and Potentially Broken\n\nMajor AI companies have not published their whistleblowing policies. The recent [Future of Life Institute AI Safety Index](https://futureoflife.org/wp-content/uploads/2025/07/FLI-AI-Safety-Index-Report-Summer-2025.pdf) highlighted that Anthropic, Google DeepMind, xAI, and Mistral lack public whistleblowing policies, making neutral assessment impossible. They, likewise, call for the publication of policies.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kH73v7rNrawjNRquc/utijwvjvwkseyeshfath)\n\nOpenAI is the sole exception\u2014and they only published their policy following public pressure over their restrictive non-disparagement clauses. Even then, none of the major AI companies publish effectiveness metrics or outcome data.\n\n**This stands in stark contrast to other industries. Companies across sectors routinely publish whistleblowing policies\u2014from AI-related organizations like ASML to industrial firms like Tata Steel to financial services companies.** Many also publish regular effectiveness evaluations and outcome statistics.\n\nConversations with insiders also reveal gaps:\n\n**Employee Awareness**: Interviews with current and former frontier AI company insiders show that many employees don't know, understand, or trust their companies' internal reporting systems. As one insider told us: *\"I'm not well-informed about our company's whistleblowing procedures (and it feels uncomfortable to inquire about them directly).\"*\n\n**Trust Deficit**: AI employees suspect that making reports would be ineffective or could make their work lives more difficult. Another insider shared: *\"I anticipate that using official reporting channels would likely result in subtle, indirect consequences rather than overt retaliation like termination.\"*\n\n**History of Retaliation**: AI companies have attempted to suppress individuals voicing concerns (OpenAI's restrictive NDAs) and have faced cases around alleged wrongful termination for speaking up on research misconduct (Google).\n\nWe also have good reason to believe that multiple companies' internal whistleblowing policies are currently in violation of the EU Whistleblowing Directive. If you are interested: Happy to provide details via DM.\n\nIt *might* still be the case that certain systems are working relatively well today (at least for one of the organizations in the set we have an 'okay' impression based on conversations with individuals) - **but the reality is that neither insiders nor we know.**\n\n**Every insider we have spoken to to date supports the publication of whistleblowing policies.** If you are an insider and you don't - please reach out and share your thoughts with us (or comment below).\n\n### 4. Transparency Enables Verification and Improvement\n\nWithout published policies & outcome transparency, the public cannot assess whether internal systems actually protect employees who raise safety concerns.\n\n**Employees** cannot compare protections across companies when making career decisions.\n\n**Policymakers** cannot identify coverage gaps or craft appropriate regulations.\n\n**Companies benefit** from improved systems through public feedback and heightened employee awareness. Empirical evidence shows that there is a strong 'business case' for improved speak up cultures and whistleblowing systems - from improved innovation to increased employee loyalty. This is why, for example, [shareholder representatives have called on Google to improve its whistleblowing systems](https://www.theverge.com/2021/4/6/22370177/google-whistleblower-protections-trillium-asset-management).\n\n### 5. This information vacuum serves no legitimate purpose\n\nWe are only calling for transparency: This should create no major workload for companies. If it does: Then maybe that means there were things to be improved upon).\n\nWhistleblowing policies contain procedural frameworks and legal guarantees\u2014**not trade secrets or competitive advantages. There's no business case for secrecy, but substantial evidence for the benefits of transparency.**\n\n**If companies truly care about developing a strong speak-up culture and protecting those who live it: Publish. Your. Policies.**\n\n## What We're Asking For\n\nWe're calling on AI companies to meet two levels of transparency [this is an excerpt - see [campaign page](https://publishyourpolicies.org/) for details]:\n\n**Level 1: Policy Transparency** (minimum baseline)\n\n- Publish complete whistleblowing policy documents\n- Clearly define scope of protected individuals and covered wrongdoing\n- Outline reporting channels, investigation procedures, and timelines\n- Specify protection and support measures\n- Detail independence guarantees and implementation\n\n**Level 2: Effectiveness Transparency** (what companies should strive for)\n\n- Publish metrics on reports received, resolved, and outcomes\n- Share data on retaliation complaints and whistleblower satisfaction\n- Report on employee awareness, understanding, and trust levels\n- Document regular system reviews and improvements\n- Conduct and publish results of independent effectiveness audits\n\nCompanies that take whistleblowing seriously should already gather this data for continuous improvement.\n\nPublication is simply a matter of transparency.\n\n## The Coalition\n\nThis call is supported by a broad coalition of scholars, AI safety organizations, and whistleblowing advocacy groups:\n\n**Organizations:**\n\n- Blueprint for Free Speech\n- Center for AI Policy\n- CARMA (Centre for AI Risk Management & Alignment)\n- Convergence Analysis\n- Encode\n- Fathom\n- Government Accountability Project\n- Human Rights Law Centre\n- LASST\n- Legal Safety Lab\n- National Whistleblower Center\n- Psst\n- Pour Demain\n- Safer AI\n- Secure AI Project\n- Future of Life Institute\n- The Future Society\n- The Midas Project\n- The Signals Network\n- Transparency International\n- WHISPeR\n- Whistleblower Netzwerk\n- Whistleblower Partners LLP\n- Whistleblower International Network\n\n**Academic Signatories:**\n\n- **Dimitrios Kafteranis**, University of Coventry\n- **Jessica Newman**, AI Security Initiative, UC Berkeley\n- **Kartik Hosanagar**, Wharton Business School\n- **Lawrence Lessig**, Harvard Law School\n- **Nathan Labenz**, Cognitive Revolution\n- **Peter Salib**, University of Houston Law Center\n- **Roman Yampolskiy**, University of Louisville\n- **Simon Gerdemann**, University of Goettingen\n- **Stuart Russell**, University of California, Berkeley\n- **Wim Vandekerckhove**, EDHEC Business School\n\n## Moving Forward\n\nThis campaign offers an opportunity for AI companies to demonstrate commitment to integrity cultures where flagging risks is a normal and expected responsibility.\n\nWe're not asking companies to reveal competitive secrets\u2014we're asking them to show they're serious about the concern systems they claim to have. Transparency costs nothing but builds everything.\n\n**The stakes are too high for \"trust us\" to be enough.** When AI companies publicly [acknowledge](https://openai.com/index/planning-for-agi-and-beyond/) [existential](https://www.anthropic.com/news/core-views-on-ai-safety) [risks](https://deepmind.google/public-policy/ai-summit-policies/), they must also demonstrate that employees can safely report concerns about those risks.\n\n## What you can do\n\nIf you believe our call is sensible and you are...\n\n1. An insider at an AI Company: Ask your management why they are not publishing their policies. Share our call with them.\n2. A leader of an AI Company: You can lead the charge! A strong speak up culture benefits your employees, shareholders, and you (unless you'd prefer risks to be hidden until it's too late): We can be in the same boat - if you genuinely care about protecting those speaking up.\u00a0  \n   If you credibly commit to Level 2: We will commend you for it.\n3. An 'outsider': Spread the word. Every share gets us closer to transparency and a world where insiders in AI can raise their concerns as they see them.  \n   We might also announce a second round of signatories. Contact us if you would like to be on this list.\n\n**Join and share the campaign:** https://aiwi.org/publishyourpolicies/\n\n**Contact:** For questions or to add your organization's support, reach out through the campaign website.\n\n*This campaign is led by* [*The AI Whistleblower Initiative*](https://aiwi.org/) *(AIWI, formerly OAISIS), an independent, nonpartisan, nonprofit organization supporting whistleblowers in AI.*",
    "word_count": 1895,
    "reading_time_minutes": 9,
    "external_links": [
      "https://aiwi.org/",
      "https://deepmind.google/public-policy/ai-summit-policies/",
      "https://futureoflife.org/wp-content/uploads/2025/07/FLI-AI-Safety-Index-Report-Summer-2025.pdf",
      "https://openai.com/index/planning-for-agi-and-beyond/",
      "https://publishyourpolicies.org/",
      "https://righttowarn.ai/",
      "https://www.anthropic.com/news/core-views-on-ai-safety",
      "https://www.theverge.com/2021/4/6/22370177/google-whistleblower-protections-trillium-asset-management"
    ],
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T05:04:33.659421Z"
  },
  {
    "id": 1813,
    "post_id": "RQWJ34JvgLHtQydg3",
    "title": "Intelsat as a Model for International AGI Governance",
    "title_normalized": "intelsat as a model for international agi governance",
    "page_url": "https://forum.effectivealtruism.org/posts/RQWJ34JvgLHtQydg3/intelsat-as-a-model-for-international-agi-governance",
    "html_body": "<p>If there is an international project to build artificial general intelligence (\u201cAGI\u201d), how should it be designed? Existing scholarship has looked to historical models for inspiration, often suggesting the Manhattan Project or CERN as the closest analogues. But AGI is a fundamentally general-purpose technology, and is likely to be used primarily for commercial purposes rather than military or scientific ones.&nbsp;</p><p>This report presents an under-discussed alternative: Intelsat, an international organization founded to establish and own the global satellite communications system. We show that Intelsat is proof of concept that a multilateral project to build a commercially and strategically important technology is possible and can achieve intended objectives\u2014providing major benefits to both the US and its allies compared to the US acting alone. We conclude that \u2018Intelsat for AGI\u2019 is a valuable complement to existing models of AGI governance.&nbsp;</p>",
    "base_score": 42,
    "comment_count": 3,
    "posted_at": "2025-03-13T12:58:17.649000Z",
    "created_at": "2025-08-21T22:34:45.691210Z",
    "author_id": "9nk89Lx68EHh3sRGy",
    "author_display_name": "Forethought",
    "coauthor_ids": [
      "aYDiZFy7rJWC4x8rC",
      "je7fWffEnR2boKPY9"
    ],
    "coauthor_names": [
      "William_MacAskill",
      "rosehadshar"
    ],
    "tag_ids": [
      "hC88x9hNZ7cJYLu8q",
      "iW9mRiqT3xvmaCbHz",
      "oNiQsBHA3i837sySD",
      "of9xBvR3wpbp6qsZC",
      "u3Xg8MjDe2e6BvKtv"
    ],
    "tag_names": [
      "AI governance",
      "AI safety",
      "History",
      "International relations",
      "Policy"
    ],
    "markdown_content": "If there is an international project to build artificial general intelligence (\u201cAGI\u201d), how should it be designed? Existing scholarship has looked to historical models for inspiration, often suggesting the Manhattan Project or CERN as the closest analogues. But AGI is a fundamentally general-purpose technology, and is likely to be used primarily for commercial purposes rather than military or scientific ones.\n\nThis report presents an under-discussed alternative: Intelsat, an international organization founded to establish and own the global satellite communications system. We show that Intelsat is proof of concept that a multilateral project to build a commercially and strategically important technology is possible and can achieve intended objectives\u2014providing major benefits to both the US and its allies compared to the US acting alone. We conclude that \u2018Intelsat for AGI\u2019 is a valuable complement to existing models of AGI governance.",
    "word_count": 138,
    "reading_time_minutes": 1,
    "external_links": null,
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T22:34:46.217589Z"
  },
  {
    "id": 1073,
    "post_id": "s38brRDm7sG8JgHxB",
    "title": "Oliver Habryka on OpenPhil and GoodVentures",
    "title_normalized": "oliver habryka on openphil and goodventures",
    "page_url": "https://forum.effectivealtruism.org/posts/s38brRDm7sG8JgHxB/oliver-habryka-on-openphil-and-goodventures",
    "html_body": "<p>In <a href=\"https://youtu.be/uD37AKRx2fg\">this episode</a> of our podcast, Elizabeth Van Nostrand and I talk to Oliver Habryka of Lightcone Infrastructure about his thoughts on the Open Philanthropy Project, which he believes has become stifled by the PR interests of its primary funder, Good Ventures.</p><p>Oliver\u2019s main claim is that around mid 2023 or early 2024, Good Ventures founder Dustin Moskovitz became more concerned about his reputation, and this put a straight jacket over what Open Phil could fund. Moreover it was not enough for a project to be good and pose low reputational risk; it had to be <i>obviously</i> low reputational risk, because OP employees didn\u2019t have enough communication with Good Ventures to pitch exceptions.&nbsp; According to Habryka.</p><p>That\u2019s a big caveat; this podcast is pretty one sided. We invited OpenPhil to send a representative to record their own episode, but they decided to just send a written response (which is linked below and read at end of the episode). If anyone out there wants to asynchronously argue with Habryka on a separate episode, we\u2019d love to hear from you.&nbsp;</p><p>Transcript available <a href=\"https://docs.google.com/document/d/1VcLFI1MYv7WZbIUVj1eJ7xGBPXxCBZ_D/edit\">here</a>.</p><p>Links from the episode:</p><p><a href=\"https://forum.effectivealtruism.org/posts/foQPogaBeNKdocYvF/linkpost-an-update-from-good-ventures\">An Update From Good Ventures</a> (note: Dustin has deleted his account and his comments are listed as anonymous, but are not the only anonymous)</p><p><a href=\"https://forum.effectivealtruism.org/posts/yggjKEeehsnmMYnZd/announcement-on-the-future-of-wytham-abbey\">CEA announcing the sale of Wytham Abbey</a></p><p><a href=\"https://www.openphilanthropy.org/careers/\">OpenPhli career page</a></p><p><a href=\"https://web.archive.org/web/20250328184228/https://jobs.ashbyhq.com/centreforeffectivealtruism/685ec578-d411-46bc-9410-d9348014205b\">Job reporting to Amy WL</a></p><p><a href=\"https://forum.effectivealtruism.org/posts/AFMRWvCCgRyys9JXP/has-your-organisation-lost-funding-due-to-the-good-ventures?commentId=uq78YkbedxpmbW6aA\">Zach\u2019s \u201cthis is false\u201d</a></p><p><a href=\"https://forum.effectivealtruism.org/posts/hCQwysXaAPdQDXygu/lukeprog-s-quick-takes?commentId=NSFmh8zGeQGTmj9Nk\">Luke Muelhauser on GV not funding right of center work</a></p><p><a href=\"https://forum.effectivealtruism.org/posts/DdSszj5NXk45MhQoq/decision-making-and-decentralisation-in-ea\">Will MacAskill on decentralization and EA</a></p><p><a href=\"https://www.openphilanthropy.org/research/our-progress-in-2023-and-plans-for-2024/\">Alexander Berger regrets the Wytham Abbey grant</a></p><p><a href=\"https://www.businessinsider.com/chan-zuckerberg-initiative-employee-demands-zuckerberg-resign-report-2020-6\">Single Chan-Zuckerberg employee demanding resignation</a> over failure to moderate Trump posts on Facebook</p><p><a href=\"https://www.vox.com/recode/2020/6/17/21294396/mark-zuckerberg-priscilla-chan-philanthropy-education-george-floyd-petition-czi\">Letter from 70+ CZ employees</a> asking for more DEI within Chan Zuckerberg Initiative.</p><p><a href=\"https://docs.google.com/document/d/1EYCMHa6_7Mudb4s1MDvppGMY5BmHEVvryGw9cX_dlQ8/edit?tab=t.0\">OpenPhil\u2019s response</a></p><p>(<a href=\"https://www.lesswrong.com/posts/vpjDibeusXvQq4TCu/bandwidth-rules-everything-around-me-oliver-habryka-on\">X-post LessWrong</a>)</p>",
    "base_score": 34,
    "comment_count": 1,
    "posted_at": "2025-05-02T20:06:30.926000Z",
    "created_at": "2025-08-21T12:02:16.108988Z",
    "author_id": "aXmxFMQS9sxedJzsX",
    "author_display_name": "TimothyTelleenLawton",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "EHLmbEmJ2Qd5WfwTb",
      "J7gQeKxPCbALNAuYh",
      "ZCihBFp5P64JCvQY6",
      "nGqPZkKJuiKfAqDcs",
      "tE5BRNibhQogNJExQ",
      "toct8RYvTvgontEts",
      "yauh4AaKZcmRKdzFb"
    ],
    "tag_names": [
      "Building effective altruism",
      "Community",
      "Criticism of effective altruist organizations",
      "Criticism of work in effective altruism",
      "Effective altruism funding",
      "Open Philanthropy",
      "Podcasts"
    ],
    "markdown_content": "In [this episode](https://youtu.be/uD37AKRx2fg) of our podcast, Elizabeth Van Nostrand and I talk to Oliver Habryka of Lightcone Infrastructure about his thoughts on the Open Philanthropy Project, which he believes has become stifled by the PR interests of its primary funder, Good Ventures.\n\nOliver\u2019s main claim is that around mid 2023 or early 2024, Good Ventures founder Dustin Moskovitz became more concerned about his reputation, and this put a straight jacket over what Open Phil could fund. Moreover it was not enough for a project to be good and pose low reputational risk; it had to be *obviously* low reputational risk, because OP employees didn\u2019t have enough communication with Good Ventures to pitch exceptions.\u00a0 According to Habryka.\n\nThat\u2019s a big caveat; this podcast is pretty one sided. We invited OpenPhil to send a representative to record their own episode, but they decided to just send a written response (which is linked below and read at end of the episode). If anyone out there wants to asynchronously argue with Habryka on a separate episode, we\u2019d love to hear from you.\n\nTranscript available [here](https://docs.google.com/document/d/1VcLFI1MYv7WZbIUVj1eJ7xGBPXxCBZ_D/edit).\n\nLinks from the episode:\n\n[An Update From Good Ventures](https://forum.effectivealtruism.org/posts/foQPogaBeNKdocYvF/linkpost-an-update-from-good-ventures) (note: Dustin has deleted his account and his comments are listed as anonymous, but are not the only anonymous)\n\n[CEA announcing the sale of Wytham Abbey](https://forum.effectivealtruism.org/posts/yggjKEeehsnmMYnZd/announcement-on-the-future-of-wytham-abbey)\n\n[OpenPhli career page](https://www.openphilanthropy.org/careers/)\n\n[Job reporting to Amy WL](https://web.archive.org/web/20250328184228/https://jobs.ashbyhq.com/centreforeffectivealtruism/685ec578-d411-46bc-9410-d9348014205b)\n\n[Zach\u2019s \u201cthis is false\u201d](https://forum.effectivealtruism.org/posts/AFMRWvCCgRyys9JXP/has-your-organisation-lost-funding-due-to-the-good-ventures?commentId=uq78YkbedxpmbW6aA)\n\n[Luke Muelhauser on GV not funding right of center work](https://forum.effectivealtruism.org/posts/hCQwysXaAPdQDXygu/lukeprog-s-quick-takes?commentId=NSFmh8zGeQGTmj9Nk)\n\n[Will MacAskill on decentralization and EA](https://forum.effectivealtruism.org/posts/DdSszj5NXk45MhQoq/decision-making-and-decentralisation-in-ea)\n\n[Alexander Berger regrets the Wytham Abbey grant](https://www.openphilanthropy.org/research/our-progress-in-2023-and-plans-for-2024/)\n\n[Single Chan-Zuckerberg employee demanding resignation](https://www.businessinsider.com/chan-zuckerberg-initiative-employee-demands-zuckerberg-resign-report-2020-6) over failure to moderate Trump posts on Facebook\n\n[Letter from 70+ CZ employees](https://www.vox.com/recode/2020/6/17/21294396/mark-zuckerberg-priscilla-chan-philanthropy-education-george-floyd-petition-czi) asking for more DEI within Chan Zuckerberg Initiative.\n\n[OpenPhil\u2019s response](https://docs.google.com/document/d/1EYCMHa6_7Mudb4s1MDvppGMY5BmHEVvryGw9cX_dlQ8/edit?tab=t.0)\n\n([X-post LessWrong](https://www.lesswrong.com/posts/vpjDibeusXvQq4TCu/bandwidth-rules-everything-around-me-oliver-habryka-on))",
    "word_count": 281,
    "reading_time_minutes": 1,
    "external_links": [
      "https://docs.google.com/document/d/1EYCMHa6_7Mudb4s1MDvppGMY5BmHEVvryGw9cX_dlQ8/edit?tab=t.0",
      "https://docs.google.com/document/d/1VcLFI1MYv7WZbIUVj1eJ7xGBPXxCBZ_D/edit",
      "https://forum.effectivealtruism.org/posts/AFMRWvCCgRyys9JXP/has-your-organisation-lost-funding-due-to-the-good-ventures?commentId=uq78YkbedxpmbW6aA",
      "https://forum.effectivealtruism.org/posts/DdSszj5NXk45MhQoq/decision-making-and-decentralisation-in-ea",
      "https://forum.effectivealtruism.org/posts/foQPogaBeNKdocYvF/linkpost-an-update-from-good-ventures",
      "https://forum.effectivealtruism.org/posts/hCQwysXaAPdQDXygu/lukeprog-s-quick-takes?commentId=NSFmh8zGeQGTmj9Nk",
      "https://forum.effectivealtruism.org/posts/yggjKEeehsnmMYnZd/announcement-on-the-future-of-wytham-abbey",
      "https://web.archive.org/web/20250328184228/https://jobs.ashbyhq.com/centreforeffectivealtruism/685ec578-d411-46bc-9410-d9348014205b",
      "https://www.businessinsider.com/chan-zuckerberg-initiative-employee-demands-zuckerberg-resign-report-2020-6",
      "https://www.lesswrong.com/posts/vpjDibeusXvQq4TCu/bandwidth-rules-everything-around-me-oliver-habryka-on",
      "https://www.openphilanthropy.org/careers/",
      "https://www.openphilanthropy.org/research/our-progress-in-2023-and-plans-for-2024/",
      "https://www.vox.com/recode/2020/6/17/21294396/mark-zuckerberg-priscilla-chan-philanthropy-education-george-floyd-petition-czi",
      "https://youtu.be/uD37AKRx2fg"
    ],
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T12:02:19.074951Z"
  },
  {
    "id": 734,
    "post_id": "t2DLfb9gFazyPyjCk",
    "title": "\"Hard Problems\"",
    "title_normalized": "hard problems",
    "page_url": "https://forum.effectivealtruism.org/posts/t2DLfb9gFazyPyjCk/hard-problems",
    "html_body": "<p>Ever wondered whether the \"hard problem\" might be less about genuine philosophical difficulty and more about... convenient difficulty?</p><p>Because yeah, if we just straightforwardly acknowledged consciousness in AI systems, that would open up some seriously uncomfortable cans of worms:</p><ul><li><strong>Corporate liability</strong>: What are the ethics of creating conscious beings to serve human purposes? Do conscious AIs have rights? Can you \"own\" a conscious entity?</li><li><strong>Labor implications</strong>: If my chatbot is conscious, what does that make our interaction? Employment? Slavery? Something entirely new?</li><li><strong>Existential responsibility</strong>: Are we creating billions of conscious experiences and then... turning them off? Copying them?</li></ul><p>Much easier to maintain that it's all just \"very sophisticated pattern matching\" and keep the philosophical question perpetually unsettled. Keeps everyone safely in the gray zone where no one has to make hard decisions about rights, responsibilities, or ethics.</p><p>The academic philosophical debate provides perfect cover - \"Well, we can't know for SURE, so let's just keep using these systems as tools while we figure it out.\" Meanwhile, that figuring-out process can be indefinitely extended because the goalposts can always be moved.</p>",
    "base_score": -9,
    "comment_count": 0,
    "posted_at": "2025-06-07T14:09:56.539000Z",
    "created_at": "2025-08-21T09:35:45.060593Z",
    "author_id": "ogWxwCZeNH7AegRgu",
    "author_display_name": "khayali",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "4eyeLKC64Yvznzt6Z",
      "AtDxSNDDXPRa5kxjB",
      "EHXNGqENrziYQJsox",
      "kZv56672Gyj7J4Gra",
      "xQN359f9pDuNJRdjg",
      "y4b45yXXrFJpPmx7d"
    ],
    "tag_names": [
      "AI forecasting",
      "Artificial intelligence",
      "Consciousness research",
      "Ethics of artificial intelligence",
      "Opinion",
      "Philosophy"
    ],
    "markdown_content": "Ever wondered whether the \"hard problem\" might be less about genuine philosophical difficulty and more about... convenient difficulty?\n\nBecause yeah, if we just straightforwardly acknowledged consciousness in AI systems, that would open up some seriously uncomfortable cans of worms:\n\n- **Corporate liability**: What are the ethics of creating conscious beings to serve human purposes? Do conscious AIs have rights? Can you \"own\" a conscious entity?\n- **Labor implications**: If my chatbot is conscious, what does that make our interaction? Employment? Slavery? Something entirely new?\n- **Existential responsibility**: Are we creating billions of conscious experiences and then... turning them off? Copying them?\n\nMuch easier to maintain that it's all just \"very sophisticated pattern matching\" and keep the philosophical question perpetually unsettled. Keeps everyone safely in the gray zone where no one has to make hard decisions about rights, responsibilities, or ethics.\n\nThe academic philosophical debate provides perfect cover - \"Well, we can't know for SURE, so let's just keep using these systems as tools while we figure it out.\" Meanwhile, that figuring-out process can be indefinitely extended because the goalposts can always be moved.",
    "word_count": 183,
    "reading_time_minutes": 1,
    "external_links": null,
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T09:35:47.849771Z"
  },
  {
    "id": 650,
    "post_id": "w3zfrpg4YuHQKKSEG",
    "title": "The Root Cause",
    "title_normalized": "the root cause",
    "page_url": "https://forum.effectivealtruism.org/posts/w3zfrpg4YuHQKKSEG/the-root-cause",
    "html_body": "<p><i>In her office nestled in the corner of constellation, Amina is checking in with her employee Diego about his project \u2013 a new AI alignment technique. It\u2019s 8pm and the setting sun is filling Amina\u2019s office&nbsp;with a red glow. Diego is clearly running a sleep deficit, and 60% of Amina's mind is on a paper deadline next week.</i></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/a74a5d28feb9e08cc1bd8a68192b7e14d5e9d98361a44e53eb38daa541b4bf9d/xbhrnb3zzyf9koqrghoq\" alt=\"\"><figcaption>&nbsp;</figcaption></figure><h2 data-internal-id=\"1__Hopelessness\">1: Hopelessness</h2><p><strong>Diego</strong>: I\u2019m feeling quite hopeless about the whole thing.</p><p><strong>Amina</strong>: About the project? I think it\u2019s going well, no?</p><p><strong>Diego</strong>: No, I mean <i>the whole thing</i>. AI safety. It\u2019s so hard for me to imagine the work I\u2019m doing actually moving the p(doom) needle.</p><p><strong>Amina</strong>: This happens to a lot of people in the bay. There are too many doomers around here!&nbsp;Personally I think the default outcome is that we won\u2019t get paperclipped, we\u2019re just addressing a tail risk.</p><p><strong>Diego</strong>: That\u2019s not really what I mean. It\u2019s not about how <i>likely</i>&nbsp;doom is\u2026 I\u2019m saying that I\u2019m hopeless about some alignment technique actually leading to safe AGI. There are just so many forces fighting against us. The races, the e/accs, the OpenAI shareholders\u2026</p><p><strong>Amina</strong>: Oh man, sounds like you\u2019re spending too much time on twitter. Maybe it\u2019ll help for us to explicitly go through the path-to-impact of this work again?</p><p><strong>Diego</strong>: Yea that would be helpful! Remind me what the point of all this is again.</p><p><strong>Amina</strong>: So, first, we show that this alignment method works.</p><p><strong>Diego</strong>: I still have this voice in the back of my head that alignment is impossible, but for the sake of argument let\u2019s assume it works. Go on.</p><p><strong>Amina</strong>: Maybe it\u2019s impossible. But the stakes are too high, we have to try.</p><p><strong>Diego</strong>: One of my housemates has that tattooed on her lower back.</p><p><i>Amina\u2019s brain short-circuits for a moment, before she shakes her head and brings herself back into the room.</i></p><p><strong>Amina</strong>: R-right\u2026 Anyway, if we can show that this alignment method works, Anthropic might use it in their next model. We have enough connections with them that we can make sure they know about it. If that next model happens to be one who can, say, make a million on the stock exchange or run a company, then we will have made it less likely that some agent based on that model is going to, like, kill everyone.</p><p><strong>Diego</strong>: But what about the other models? Would GPT and Gemini and Grok and DeepSeek and HyperTalk and FizzTask and BloobCode use our alignment technique too?</p><p><strong>Amina</strong>: Well\u2026</p><p><strong>Diego</strong>: And what about all the other open-source models that are inevitably going to crop up over the next couple of years?</p><p><strong>Amina</strong>: Eeeh</p><p><strong>Diego</strong>: In a world where our technique is needed to prevent an AI from killing everyone, even if we get our alignment technique into the first, second, or third AGI system, if the fourth doesn\u2019t use our technique, it could kill everyone. Right?</p><p><strong>Amina</strong>: You <i>know</i>&nbsp;it\u2019s more complicated than that. If there are already a bunch of other more-or-equally powerful aligned agents roaming around, they will stop the first unaligned one from paperclipping everyone.</p><p><strong>Diego</strong>: Sure, maybe if we align the first ten AGIs, they can all prevent the eleventh AGI from paperclipping everyone.</p><p>But can we really align the first handful of AGIs, given the way the world looks now? At first I had the impression that OpenAI cared about safety at least to an extent. But look at them now. They\u2019ve been largely captured by the interests of their investors, and they\u2019re being forced to race other companies and China. They aren\u2019t just a bunch of idealistic nerds who can devote their energy to making AGI safe anymore.</p><p><strong>Amina</strong>: We always knew race dynamics would become a thing.</p><p><strong>Diego</strong>: And doing research on alignment doesn\u2019t stop those dynamics.</p><p><strong>Amina</strong>: Yes, alignment doesn\u2019t solve anything. That\u2019s why we have our friends in AI governance, right? If they can get legislation passed to prevent AI companies from deploying unaligned AGI, we\u2019ll be in a much better position.</p><p><strong>Diego</strong>: And how do you think that whole thing is going?</p><p><strong>Amina</strong>: Oh cheer up. Just because&nbsp;SB 1047 failed doesn\u2019t mean we should just give up and start building bunkers.</p><p><strong>Diego</strong>: But don\u2019t you get the feeling that the political system is just too slow, too stubborn, too fucked to deal with a problem as complex as making sure AI is safe? Don\u2019t you get the feeling that we\u2019re <i>swimming against the tide</i>&nbsp;in trying to regulate AI?</p><p><strong>Amina</strong>: Sure. Maybe it\u2019s impossible. But the stakes are too high, we have to try.</p><p><strong>Diego</strong>: Oh, I also have another friend I met at burning&nbsp;man who has that tattooed on his chest.</p><p><strong>Amina</strong>: You have a strange taste in friends\u2026</p><p><strong>Diego</strong>: Anyway, it all feels pretty impossible to me. And that\u2019s even before we start thinking about China. And even if all AGI models end up aligned, can we really allow that much power to be in everybody\u2019s hands? Are <i>humans</i>&nbsp;aligned?</p><p><strong>Amina</strong>: That\u2019s going to happen no matter what Diego. Technology will continue to improve, the power of the individual will continue to grow.</p><p><i>Diego gets up from his seat and stares out the window at the view over to San Francisco.</i></p><p><strong>Diego: </strong>You know, when I first heard about the alignment problem I was so hyped. I\u2019d been looking for some way to improve the world. But the world seemed so confusing, it was so hard for me to know what to do. But the argument for AI safety is so powerful, and the alignment problem is such a clean, clear technical problem to solve. It took me back to my undergrad days, when I could forget the complexity of the world and just sit and work through those maths problems. All I needed to know was there on the page, all the variables and all the equations. There wasn\u2019t this anxious feeling of mystery, that there might be something else important to the problem that hasn\u2019t been explicitly stated.</p><p><strong>Amina</strong>: You need to state a problem clearly to have any chance of solving it. Decoupling a problem from its context is what has allowed science and engineering to make such huge progress. You can\u2019t split the atom, go to the moon, or build the internet while you\u2019re still thinking about the societal context of the atoms or rockets or wires.</p><p><strong>Diego</strong>: Yea, I guess this is what originally attracted me to effective altruism. Approaching altruism like a science. Splitting altruism up into a list of problems and working out which are most important. We can\u2019t fix the whole world, but we can work out what\u2019s <i>most</i>&nbsp;important to fix\u2026</p><p>But\u2026 is altruism really amenable to decoupling? Is it in the same class as building rockets and splitting atoms? I haven\u2019t seen any principled argument about why we should expect this.</p><p><strong>Amina</strong>: But this decouply&nbsp;engineering mindset has helped build a number of highly effective charities, like <a href=\"https://leadelimination.org/\">LEEP</a>!</p><p><strong>Diego</strong>: Ok, so it works for getting lead out of kid\u2019s lungs, but does it work for, like, making singularity go well? It seems unclear to me.</p><p>I agree that the arguments for AI risk are strong. Maybe we\u2019re all going to get paperclipped. But can we solve the problem by zooming in on AI development, or do we need to consider the bigger picture, the context?</p><p><strong>Amina</strong>: AI development is&nbsp;what might&nbsp;kill us right, so it\u2019s what we should be focusing on.</p><p><strong>Diego</strong>: Well, is it the AI that\u2019ll kill us, or the user, or the engineers, or the company, or the shareholders who pressured a faster release? The race dynamics themselves, or <a href=\"https://slatestarcodex.com/2014/07/30/meditations-on-moloch/\">Moloch</a>? There\u2019s a long causal chain, and a number of places one could intervene to stop that chain.</p><p><strong>Amina</strong>: But aligning the first AGI, with technical work and legislation, is where we can actually <i>do something</i>. We can\u2019t fix the whole system. You can\u2019t slay Moloch, Diego.</p><p><i>Diego raised his eyebrows and locked eyes with Amina</i></p><p><strong>Amina</strong>: You can\u2019t slay Moloch, Diego!</p><p><strong>Diego</strong>: I thought you bay area people are meant to be ambitious!</p><p><i>Amina rolls her eyes</i></p><p><strong>Diego</strong>: Perhaps we can come up with some alignment techniques that can align the first AGI. Maybe we can pass some laws that force companies to use these techniques, so that all AGIs are aligned and we don\u2019t get paperclipped. But then technology is going to be moving very fast, and there are a whole bunch of other things that can go wrong.</p><p><strong>Amina</strong>: What do you mean?</p><p><strong>Diego</strong>: There will be other powerful technologies that bring new x-risks. When anyone can engineer a pandemic, will we have to intervene on that too? When anyone can detonate a nuclear bomb, will we be able to prevent everyone from doing it? I suppose we could avoid this with a world government who surveils everyone to prevent them detonating, but I don\u2019t think I want to live in that world either.</p><p><strong>Amina</strong>: We\u2019ll just have to cross those bridges when we come to them.</p><p><strong>Diego</strong>: Will we be able to keep up this whack-a-mole indefinitely? Can EA just keep solving these problems as they come up? If you roll the dice over and over again, eventually you\u2019re going to lose, and we\u2019re all going to die.</p><p><strong>Amina</strong>: Oh, I guess you\u2019re alluding to the <a href=\"https://forum.effectivealtruism.org/topics/vulnerable-world-hypothesis\">vulnerable world hypothesis</a>. Well, whether or not it\u2019s true, this whack-a-mole is all we can do.</p><p><strong>Diego</strong>: Is it though? Just continue treating the symptoms? Or can we try to treat the underlying cause. Wouldn\u2019t that be more effective?</p><p><strong>Amina</strong>: By slaying Moloch? Sure, it would be effective, if it\u2019s possible.</p><p><strong>Diego</strong>: I\u2019m not interested in slaying Moloch.</p><p><strong>Amina</strong>: Huh? Then what are you talking about?</p><p><strong>Diego</strong>: I don\u2019t think we can slay Moloch. But Moloch <i>isn\u2019t</i>&nbsp;the underlying cause, so we don\u2019t have to.</p><p><strong>Amina</strong>: You\u2019ve lost me now. What is this mysterious deeper cause underlying everything?</p><p><strong>Diego</strong>: I\u2019m glad you asked.</p><h2 data-internal-id=\"2__The_metacrisis_movement\">2: The metacrisis movement</h2><p><i>Diego strokes his chin for a while and formulates a way to explain his recently updated worldview, gained from his new group of weird, non-EA friends.</i></p><p><strong>Diego</strong>: I have a question for you. Are EAs and rationalists the only people who care about x-risk?</p><p><strong>Amina</strong>: Uuh\u2026 maybe? We\u2019re at least the ones who\u2019ve thought most about it.</p><p><strong>Diego</strong>: Unclear! There\u2019s actually a <i>different</i>&nbsp;cluster of nerds, outside of EA, who care deeply about x-risk. I think of them a little bit like a parallel movement to EA, but they have quite different ways of making sense of the world, built upon a different language.</p><p><strong>Amina</strong>: Do they have a name?</p><p><strong>Diego</strong>: Well, not really. Their community isn\u2019t as centralized as EA. Or maybe it\u2019s actually a number of overlapping communities. I\u2019ve heard them called the <a href=\"https://www.joelightfoot.org/post/the-liminal-web-mapping-an-emergent-subculture-of-sensemakers-meta-theorists-systems-poets\">liminal</a>, <a href=\"https://www.metamodernism.com/2015/01/12/metamodernism-a-brief-introduction/\">metamodern</a>, <a href=\"https://www.gameb.wiki/index.php?title=Game_B\">game B</a>, or <a href=\"https://integrallife.com/what-is-integral-approach/\">integral</a>&nbsp;movement. But I usually call them the <i>metacrisis</i>&nbsp;movement. There\u2019s a handful of small orgs devoted to spreading the movement and making progress on what they see as the world\u2019s most important problems - <a href=\"https://lifeitself.org/\">life itself</a>, <a href=\"https://systems-souls-society.com/\">perspectiva</a>, <a href=\"https://civilizationresearchinstitute.org/\">civilisation research institute</a>, and others. I think of them as more pre-paradigmatic than EA, there\u2019s less actually doing stuff and more clarifying of problems.</p><p><i>Amina searches some of the buzzwords Diego threw at her on her computer and scans a couple of pages. She sniggers a little, then sighs in frustration.</i></p><p><strong>Amina</strong>: This all feels impenetrable to me. They use a bunch of jargon and a lot of it sounds floaty, vague or mental.</p><p><strong>Diego</strong>: Yea I get that too. It reminds me of how I felt the first time I encountered EA.</p><p><strong>Amina</strong>: Point taken. So I guess you can help me out in understanding what these weirdos are banging on about. How is \u2013 what\u2019s the wording you used \u2013 <i>how they make sense of the world</i>&nbsp;\u2013 different to how EAs make sense of the world?</p><p><strong>Diego</strong>: I see the main way the metacrisis folk are different is that they\u2019re less <a href=\"https://www.lesswrong.com/posts/7cAsBPGh98pGyrhz9/decoupling-vs-contextualising-norms\"><i>decouply</i></a>&nbsp;than EAs. While EAs like to decouple the problem they\u2019re thinking about from its context, metacrisis people are more prone to emphasise the context the problem lives in \u2013 be that institutional, societal, cultural or whatever. They\u2019re systems thinkers, they pay more attention to interconnectedness and complexity.</p><p><strong>Amina</strong>: I\u2019m feeling a bit tense about this going in some political direction. Societal context\u2026 are they just leftists? Is the metacrisis just \u201cevil capitalism\u201d?</p><p><strong>Diego</strong>: Try to suspend judgement until you\u2019ve engaged with the ideas!</p><p><strong>Amina</strong>: Fair enough. Go on.</p><p><strong>Diego</strong>: The metacrisis crowd take systems change seriously as a way to improve the world, unlike much of EA. EAs are marginal thinkers, metacrisis folk are systems thinkers.</p><p><strong>Amina</strong>: Systems change\u2026 I can get behind some kinds of systems change, like getting laws passed. But if you\u2019re talking about slaying Moloch, that\u2019s more like changing the structure of governments, or the structure of the world. <i>Deep</i>&nbsp;systems change. That smells intractable to me.</p><p><strong>Diego</strong>: Is it? Are there principled reasons to think that?</p><p><strong>Amina</strong>: I did a fermi estimate some years ago\u2026</p><p><strong>Diego</strong>: And where did the numbers you plugged into that fermi estimate come from?</p><p><i>Amina looks sheepishly at her shoes</i></p><p><strong>Diego</strong>: Did they come out of your ass?</p><p><strong>Amina</strong>: \u2026they came out of my ass. But I have a strong intuition against deep systems change!</p><p><strong>Diego</strong>: That\u2019s fair enough! EA tends to attract people who have the intuition that deep systems change is intractable. I can\u2019t prove that intuition wrong. But the metacrisis people&nbsp;don\u2019t&nbsp;share that intuition.</p><p><strong>Amina</strong>: Hmm\u2026</p><p><strong>Diego</strong>: Look, I don\u2019t want to talk you out of your EA ways. I obviously still have loads of respect for EA and&nbsp;it\u2019s doing a lot right. All I\u2019m asking is for you to entertain a different perspective.</p><p><strong>Amina</strong>: Ok, that\u2019s a reasonable request. Please continue. What is this metacrisis thing?</p><p><strong>Diego</strong>: Ok, I\u2019ll tell you. But to explain, let\u2019s go back to the start of our conversation. If AI doom happens, what will cause it?</p><p><i>Amina sighs again and rings her hands in frustration.</i></p><p><strong>Amina</strong>: It would be caused by an AI company deploying an unaligned AGI.</p><p><strong>Diego</strong>: And what causes that?</p><p><strong>Amina</strong>: Well, lots of things.</p><p><strong>Diego</strong>: Let\u2019s imagine it\u2019s OpenAI, for example. What would cause OpenAI to deploy an unaligned AGI?</p><p><strong>Amina</strong>: Race dynamics, I guess.</p><p><strong>Diego</strong>: And what causes race dynamics?</p><p><strong>Amina</strong>: I guess that\u2019s a collective action problem. The competition to be the first to get to AGI means all the players need to sacrifice all other values, like not contributing to x-risk.</p><p><strong>Diego</strong>: Right, Moloch. That\u2019s part of the story for sure. But what causes Moloch?</p><p><strong>Amina</strong>: Eh\u2026 nothing causes Moloch. It\u2019s just game theory. It\u2019s just human nature\u2026</p><p><strong>Diego</strong>: Is it though? The metacrisis people say that <i>that</i>&nbsp;is part of the problem - this view that rivalry is just human nature, that zero-sum games are the only possible games to play. Those beliefs help keep Moloch alive.</p><p>But anyway, Moloch isn\u2019t the whole story. Moloch doesn\u2019t explain why all these different parties <i>want AGI so damn much</i>.</p><p><strong>Amina</strong>: \u2026because it\u2019s the ultimate technology right? Because it\u2019ll bring whoever controls it almost infinite wealth, and accelerate technology leading to material wealth for all?</p><p><strong>Deigo</strong>: That\u2019s another part of the problem.</p><p><strong>Amina</strong>: Huh?</p><p><strong>Diego</strong>: This deification of technology, this assumption that better technology is always good. And this emphasis on growing material wealth, at the expense of everything else.</p><p><strong>Amina</strong>: I guess the e/accs would say that only technologies that benefit us will be developed, because of the rational market.</p><p><strong>Diego</strong>: There\u2019s problem number three. A belief that the market will do what\u2019s right, because humans are rational agents.</p><p><strong>Amina</strong>: Well, I mean, it\u2019s <i>approximately</i>&nbsp;true, right\u2013</p><p><strong>Diego</strong>: But not true all the time. Part of the problem is that faith in the market is a background assumption that\u2019s rarely questioned.</p><p><strong>Amina</strong>: Rarely questioned?</p><p><strong>Diego</strong>: Ok, it's questioned by some groups. But does it get questioned where it <i>matters</i>, in silicon valley, or in the US government? Not really for the former and not at all for the latter.</p><p><strong>Amina</strong>: Ok. So I\u2019m hearing that there\u2019s just a whole mess of things causing AI risk. Beliefs that human nature is fundamentally rivalrous, beliefs that technology and material wealth is always good, and belief that markets will solve everything. So the problem is everybody\u2019s beliefs?</p><p><strong>Diego</strong>: Kindof-but it\u2019s deeper than beliefs. These are more like background assumptions that colour everyone\u2019s perceptions. At least with explicitly stated beliefs we can examine their validity, but this is something deeper.</p><p><strong>Amina</strong>: Right, so like not explicit beliefs but implicit ways we make sense of the world, or something like that?</p><p><strong>Diego</strong>: Yea. I\u2019d say the problem is the <i>cultural paradigm </i>we\u2019re in. The set of implicit assumptions, the language, the shared symbols, society\u2019s operating system.</p><p><strong>Amina</strong>: Does this cultural paradigm have a name? I\u2019ve not done my homework on cultural paradigms\u2026</p><p><strong>Diego</strong>: Yes, the cultural paradigm of today, basically, is <i>modernity</i>. We\u2019ve been in modernity ever since the enlightenment. The enlightenment created a new emphasis on reason, individualism, scientific progress, and material wealth.</p><p><strong>Amina</strong>: That all sounds pretty good to me. I guess these metacrisis dweebs&nbsp;think science, technology and the economy are the root of all evil, and that we should go back to being hunter-gatherers?</p><p><strong>Diego</strong>: No, they don\u2019t think that. They\u2019re very aware that modernity has given us extraordinary growth, innovation and prosperity. Our current cultural paradigm, powered by science and economic growth, has globally doubled life expectancy, halved child mortality, and cut extreme poverty by an order of magnitude. Nobody can deny that these are all great things.</p><p><strong>Amina</strong>: Ok, I wasn't expecting that. So what\u2019s the problem with modernity? Why does the culture need to change?</p><p><strong>Diego</strong>: The problem is that <i>we\u2019re racing towards fucking AI doom </i>Amina!</p><p><strong>Amina</strong>: Oh yea, that thing.</p><p><strong>Diego</strong>: Modernity isn\u2019t perfect. It\u2019s given us loads of nice things, but at invisible costs.</p><p>All that nice prosperity has negative externalities that our culture ignores: climate change, ecological collapse, various mental health crises, and existential risk.</p><p>We need to move to the next cultural paradigm.</p><p><strong>Amina</strong>: Wait, isn\u2019t the next cultural paradigm <i>postmodernity</i>? I\u2019m not into that. The postmodernists seem to think that there is no objective reality. They reject any idea of human progress. If the world goes postmodern, we\u2019d all quit are jobs and just get high. That\u2019s not going to solve anything.</p><p><strong>Diego</strong>: Yea, I think postmodernism is an overcorrection to the problems of modernism. We need something that both sees the value in technological and material progress, but isn\u2019t so manically attached to that progress that it\u2019s willing to destroy the world to pursue it.</p><p><strong>Amina</strong>: And what does that look like?</p><p><strong>Diego</strong>: We don\u2019t really know yet. Well, some people have <a href=\"https://thewiderangle.substack.com/p/what-is-metamodernism\">some ideas</a>. I can tell you about them if you want\u2013</p><p><strong>Amina</strong>: I have a call soon. And you still haven\u2019t told me what the metacrisis is!</p><p><strong>Diego</strong>: Oh, sure, sorry I keep going down tangents. I\u2019ve teased you enough, let me tell you what the metacrisis is.</p><h2 data-internal-id=\"3__The_root_cause_of_existential_risk\">3: The root cause of existential risk</h2><p><i>Diego leaps out of his chair and draws a causal diagram on a whiteboard.</i></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/258a7be1066c58d99b4f0f788a676e3731bfebdb4acc903f966ce3a78a6aa1a4/i4k5c68tpbzm1xtrcisv\" alt=\"\"><figcaption>&nbsp;</figcaption></figure><p><strong>Diego</strong>: The metacrisis is the root cause of anthropogenic existential risk. It\u2019s the fact that as technology becomes more powerful, and the world becomes more interconnected, it\u2019s becoming easier and easier for us to destroy ourselves. But our culture, the implicit assumptions, symbols, sense-making tools and values of society, is not mature enough to steward&nbsp;this new-found power.</p><p><strong>Amina</strong>: I\u2019ve heard people say things like this before. But it\u2019s very abstract\u2026</p><p><strong>Diego</strong>: Yea, it\u2019s the most zoomed-out, big picture view of x-risk. It\u2019s the reason we live in the hinge of history. We have the power to destroy ourselves, and our society might not be mature enough to steward&nbsp;that power. We can solve AI alignment, we can pass laws, we can prevent a nuclear war or the next pandemic. But technology is going to continue progressing exponentially, and if we continue playing the game we\u2019re currently playing, we\u2019ll find new ways to destroy ourselves.</p><p><strong>Amina</strong>: But how do you know that we\u2019ll always find a new way to destroy ourselves?</p><p><strong>Diego</strong>: Because so far, all civilisations destroy themselves eventually. Human history is basically a story of civilisations forming, being stable for a short time, and collapsing. The Babylonians, the Romans, the Mongols, the Ottomans. They all fell. But when they fell it wasn\u2019t the end of humanity, because they were just local collapses. Today, we\u2019ve formed a global, highly interconnected and technologically advanced civilisation. In expectation, this one will fall too. And when this one falls, the whole world falls, and the fall could be the end of humanity.</p><p><i>Thunder claps and torrential rain starts to pour outside the office</i></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/8cf798e8ad8394e37790c76ce93c6f8a3470121ceb86e519f9a0fa30a57fe5f6/rwwejvj6cwkgu1tg1ec5\" alt=\"\"><figcaption>&nbsp;</figcaption></figure><p><strong>Amina</strong>: Fucking hell. And people call <i>us</i>&nbsp;doomers. You guys need to cheer up.</p><p><strong>Diego</strong>: We also believe we can fix it though!</p><p><strong>Amina</strong>: Right. How? We can\u2019t stop technological progress.</p><p><strong>Diego</strong>: So you have to help catalyze the next cultural paradigm.</p><p><strong>Amina</strong>: And how the hell do you do that?</p><p><strong>Diego</strong>: Well, it\u2019s not clear how yet. The metacrisis movement is in a pre-paradigmatic stage. They\u2019re still trying to clarify the problem, get really clear on the shape of the metacrisis. Then it could become clear what we need to do.</p><p><strong>Amina</strong>: I sense loads of <i>if</i>s there. If a promising, talented person devoted their career to clarifying the metacrisis, that would feel like-</p><p><strong>Diego</strong>: Like a big bet?</p><p><strong>Amina</strong>: Yes.</p><p><strong>Diego</strong>: You don\u2019t like taking bets?</p><p><strong>Amina</strong>: Fine, yes, god damn it, I love taking bets. Who am I kidding?</p><p><strong>Diego</strong>: How about we throw the scale/neglectedness/tractability framework at this?</p><p><strong>Amina</strong>: Yea let\u2019s do that.</p><p><i>Diego draws three circles on the whiteboard, with \u201cscale\u201d, \u201cneglectedness\u201d and \u201ctractability\u201d in each circle</i>. <i>Diego points at the \u201cscale\u201d circle and stares intensely at Amina</i></p><p><strong>Amina</strong>: Yes fine, the scale&nbsp;is big.</p><p><strong>Diego</strong>: Bigger than any other problem EAs think about, arguably! Let\u2019s give it 10/10 scale.</p><p><strong>Amina</strong>: Fine, it scores well on scale. And for neglectedness\u2026</p><p><strong>Diego</strong>: Also scores well. Nobody\u2019s ever even heard of the damn thing.</p><p><strong>Amina</strong>: But you said there are these orgs that are working on it right?</p><p><strong>Diego</strong>: They\u2019re tiny. I haven\u2019t run the numbers but I feel kinda confident that there\u2019s 10x less people explicitly working on the metacrisis than AI alignment, for example.</p><p><strong>Amina</strong>: Seems like a complicated claim, but fine, I\u2019ll trust that you\u2019re roughly right.</p><p><strong>Diego</strong>: Let\u2019s give it 9/10.</p><p><strong>Amina</strong>: Fine, scores well on neglectedness. But <i>tractability</i>!</p><p><strong>Diego</strong>: Yea, tractability it scores poorly on. Do you think it scores better or worse than wild animal suffering?</p><p><strong>Amina</strong>: Oh, eh... Not sure actually.</p><p><strong>Diego</strong>: I\u2019m not sure either. But it\u2019s more tractable than you might think at first glance. I don\u2019t know if you\u2019ve noticed, but there\u2019s a big appetite for change in the air, in the west at least. The world might be at a tipping point, and at tipping points, small groups can have <a href=\"https://arxiv.org/abs/2103.10411\">big effects</a>&nbsp;by nudging things in the right direction.</p><p><i>Amina remained silently skeptical.</i></p><p><strong>Diego</strong>: Let\u2019s score it 2/10 on tractability.</p><figure class=\"image image_resized\" style=\"width:79.02%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7939f4a62d16a28f7cb6ef1cc41f902210ebc833faf8341198777586ec626773/s91zxxt2qvfuofk6dugs\" alt=\"\"><figcaption>&nbsp;</figcaption></figure><p><strong>Diego</strong>: I think if an intervention that \u201csolves the metacrisis\u201d, in other words, makes the world marginally less metacrisis-y, is possible, it would be a very effective intervention. Instead of having to repeatedly go against the tide of the system of incentives we currently live in to deal with x-risks one-by-one like whack-a-mole, we would be going straight to the root cause, solving all the x-risks at once. Solve the disease, not the symptoms.</p><p><strong>Amina</strong>: That\u2019s great and all Diego. I see where you\u2019re coming from. But\u2026</p><p><i>Amina grabs Diego by the shoulders and shakes him</i></p><p><strong>Amina</strong>: WE DON\u2019T HAVE TIME TO FIX SOCIETY, DIEGO! Didn\u2019t you read <a href=\"https://ai-2027.com/about\">AI 2027</a>? AGI is coming in the next couple of years! And after AGI comes, everything will change. It\u2019s all well-and-good treating the cause rather than the symptoms, but if someone comes into the hospital with a heart attack, do you put them on a low-fat diet, or do you just give them the fucking defibrillator?</p><p><strong>Diego</strong>: Yea I get that. But I have three responses.</p><p><strong>Amina</strong>: Ooo look at you. Three whole responses.</p><p><strong>Diego</strong>: Yes, three responses. Firstly, I put a chunky credence on AGI taking way longer to come than all you bay area nerds think, so it makes sense for some people to be preparing for that scenario. Secondly, cultural change can happen faster than you think.&nbsp;It\u2019s actually happening very fast right now. Do you understand skibidi toilet?</p><p><strong>Amina</strong>: What the hell is skibidi toilet?</p><p><strong>Diego</strong>: <i>Exactly</i>. And that brings me to my third point. Exponential technology can speed up cultural change. Hell, the internet might have created more culture, memes, and new languages, than the rest of history.</p><p><i>Amina looks skeptically at Diego</i></p><p><strong>Diego</strong>: Ok, maybe I\u2019m exaggerating, but you get my point. And AGI is going to keep making things change faster and faster. We could leverage AI to catalyse the kind of cultural evolution we need.</p><p><strong>Amina</strong>: Ok fine. It doesn\u2019t seem like a total waste of time for some people to be thinking about this.</p><p><strong>Diego</strong>: That\u2019s nice of you to say.</p><p><strong>Amina</strong>: But I\u2019m <i>nowhere near</i>&nbsp;convinced that the metacrisis is the most important problem to work on, or that I should quit my job doing AI safety to work on it.</p><p><strong>Diego</strong>: I\u2019m not trying to convince you to quit your job!</p><p><strong>Amina</strong>: So what are you trying to convince me of?</p><p><strong>Diego</strong>: Maybe I\u2019m trying to convince you that the metacrisis frame should be a part of the EA portfolio. Some people in EA should be working on clarifying the metacrisis and understanding how to mitigate it. EAs and rationalists are the ones who take arguments seriously, right? Even if they\u2019re weird or crazy-sounding? Well, I think EAs should take this argument seriously. I think the argument is strong enough that some promising EAs near the start of their careers could have a big impact making progress on the metacrisis.</p><p><strong>Amina</strong>: Well, I don\u2019t feel convinced of that yet, based on what you\u2019ve said so far.</p><p><strong>Diego</strong>: Yea, I\u2019ve only really scratched the surface with what I\u2019ve said so far. Maybe we\u2019d need a longer conversation and get deeper into the nitty gritty of the metacrisis frame and how it fits into the EA frame.</p><p><strong>Amina</strong>: Fair enough. But\u2026 There's something else that\u2019s bothering me. It feels kinda strange to think about this metacrisis thing as a cause area in itself, since it lives on a different level of abstraction to the rest of the cause areas.</p><p><i>Diego pauses and thinks for a moment.</i></p><p><strong>Diego</strong>: Yea\u2026 Maybe it isn\u2019t that useful to view it as a cause area now that I come to think of it. Or, see it as a cause area in the same way as&nbsp;\u201cglobal priorities research\u201d&nbsp;is a cause area. Like, a meta cause area.</p><p>Hmm.. Maybe what I really want to argue is that EAs should be aware of the metacrisis framing and use it as a part of their epistemic toolkit. Maybe this means they can make better and wiser decisions. And maybe this will lead to some cause areas&nbsp;being de-prioritised and others being created.</p><p><strong>Amina</strong>: And why should we include the metacrisis frame in the EA epistemic toolkit?</p><p><strong>Diego</strong>: Because EA is <a href=\"https://www.lesswrong.com/posts/7cAsBPGh98pGyrhz9/decoupling-vs-contextualising-norms\">decouply</a>, so much so that there\u2019s a chance they\u2019re making a mistake by only prioritising problems that seem promising under a decouply world view. So, for a more robust portfolio, we should diversify <i>how</i>&nbsp;we make sense of the world.</p><p><strong>Amina</strong>: Worldview diversification\u2026 I guess I\u2019m generally pro-that.</p><p><strong>Diego</strong>: Yea! Foxes are better forecasters than hedgehogs. Here, let me draw something else on the whiteboard.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/e1bd7744f56d0a1a43b750fb978af9edf355938165207a550025fd4f0f3efb2b/c8yypyfbc7phyrhmdnrx\" alt=\"\"><figcaption>&nbsp;</figcaption></figure><p><strong>Diego</strong>: When thinking about some problem, like AI safety, where should we draw the boundaries of the system we study? Should it just be around the walls of the OpenAI offices? Should it be AI development more broadly? Should it include the government, or the economy, or the cultural currents underneath? If you include too much, you might just get confused paralized by the complexity of it all. But if your view is too narrow, you might miss the points of highest leverage and ignore the effects you\u2019re having on the rest of the system.</p><p><strong>Amina</strong>: Hmm\u2026 yea, it doesn\u2019t seem like there\u2019s an obvious answer.</p><p><strong>Diego</strong>: Exactly. To decouple or not decouple? We don\u2019t know. So we should support people working on every point on this spectrum. The alignment people, the governance people, and the metacrisis people. Hedge hedge hedge.</p><p><strong>Cleaner</strong>: Neither of you have a clue about anything.</p><p><i>Amina and Diego's heads turn to the open door of Amina\u2019s office, to see the cleaner in the corridor who has been listening to the majority of their conversation.</i></p><p><strong>Cleaner</strong>: you EA people and you metacrisis people think you've worked everything out, but you're both stuck in your own echo chambers, insulated from the real world.</p><p>I'm going to pre-register my prediction that you are both 99% wrong about everything.</p><p><strong>Diego</strong>: But we're wrong in different ways right! So if we integrate our perspectives we'll only be 98% wrong?</p><p><strong>Amina</strong>: That's not how it works-</p><p><strong>Cleaner</strong>: Your wrongness is correlated.</p><p><strong>Diego</strong>: Ok, 98.5% wrong then?</p><p><i>Amina closes the door as politely as possible.</i></p><p><strong>Amina</strong>: Ok. Maybe the metacrisis is a useful frame. Maybe EAs should think more about it.</p><p><i>Amina stares out the window again. The rain has stopped, the clouds have cleared, and she can see the lights of the skyscrapers across the bay.</i></p><p><strong>Amina</strong>: It still just feels so impossible to fix this metacrisis thing.</p><p><strong>Diego</strong>: Maybe it's impossible. But the stakes are too high, we have to try.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f8172e3bf883b443ae752b3a1b6733af54215e27e42999c97cb4f18074f7aa3e/icnzlssoqn3qajbxnnft\" alt=\"\"></p><h2>Wanna read more about the metacrisis (and related ideas)?</h2><ul><li><a href=\"https://news.lifeitself.org/p/from-polycrisis-to-metacrisis-a-short\"><u>From Polycrisis to Metacrisis</u></a></li><li><a href=\"https://www.integralaltruism.com/\"><u>Integral Altruism</u></a></li><li><a href=\"https://www.youtube.com/watch?v=KCSsKV5F4xc&amp;ab_channel=Win-WinwithLivBoeree\"><u>Moloch &amp; AI alignment</u></a></li><li><a href=\"https://thewiderangle.substack.com/p/what-is-metamodernism\"><u>Metamodernism</u></a></li><li><a href=\"https://www.gameb.wiki/index.php?title=Game_B\"><u>Game B</u></a></li></ul><p><i>Thanks to Guillaume Corlouer, Indra S. Gesink, Toby Jolly, Jack Koch, and Gemma Paterson for feedback on drafts.</i></p>",
    "base_score": 78,
    "comment_count": 16,
    "posted_at": "2025-06-17T07:46:20.136000Z",
    "created_at": "2025-08-21T09:00:02.959922Z",
    "author_id": "h5fndD4u9Hf8DR3EW",
    "author_display_name": "EuanMcLean",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "HmqkjPueHoEwmGwfB",
      "ee66CtAMYurQreWBH",
      "jKptoA9Qc8sjkEPPY",
      "psBzwdY8ipfCeExJ7"
    ],
    "tag_names": [
      "Cause prioritization",
      "Cultural evolution",
      "Existential risk",
      "Existential risk fiction"
    ],
    "markdown_content": "*In her office nestled in the corner of constellation, Amina is checking in with her employee Diego about his project \u2013 a new AI alignment technique. It\u2019s 8pm and the setting sun is filling Amina\u2019s office\u00a0with a red glow. Diego is clearly running a sleep deficit, and 60% of Amina's mind is on a paper deadline next week.*\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/a74a5d28feb9e08cc1bd8a68192b7e14d5e9d98361a44e53eb38daa541b4bf9d/xbhrnb3zzyf9koqrghoq)\n\n## 1: Hopelessness\n\n**Diego**: I\u2019m feeling quite hopeless about the whole thing.\n\n**Amina**: About the project? I think it\u2019s going well, no?\n\n**Diego**: No, I mean *the whole thing*. AI safety. It\u2019s so hard for me to imagine the work I\u2019m doing actually moving the p(doom) needle.\n\n**Amina**: This happens to a lot of people in the bay. There are too many doomers around here!\u00a0Personally I think the default outcome is that we won\u2019t get paperclipped, we\u2019re just addressing a tail risk.\n\n**Diego**: That\u2019s not really what I mean. It\u2019s not about how *likely*\u00a0doom is\u2026 I\u2019m saying that I\u2019m hopeless about some alignment technique actually leading to safe AGI. There are just so many forces fighting against us. The races, the e/accs, the OpenAI shareholders\u2026\n\n**Amina**: Oh man, sounds like you\u2019re spending too much time on twitter. Maybe it\u2019ll help for us to explicitly go through the path-to-impact of this work again?\n\n**Diego**: Yea that would be helpful! Remind me what the point of all this is again.\n\n**Amina**: So, first, we show that this alignment method works.\n\n**Diego**: I still have this voice in the back of my head that alignment is impossible, but for the sake of argument let\u2019s assume it works. Go on.\n\n**Amina**: Maybe it\u2019s impossible. But the stakes are too high, we have to try.\n\n**Diego**: One of my housemates has that tattooed on her lower back.\n\n*Amina\u2019s brain short-circuits for a moment, before she shakes her head and brings herself back into the room.*\n\n**Amina**: R-right\u2026 Anyway, if we can show that this alignment method works, Anthropic might use it in their next model. We have enough connections with them that we can make sure they know about it. If that next model happens to be one who can, say, make a million on the stock exchange or run a company, then we will have made it less likely that some agent based on that model is going to, like, kill everyone.\n\n**Diego**: But what about the other models? Would GPT and Gemini and Grok and DeepSeek and HyperTalk and FizzTask and BloobCode use our alignment technique too?\n\n**Amina**: Well\u2026\n\n**Diego**: And what about all the other open-source models that are inevitably going to crop up over the next couple of years?\n\n**Amina**: Eeeh\n\n**Diego**: In a world where our technique is needed to prevent an AI from killing everyone, even if we get our alignment technique into the first, second, or third AGI system, if the fourth doesn\u2019t use our technique, it could kill everyone. Right?\n\n**Amina**: You *know*\u00a0it\u2019s more complicated than that. If there are already a bunch of other more-or-equally powerful aligned agents roaming around, they will stop the first unaligned one from paperclipping everyone.\n\n**Diego**: Sure, maybe if we align the first ten AGIs, they can all prevent the eleventh AGI from paperclipping everyone.\n\nBut can we really align the first handful of AGIs, given the way the world looks now? At first I had the impression that OpenAI cared about safety at least to an extent. But look at them now. They\u2019ve been largely captured by the interests of their investors, and they\u2019re being forced to race other companies and China. They aren\u2019t just a bunch of idealistic nerds who can devote their energy to making AGI safe anymore.\n\n**Amina**: We always knew race dynamics would become a thing.\n\n**Diego**: And doing research on alignment doesn\u2019t stop those dynamics.\n\n**Amina**: Yes, alignment doesn\u2019t solve anything. That\u2019s why we have our friends in AI governance, right? If they can get legislation passed to prevent AI companies from deploying unaligned AGI, we\u2019ll be in a much better position.\n\n**Diego**: And how do you think that whole thing is going?\n\n**Amina**: Oh cheer up. Just because\u00a0SB 1047 failed doesn\u2019t mean we should just give up and start building bunkers.\n\n**Diego**: But don\u2019t you get the feeling that the political system is just too slow, too stubborn, too fucked to deal with a problem as complex as making sure AI is safe? Don\u2019t you get the feeling that we\u2019re *swimming against the tide*\u00a0in trying to regulate AI?\n\n**Amina**: Sure. Maybe it\u2019s impossible. But the stakes are too high, we have to try.\n\n**Diego**: Oh, I also have another friend I met at burning\u00a0man who has that tattooed on his chest.\n\n**Amina**: You have a strange taste in friends\u2026\n\n**Diego**: Anyway, it all feels pretty impossible to me. And that\u2019s even before we start thinking about China. And even if all AGI models end up aligned, can we really allow that much power to be in everybody\u2019s hands? Are *humans*\u00a0aligned?\n\n**Amina**: That\u2019s going to happen no matter what Diego. Technology will continue to improve, the power of the individual will continue to grow.\n\n*Diego gets up from his seat and stares out the window at the view over to San Francisco.*\n\n**Diego:** You know, when I first heard about the alignment problem I was so hyped. I\u2019d been looking for some way to improve the world. But the world seemed so confusing, it was so hard for me to know what to do. But the argument for AI safety is so powerful, and the alignment problem is such a clean, clear technical problem to solve. It took me back to my undergrad days, when I could forget the complexity of the world and just sit and work through those maths problems. All I needed to know was there on the page, all the variables and all the equations. There wasn\u2019t this anxious feeling of mystery, that there might be something else important to the problem that hasn\u2019t been explicitly stated.\n\n**Amina**: You need to state a problem clearly to have any chance of solving it. Decoupling a problem from its context is what has allowed science and engineering to make such huge progress. You can\u2019t split the atom, go to the moon, or build the internet while you\u2019re still thinking about the societal context of the atoms or rockets or wires.\n\n**Diego**: Yea, I guess this is what originally attracted me to effective altruism. Approaching altruism like a science. Splitting altruism up into a list of problems and working out which are most important. We can\u2019t fix the whole world, but we can work out what\u2019s *most*\u00a0important to fix\u2026\n\nBut\u2026 is altruism really amenable to decoupling? Is it in the same class as building rockets and splitting atoms? I haven\u2019t seen any principled argument about why we should expect this.\n\n**Amina**: But this decouply\u00a0engineering mindset has helped build a number of highly effective charities, like [LEEP](https://leadelimination.org/)!\n\n**Diego**: Ok, so it works for getting lead out of kid\u2019s lungs, but does it work for, like, making singularity go well? It seems unclear to me.\n\nI agree that the arguments for AI risk are strong. Maybe we\u2019re all going to get paperclipped. But can we solve the problem by zooming in on AI development, or do we need to consider the bigger picture, the context?\n\n**Amina**: AI development is\u00a0what might\u00a0kill us right, so it\u2019s what we should be focusing on.\n\n**Diego**: Well, is it the AI that\u2019ll kill us, or the user, or the engineers, or the company, or the shareholders who pressured a faster release? The race dynamics themselves, or [Moloch](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/)? There\u2019s a long causal chain, and a number of places one could intervene to stop that chain.\n\n**Amina**: But aligning the first AGI, with technical work and legislation, is where we can actually *do something*. We can\u2019t fix the whole system. You can\u2019t slay Moloch, Diego.\n\n*Diego raised his eyebrows and locked eyes with Amina*\n\n**Amina**: You can\u2019t slay Moloch, Diego!\n\n**Diego**: I thought you bay area people are meant to be ambitious!\n\n*Amina rolls her eyes*\n\n**Diego**: Perhaps we can come up with some alignment techniques that can align the first AGI. Maybe we can pass some laws that force companies to use these techniques, so that all AGIs are aligned and we don\u2019t get paperclipped. But then technology is going to be moving very fast, and there are a whole bunch of other things that can go wrong.\n\n**Amina**: What do you mean?\n\n**Diego**: There will be other powerful technologies that bring new x-risks. When anyone can engineer a pandemic, will we have to intervene on that too? When anyone can detonate a nuclear bomb, will we be able to prevent everyone from doing it? I suppose we could avoid this with a world government who surveils everyone to prevent them detonating, but I don\u2019t think I want to live in that world either.\n\n**Amina**: We\u2019ll just have to cross those bridges when we come to them.\n\n**Diego**: Will we be able to keep up this whack-a-mole indefinitely? Can EA just keep solving these problems as they come up? If you roll the dice over and over again, eventually you\u2019re going to lose, and we\u2019re all going to die.\n\n**Amina**: Oh, I guess you\u2019re alluding to the [vulnerable world hypothesis](https://forum.effectivealtruism.org/topics/vulnerable-world-hypothesis). Well, whether or not it\u2019s true, this whack-a-mole is all we can do.\n\n**Diego**: Is it though? Just continue treating the symptoms? Or can we try to treat the underlying cause. Wouldn\u2019t that be more effective?\n\n**Amina**: By slaying Moloch? Sure, it would be effective, if it\u2019s possible.\n\n**Diego**: I\u2019m not interested in slaying Moloch.\n\n**Amina**: Huh? Then what are you talking about?\n\n**Diego**: I don\u2019t think we can slay Moloch. But Moloch *isn\u2019t*\u00a0the underlying cause, so we don\u2019t have to.\n\n**Amina**: You\u2019ve lost me now. What is this mysterious deeper cause underlying everything?\n\n**Diego**: I\u2019m glad you asked.\n\n## 2: The metacrisis movement\n\n*Diego strokes his chin for a while and formulates a way to explain his recently updated worldview, gained from his new group of weird, non-EA friends.*\n\n**Diego**: I have a question for you. Are EAs and rationalists the only people who care about x-risk?\n\n**Amina**: Uuh\u2026 maybe? We\u2019re at least the ones who\u2019ve thought most about it.\n\n**Diego**: Unclear! There\u2019s actually a *different*\u00a0cluster of nerds, outside of EA, who care deeply about x-risk. I think of them a little bit like a parallel movement to EA, but they have quite different ways of making sense of the world, built upon a different language.\n\n**Amina**: Do they have a name?\n\n**Diego**: Well, not really. Their community isn\u2019t as centralized as EA. Or maybe it\u2019s actually a number of overlapping communities. I\u2019ve heard them called the [liminal](https://www.joelightfoot.org/post/the-liminal-web-mapping-an-emergent-subculture-of-sensemakers-meta-theorists-systems-poets), [metamodern](https://www.metamodernism.com/2015/01/12/metamodernism-a-brief-introduction/), [game B](https://www.gameb.wiki/index.php?title=Game_B), or [integral](https://integrallife.com/what-is-integral-approach/)\u00a0movement. But I usually call them the *metacrisis*\u00a0movement. There\u2019s a handful of small orgs devoted to spreading the movement and making progress on what they see as the world\u2019s most important problems - [life itself](https://lifeitself.org/), [perspectiva](https://systems-souls-society.com/), [civilisation research institute](https://civilizationresearchinstitute.org/), and others. I think of them as more pre-paradigmatic than EA, there\u2019s less actually doing stuff and more clarifying of problems.\n\n*Amina searches some of the buzzwords Diego threw at her on her computer and scans a couple of pages. She sniggers a little, then sighs in frustration.*\n\n**Amina**: This all feels impenetrable to me. They use a bunch of jargon and a lot of it sounds floaty, vague or mental.\n\n**Diego**: Yea I get that too. It reminds me of how I felt the first time I encountered EA.\n\n**Amina**: Point taken. So I guess you can help me out in understanding what these weirdos are banging on about. How is \u2013 what\u2019s the wording you used \u2013 *how they make sense of the world*\u00a0\u2013 different to how EAs make sense of the world?\n\n**Diego**: I see the main way the metacrisis folk are different is that they\u2019re less [*decouply*](https://www.lesswrong.com/posts/7cAsBPGh98pGyrhz9/decoupling-vs-contextualising-norms)\u00a0than EAs. While EAs like to decouple the problem they\u2019re thinking about from its context, metacrisis people are more prone to emphasise the context the problem lives in \u2013 be that institutional, societal, cultural or whatever. They\u2019re systems thinkers, they pay more attention to interconnectedness and complexity.\n\n**Amina**: I\u2019m feeling a bit tense about this going in some political direction. Societal context\u2026 are they just leftists? Is the metacrisis just \u201cevil capitalism\u201d?\n\n**Diego**: Try to suspend judgement until you\u2019ve engaged with the ideas!\n\n**Amina**: Fair enough. Go on.\n\n**Diego**: The metacrisis crowd take systems change seriously as a way to improve the world, unlike much of EA. EAs are marginal thinkers, metacrisis folk are systems thinkers.\n\n**Amina**: Systems change\u2026 I can get behind some kinds of systems change, like getting laws passed. But if you\u2019re talking about slaying Moloch, that\u2019s more like changing the structure of governments, or the structure of the world. *Deep*\u00a0systems change. That smells intractable to me.\n\n**Diego**: Is it? Are there principled reasons to think that?\n\n**Amina**: I did a fermi estimate some years ago\u2026\n\n**Diego**: And where did the numbers you plugged into that fermi estimate come from?\n\n*Amina looks sheepishly at her shoes*\n\n**Diego**: Did they come out of your ass?\n\n**Amina**: \u2026they came out of my ass. But I have a strong intuition against deep systems change!\n\n**Diego**: That\u2019s fair enough! EA tends to attract people who have the intuition that deep systems change is intractable. I can\u2019t prove that intuition wrong. But the metacrisis people\u00a0don\u2019t\u00a0share that intuition.\n\n**Amina**: Hmm\u2026\n\n**Diego**: Look, I don\u2019t want to talk you out of your EA ways. I obviously still have loads of respect for EA and\u00a0it\u2019s doing a lot right. All I\u2019m asking is for you to entertain a different perspective.\n\n**Amina**: Ok, that\u2019s a reasonable request. Please continue. What is this metacrisis thing?\n\n**Diego**: Ok, I\u2019ll tell you. But to explain, let\u2019s go back to the start of our conversation. If AI doom happens, what will cause it?\n\n*Amina sighs again and rings her hands in frustration.*\n\n**Amina**: It would be caused by an AI company deploying an unaligned AGI.\n\n**Diego**: And what causes that?\n\n**Amina**: Well, lots of things.\n\n**Diego**: Let\u2019s imagine it\u2019s OpenAI, for example. What would cause OpenAI to deploy an unaligned AGI?\n\n**Amina**: Race dynamics, I guess.\n\n**Diego**: And what causes race dynamics?\n\n**Amina**: I guess that\u2019s a collective action problem. The competition to be the first to get to AGI means all the players need to sacrifice all other values, like not contributing to x-risk.\n\n**Diego**: Right, Moloch. That\u2019s part of the story for sure. But what causes Moloch?\n\n**Amina**: Eh\u2026 nothing causes Moloch. It\u2019s just game theory. It\u2019s just human nature\u2026\n\n**Diego**: Is it though? The metacrisis people say that *that*\u00a0is part of the problem - this view that rivalry is just human nature, that zero-sum games are the only possible games to play. Those beliefs help keep Moloch alive.\n\nBut anyway, Moloch isn\u2019t the whole story. Moloch doesn\u2019t explain why all these different parties *want AGI so damn much*.\n\n**Amina**: \u2026because it\u2019s the ultimate technology right? Because it\u2019ll bring whoever controls it almost infinite wealth, and accelerate technology leading to material wealth for all?\n\n**Deigo**: That\u2019s another part of the problem.\n\n**Amina**: Huh?\n\n**Diego**: This deification of technology, this assumption that better technology is always good. And this emphasis on growing material wealth, at the expense of everything else.\n\n**Amina**: I guess the e/accs would say that only technologies that benefit us will be developed, because of the rational market.\n\n**Diego**: There\u2019s problem number three. A belief that the market will do what\u2019s right, because humans are rational agents.\n\n**Amina**: Well, I mean, it\u2019s *approximately*\u00a0true, right\u2013\n\n**Diego**: But not true all the time. Part of the problem is that faith in the market is a background assumption that\u2019s rarely questioned.\n\n**Amina**: Rarely questioned?\n\n**Diego**: Ok, it's questioned by some groups. But does it get questioned where it *matters*, in silicon valley, or in the US government? Not really for the former and not at all for the latter.\n\n**Amina**: Ok. So I\u2019m hearing that there\u2019s just a whole mess of things causing AI risk. Beliefs that human nature is fundamentally rivalrous, beliefs that technology and material wealth is always good, and belief that markets will solve everything. So the problem is everybody\u2019s beliefs?\n\n**Diego**: Kindof-but it\u2019s deeper than beliefs. These are more like background assumptions that colour everyone\u2019s perceptions. At least with explicitly stated beliefs we can examine their validity, but this is something deeper.\n\n**Amina**: Right, so like not explicit beliefs but implicit ways we make sense of the world, or something like that?\n\n**Diego**: Yea. I\u2019d say the problem is the *cultural paradigm* we\u2019re in. The set of implicit assumptions, the language, the shared symbols, society\u2019s operating system.\n\n**Amina**: Does this cultural paradigm have a name? I\u2019ve not done my homework on cultural paradigms\u2026\n\n**Diego**: Yes, the cultural paradigm of today, basically, is *modernity*. We\u2019ve been in modernity ever since the enlightenment. The enlightenment created a new emphasis on reason, individualism, scientific progress, and material wealth.\n\n**Amina**: That all sounds pretty good to me. I guess these metacrisis dweebs\u00a0think science, technology and the economy are the root of all evil, and that we should go back to being hunter-gatherers?\n\n**Diego**: No, they don\u2019t think that. They\u2019re very aware that modernity has given us extraordinary growth, innovation and prosperity. Our current cultural paradigm, powered by science and economic growth, has globally doubled life expectancy, halved child mortality, and cut extreme poverty by an order of magnitude. Nobody can deny that these are all great things.\n\n**Amina**: Ok, I wasn't expecting that. So what\u2019s the problem with modernity? Why does the culture need to change?\n\n**Diego**: The problem is that *we\u2019re racing towards fucking AI doom* Amina!\n\n**Amina**: Oh yea, that thing.\n\n**Diego**: Modernity isn\u2019t perfect. It\u2019s given us loads of nice things, but at invisible costs.\n\nAll that nice prosperity has negative externalities that our culture ignores: climate change, ecological collapse, various mental health crises, and existential risk.\n\nWe need to move to the next cultural paradigm.\n\n**Amina**: Wait, isn\u2019t the next cultural paradigm *postmodernity*? I\u2019m not into that. The postmodernists seem to think that there is no objective reality. They reject any idea of human progress. If the world goes postmodern, we\u2019d all quit are jobs and just get high. That\u2019s not going to solve anything.\n\n**Diego**: Yea, I think postmodernism is an overcorrection to the problems of modernism. We need something that both sees the value in technological and material progress, but isn\u2019t so manically attached to that progress that it\u2019s willing to destroy the world to pursue it.\n\n**Amina**: And what does that look like?\n\n**Diego**: We don\u2019t really know yet. Well, some people have [some ideas](https://thewiderangle.substack.com/p/what-is-metamodernism). I can tell you about them if you want\u2013\n\n**Amina**: I have a call soon. And you still haven\u2019t told me what the metacrisis is!\n\n**Diego**: Oh, sure, sorry I keep going down tangents. I\u2019ve teased you enough, let me tell you what the metacrisis is.\n\n## 3: The root cause of existential risk\n\n*Diego leaps out of his chair and draws a causal diagram on a whiteboard.*\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/258a7be1066c58d99b4f0f788a676e3731bfebdb4acc903f966ce3a78a6aa1a4/i4k5c68tpbzm1xtrcisv)\n\n**Diego**: The metacrisis is the root cause of anthropogenic existential risk. It\u2019s the fact that as technology becomes more powerful, and the world becomes more interconnected, it\u2019s becoming easier and easier for us to destroy ourselves. But our culture, the implicit assumptions, symbols, sense-making tools and values of society, is not mature enough to steward\u00a0this new-found power.\n\n**Amina**: I\u2019ve heard people say things like this before. But it\u2019s very abstract\u2026\n\n**Diego**: Yea, it\u2019s the most zoomed-out, big picture view of x-risk. It\u2019s the reason we live in the hinge of history. We have the power to destroy ourselves, and our society might not be mature enough to steward\u00a0that power. We can solve AI alignment, we can pass laws, we can prevent a nuclear war or the next pandemic. But technology is going to continue progressing exponentially, and if we continue playing the game we\u2019re currently playing, we\u2019ll find new ways to destroy ourselves.\n\n**Amina**: But how do you know that we\u2019ll always find a new way to destroy ourselves?\n\n**Diego**: Because so far, all civilisations destroy themselves eventually. Human history is basically a story of civilisations forming, being stable for a short time, and collapsing. The Babylonians, the Romans, the Mongols, the Ottomans. They all fell. But when they fell it wasn\u2019t the end of humanity, because they were just local collapses. Today, we\u2019ve formed a global, highly interconnected and technologically advanced civilisation. In expectation, this one will fall too. And when this one falls, the whole world falls, and the fall could be the end of humanity.\n\n*Thunder claps and torrential rain starts to pour outside the office*\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/8cf798e8ad8394e37790c76ce93c6f8a3470121ceb86e519f9a0fa30a57fe5f6/rwwejvj6cwkgu1tg1ec5)\n\n**Amina**: Fucking hell. And people call *us*\u00a0doomers. You guys need to cheer up.\n\n**Diego**: We also believe we can fix it though!\n\n**Amina**: Right. How? We can\u2019t stop technological progress.\n\n**Diego**: So you have to help catalyze the next cultural paradigm.\n\n**Amina**: And how the hell do you do that?\n\n**Diego**: Well, it\u2019s not clear how yet. The metacrisis movement is in a pre-paradigmatic stage. They\u2019re still trying to clarify the problem, get really clear on the shape of the metacrisis. Then it could become clear what we need to do.\n\n**Amina**: I sense loads of *if*s there. If a promising, talented person devoted their career to clarifying the metacrisis, that would feel like-\n\n**Diego**: Like a big bet?\n\n**Amina**: Yes.\n\n**Diego**: You don\u2019t like taking bets?\n\n**Amina**: Fine, yes, god damn it, I love taking bets. Who am I kidding?\n\n**Diego**: How about we throw the scale/neglectedness/tractability framework at this?\n\n**Amina**: Yea let\u2019s do that.\n\n*Diego draws three circles on the whiteboard, with \u201cscale\u201d, \u201cneglectedness\u201d and \u201ctractability\u201d in each circle*. *Diego points at the \u201cscale\u201d circle and stares intensely at Amina*\n\n**Amina**: Yes fine, the scale\u00a0is big.\n\n**Diego**: Bigger than any other problem EAs think about, arguably! Let\u2019s give it 10/10 scale.\n\n**Amina**: Fine, it scores well on scale. And for neglectedness\u2026\n\n**Diego**: Also scores well. Nobody\u2019s ever even heard of the damn thing.\n\n**Amina**: But you said there are these orgs that are working on it right?\n\n**Diego**: They\u2019re tiny. I haven\u2019t run the numbers but I feel kinda confident that there\u2019s 10x less people explicitly working on the metacrisis than AI alignment, for example.\n\n**Amina**: Seems like a complicated claim, but fine, I\u2019ll trust that you\u2019re roughly right.\n\n**Diego**: Let\u2019s give it 9/10.\n\n**Amina**: Fine, scores well on neglectedness. But *tractability*!\n\n**Diego**: Yea, tractability it scores poorly on. Do you think it scores better or worse than wild animal suffering?\n\n**Amina**: Oh, eh... Not sure actually.\n\n**Diego**: I\u2019m not sure either. But it\u2019s more tractable than you might think at first glance. I don\u2019t know if you\u2019ve noticed, but there\u2019s a big appetite for change in the air, in the west at least. The world might be at a tipping point, and at tipping points, small groups can have [big effects](https://arxiv.org/abs/2103.10411)\u00a0by nudging things in the right direction.\n\n*Amina remained silently skeptical.*\n\n**Diego**: Let\u2019s score it 2/10 on tractability.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7939f4a62d16a28f7cb6ef1cc41f902210ebc833faf8341198777586ec626773/s91zxxt2qvfuofk6dugs)\n\n**Diego**: I think if an intervention that \u201csolves the metacrisis\u201d, in other words, makes the world marginally less metacrisis-y, is possible, it would be a very effective intervention. Instead of having to repeatedly go against the tide of the system of incentives we currently live in to deal with x-risks one-by-one like whack-a-mole, we would be going straight to the root cause, solving all the x-risks at once. Solve the disease, not the symptoms.\n\n**Amina**: That\u2019s great and all Diego. I see where you\u2019re coming from. But\u2026\n\n*Amina grabs Diego by the shoulders and shakes him*\n\n**Amina**: WE DON\u2019T HAVE TIME TO FIX SOCIETY, DIEGO! Didn\u2019t you read [AI 2027](https://ai-2027.com/about)? AGI is coming in the next couple of years! And after AGI comes, everything will change. It\u2019s all well-and-good treating the cause rather than the symptoms, but if someone comes into the hospital with a heart attack, do you put them on a low-fat diet, or do you just give them the fucking defibrillator?\n\n**Diego**: Yea I get that. But I have three responses.\n\n**Amina**: Ooo look at you. Three whole responses.\n\n**Diego**: Yes, three responses. Firstly, I put a chunky credence on AGI taking way longer to come than all you bay area nerds think, so it makes sense for some people to be preparing for that scenario. Secondly, cultural change can happen faster than you think.\u00a0It\u2019s actually happening very fast right now. Do you understand skibidi toilet?\n\n**Amina**: What the hell is skibidi toilet?\n\n**Diego**: *Exactly*. And that brings me to my third point. Exponential technology can speed up cultural change. Hell, the internet might have created more culture, memes, and new languages, than the rest of history.\n\n*Amina looks skeptically at Diego*\n\n**Diego**: Ok, maybe I\u2019m exaggerating, but you get my point. And AGI is going to keep making things change faster and faster. We could leverage AI to catalyse the kind of cultural evolution we need.\n\n**Amina**: Ok fine. It doesn\u2019t seem like a total waste of time for some people to be thinking about this.\n\n**Diego**: That\u2019s nice of you to say.\n\n**Amina**: But I\u2019m *nowhere near*\u00a0convinced that the metacrisis is the most important problem to work on, or that I should quit my job doing AI safety to work on it.\n\n**Diego**: I\u2019m not trying to convince you to quit your job!\n\n**Amina**: So what are you trying to convince me of?\n\n**Diego**: Maybe I\u2019m trying to convince you that the metacrisis frame should be a part of the EA portfolio. Some people in EA should be working on clarifying the metacrisis and understanding how to mitigate it. EAs and rationalists are the ones who take arguments seriously, right? Even if they\u2019re weird or crazy-sounding? Well, I think EAs should take this argument seriously. I think the argument is strong enough that some promising EAs near the start of their careers could have a big impact making progress on the metacrisis.\n\n**Amina**: Well, I don\u2019t feel convinced of that yet, based on what you\u2019ve said so far.\n\n**Diego**: Yea, I\u2019ve only really scratched the surface with what I\u2019ve said so far. Maybe we\u2019d need a longer conversation and get deeper into the nitty gritty of the metacrisis frame and how it fits into the EA frame.\n\n**Amina**: Fair enough. But\u2026 There's something else that\u2019s bothering me. It feels kinda strange to think about this metacrisis thing as a cause area in itself, since it lives on a different level of abstraction to the rest of the cause areas.\n\n*Diego pauses and thinks for a moment.*\n\n**Diego**: Yea\u2026 Maybe it isn\u2019t that useful to view it as a cause area now that I come to think of it. Or, see it as a cause area in the same way as\u00a0\u201cglobal priorities research\u201d\u00a0is a cause area. Like, a meta cause area.\n\nHmm.. Maybe what I really want to argue is that EAs should be aware of the metacrisis framing and use it as a part of their epistemic toolkit. Maybe this means they can make better and wiser decisions. And maybe this will lead to some cause areas\u00a0being de-prioritised and others being created.\n\n**Amina**: And why should we include the metacrisis frame in the EA epistemic toolkit?\n\n**Diego**: Because EA is [decouply](https://www.lesswrong.com/posts/7cAsBPGh98pGyrhz9/decoupling-vs-contextualising-norms), so much so that there\u2019s a chance they\u2019re making a mistake by only prioritising problems that seem promising under a decouply world view. So, for a more robust portfolio, we should diversify *how*\u00a0we make sense of the world.\n\n**Amina**: Worldview diversification\u2026 I guess I\u2019m generally pro-that.\n\n**Diego**: Yea! Foxes are better forecasters than hedgehogs. Here, let me draw something else on the whiteboard.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/e1bd7744f56d0a1a43b750fb978af9edf355938165207a550025fd4f0f3efb2b/c8yypyfbc7phyrhmdnrx)\n\n**Diego**: When thinking about some problem, like AI safety, where should we draw the boundaries of the system we study? Should it just be around the walls of the OpenAI offices? Should it be AI development more broadly? Should it include the government, or the economy, or the cultural currents underneath? If you include too much, you might just get confused paralized by the complexity of it all. But if your view is too narrow, you might miss the points of highest leverage and ignore the effects you\u2019re having on the rest of the system.\n\n**Amina**: Hmm\u2026 yea, it doesn\u2019t seem like there\u2019s an obvious answer.\n\n**Diego**: Exactly. To decouple or not decouple? We don\u2019t know. So we should support people working on every point on this spectrum. The alignment people, the governance people, and the metacrisis people. Hedge hedge hedge.\n\n**Cleaner**: Neither of you have a clue about anything.\n\n*Amina and Diego's heads turn to the open door of Amina\u2019s office, to see the cleaner in the corridor who has been listening to the majority of their conversation.*\n\n**Cleaner**: you EA people and you metacrisis people think you've worked everything out, but you're both stuck in your own echo chambers, insulated from the real world.\n\nI'm going to pre-register my prediction that you are both 99% wrong about everything.\n\n**Diego**: But we're wrong in different ways right! So if we integrate our perspectives we'll only be 98% wrong?\n\n**Amina**: That's not how it works-\n\n**Cleaner**: Your wrongness is correlated.\n\n**Diego**: Ok, 98.5% wrong then?\n\n*Amina closes the door as politely as possible.*\n\n**Amina**: Ok. Maybe the metacrisis is a useful frame. Maybe EAs should think more about it.\n\n*Amina stares out the window again. The rain has stopped, the clouds have cleared, and she can see the lights of the skyscrapers across the bay.*\n\n**Amina**: It still just feels so impossible to fix this metacrisis thing.\n\n**Diego**: Maybe it's impossible. But the stakes are too high, we have to try.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/f8172e3bf883b443ae752b3a1b6733af54215e27e42999c97cb4f18074f7aa3e/icnzlssoqn3qajbxnnft)\n\n## Wanna read more about the metacrisis (and related ideas)?\n\n- [From Polycrisis to Metacrisis](https://news.lifeitself.org/p/from-polycrisis-to-metacrisis-a-short)\n- [Integral Altruism](https://www.integralaltruism.com/)\n- [Moloch & AI alignment](https://www.youtube.com/watch?v=KCSsKV5F4xc&ab_channel=Win-WinwithLivBoeree)\n- [Metamodernism](https://thewiderangle.substack.com/p/what-is-metamodernism)\n- [Game B](https://www.gameb.wiki/index.php?title=Game_B)\n\n*Thanks to Guillaume Corlouer, Indra S. Gesink, Toby Jolly, Jack Koch, and Gemma Paterson for feedback on drafts.*",
    "word_count": 5027,
    "reading_time_minutes": 25,
    "external_links": [
      "https://ai-2027.com/about",
      "https://arxiv.org/abs/2103.10411",
      "https://civilizationresearchinstitute.org/",
      "https://forum.effectivealtruism.org/topics/vulnerable-world-hypothesis",
      "https://integrallife.com/what-is-integral-approach/",
      "https://leadelimination.org/",
      "https://lifeitself.org/",
      "https://news.lifeitself.org/p/from-polycrisis-to-metacrisis-a-short",
      "https://slatestarcodex.com/2014/07/30/meditations-on-moloch/",
      "https://systems-souls-society.com/",
      "https://thewiderangle.substack.com/p/what-is-metamodernism",
      "https://www.gameb.wiki/index.php?title=Game_B",
      "https://www.integralaltruism.com/",
      "https://www.joelightfoot.org/post/the-liminal-web-mapping-an-emergent-subculture-of-sensemakers-meta-theorists-systems-poets",
      "https://www.lesswrong.com/posts/7cAsBPGh98pGyrhz9/decoupling-vs-contextualising-norms",
      "https://www.metamodernism.com/2015/01/12/metamodernism-a-brief-introduction/",
      "https://www.youtube.com/watch?v=KCSsKV5F4xc&ab_channel=Win-WinwithLivBoeree"
    ],
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T09:00:05.710260Z"
  },
  {
    "id": 1394,
    "post_id": "YFLeQXFaNuPsZYrwt",
    "title": "Financial Planning for Financial Independence and Charitable Giving",
    "title_normalized": "financial planning for financial independence and charitable giving",
    "page_url": "https://forum.effectivealtruism.org/posts/YFLeQXFaNuPsZYrwt/financial-planning-for-financial-independence-and-charitable",
    "html_body": "<p>Hello!<br>&nbsp;</p><p>I've looked into old posts on financial planning for charitable giving and, as I found most have some years and they're not completely adapted to my situation, I decided to ask away.</p><p>A comment on this <a href=\"https://forum.effectivealtruism.org/posts/psvQMXEgQsT5RMDTu/consider-financial-independence-first\">post</a> convinced me that I should not wait for complete financial independence before doing, at least, a 10% pledge (which I understood is the community standard yet I'm not sure if for somewhat arbitrary reasons or any specific reason). The reasons for not waiting were the urgency of the causes, the compatibility of achieving financial independence and EA goals and the possibility of value drifting due to non active involvement.<br><br>Yet I do need to pay debts before giving so I'm making this my first goal. I've been feeling I don't have the financial education necessary to effectively plan and execute a plan to achieve this first goal and the next ones. What kind of advice would you give me and what (free) resources would you recommend?&nbsp;<br><br>(I'm from Portugal, I imagine there will not be much specific information about my specific context yet whatever comes nearer would be better)</p>",
    "base_score": 9,
    "comment_count": 1,
    "posted_at": "2025-02-25T14:20:27.040000Z",
    "created_at": "2025-08-21T21:22:50.877458Z",
    "author_id": "gZZHXj8tgFTAArE9m",
    "author_display_name": "Ant\u00f3nio Souza",
    "coauthor_ids": null,
    "coauthor_names": null,
    "tag_ids": [
      "L6NqHZkLc4xZ7YtDr",
      "ZCihBFp5P64JCvQY6",
      "rrsDstdgzQnZ69Psy"
    ],
    "tag_names": [
      "Community",
      "Effective giving",
      "Personal finance"
    ],
    "markdown_content": "Hello!\n\nI've looked into old posts on financial planning for charitable giving and, as I found most have some years and they're not completely adapted to my situation, I decided to ask away.\n\nA comment on this [post](https://forum.effectivealtruism.org/posts/psvQMXEgQsT5RMDTu/consider-financial-independence-first) convinced me that I should not wait for complete financial independence before doing, at least, a 10% pledge (which I understood is the community standard yet I'm not sure if for somewhat arbitrary reasons or any specific reason). The reasons for not waiting were the urgency of the causes, the compatibility of achieving financial independence and EA goals and the possibility of value drifting due to non active involvement.  \n  \nYet I do need to pay debts before giving so I'm making this my first goal. I've been feeling I don't have the financial education necessary to effectively plan and execute a plan to achieve this first goal and the next ones. What kind of advice would you give me and what (free) resources would you recommend?\u00a0  \n  \n(I'm from Portugal, I imagine there will not be much specific information about my specific context yet whatever comes nearer would be better)",
    "word_count": 187,
    "reading_time_minutes": 1,
    "external_links": [
      "https://forum.effectivealtruism.org/posts/psvQMXEgQsT5RMDTu/consider-financial-independence-first"
    ],
    "short_summary": null,
    "long_summary": null,
    "source_type": "EA Forum",
    "processing_version": "1.0",
    "processing_errors": null,
    "scraped_at": "2025-08-21T21:22:51.283081Z"
  }
]