{
  "PostValue": {
    "post_id": "nNu5cHxDDqjSqfkHz",
    "value_ea": 5,
    "value_humanity": 2,
    "explanation": "This is a practical, moderately valuable post for the EA/rationalist community: it publicises a funded opportunity to produce technical estimates of AI x-risk (including review of instrumental convergence) and could seed useful work, hires, and evidence that inform cause prioritisation. It is not itself a foundational argument about x-risk, so its truth/falsity has limited theoretical weight \u2014 important mainly for resourcing and community-building. For general humanity the post is of low importance: it\u2019s a narrow recruitment/funding announcement with only indirect, speculative downstream impact."
  },
  "PostRobustness": {
    "post_id": "nNu5cHxDDqjSqfkHz",
    "robustness_score": 3,
    "actionable_feedback": "1) Add key application facts in the post (don\u2019t just link the PDF). Readers will skip the attachment unless you give a concise summary: exact deadline(s), whether pay is hourly or a stipend, currency and gross/net/tax treatment, expected hours or FTE, eligibility (nationality/visa requirements, student vs postdoc vs contractor), deliverables, duration, whether work is remote or on-site, and the selection criteria. Action: paste a short bullet list of these items so readers can immediately judge fit.\n\n2) Be explicit about your role, expectations for collaborators, and conflicts of interest. Right now it\u2019s unclear whether you\u2019re a named applicant/lead, whether you want co-applicants on the grant, what responsibilities you expect from joining people, and how IP/authorship/credit will be handled. Action: say whether you\u2019re already on the proposal, what work you want a collaborator to do, how time/compensation will be split, and whether you\u2019ll disclose your applicant status in replies.\n\n3) Tone and framing: avoid jargon/loaded phrasing (e.g. \u201cp(doom)\u201d, \u201ccause-prioritise Al\u201d) and casual marketing language like emojis or \u201chandsomely paid\u201d without precision. That risks alienating sceptical readers and makes the call look less professional. Action: reword to a short, neutral call-to-action, include a clear contact method (preferably via forum DM or email) and a one\u2011line on preferred qualifications so replies are higher quality.",
    "improvement_potential": "The feedback identifies clear, actionable omissions that are likely to annoy readers or block responses (no concise summary of the PDF, unclear applicant role/expectations/conflicts, and casual/unprofessional tone). These are both 'own-goals' the poster would be embarrassed about and relatively easy to fix with a short bullet list and clearer wording, so implementing them would substantially improve the post. It isn't perfect (could give a suggested one-line rewrite or prioritize which facts are most important), but it's high-value and practical."
  },
  "PostAuthorAura": {
    "post_id": "nNu5cHxDDqjSqfkHz",
    "author_fame_ea": 1,
    "author_fame_humanity": 1,
    "explanation": "I don\u2019t recognize 'miller-max' as a known EA/rationalist figure or public intellectual. The name looks like a pseudonymous handle; there are no widely cited publications, talks, or community leadership roles I can attribute to them. If you can share links or context (Forum posts, GitHub, articles), I can reassess."
  },
  "PostClarity": {
    "post_id": "nNu5cHxDDqjSqfkHz",
    "clarity_score": 8,
    "explanation": "Overall clear and easy to understand: the post states the opportunity, links to full details, gives timeline and an attractive pay range, and has a concrete call-to-action. Strengths: concise, action-oriented, and provides primary links. Weaknesses: informal phrasing and minor typos (e.g. \"Al\" vs \"AI\"), ambiguous pay notation (currency, full-time/part-time), no explicit deadline or eligibility/skills listed, and a couple of jargon-y terms (\"cause-prioritise\", \"p(doom)\") that could confuse readers unfamiliar with the shorthand."
  },
  "PostNovelty": {
    "post_id": "nNu5cHxDDqjSqfkHz",
    "novelty_ea": 2,
    "novelty_humanity": 1,
    "explanation": "This is mainly an administrative/job announcement rather than a novel argument or idea. For EA Forum readers, paid calls for AI x-risk research and discussions of estimating p(doom) or instrumental convergence are already familiar, so it has low novelty (slightly higher only because of the specific tender details). For the general public, the post's content is very ordinary \u2014 a research tender/job posting about AI risk \u2014 and not conceptually new."
  },
  "PostInferentialSupport": {
    "post_id": "nNu5cHxDDqjSqfkHz",
    "reasoning_quality": 4,
    "evidence_quality": 6,
    "overall_support": 5,
    "explanation": "This post is primarily an announcement/invitation rather than an argument-driven piece. Strengths: it links to primary sources (the tender and project page), gives concrete logistics (dates, pay range), and signals the poster's relevant background and willingness to collaborate. Weaknesses: it does not present a structured argument or methodology for how the work will estimate p(doom), nor engage with counterarguments or evidence about key technical claims (e.g., instrumental convergence). The factual claims about the tender and pay are verifiable via the provided links (hence moderate evidence quality), but the post offers little empirical or logical support for broader claims about the tractability or likelihood of AI x-risk. Overall, the post is a useful recruitment/awareness item but not a well-supported technical or empirical case about AI existential risk."
  },
  "PostExternalValidation": {
    "post_id": "nNu5cHxDDqjSqfkHz",
    "emperical_claim_validation_score": 7,
    "validation_notes": "Most key factual claims in the post are supported: ITAS/KIT has an active project on \u201csystemic and existential risks of AI\u201d and ITAS posted tenders for expert assessments (including one on technical conditions for existential risks). KIT\u2019s pages confirm the project runs in 2024\u20132025 and that tenders were posted (deadline 31 Mar 2025). The author\u2019s supplied PDF link currently returns an error on KIT\u2019s server (so I could not verify line-by-line details in the tender). The poster\u2019s specific project-period dates (~May 20\u2013Oct 20, 2025) are plausible but not verifiable from the public KIT pages I found; the claim about pay \u201c25\u201360/h+\u201d is not documented on the KIT pages I could access and thus remains unverified.",
    "sources": [
      "KIT / ITAS project page: 'Uncontrollierbare k\u00fcnstliche Intelligenz: Ein existenzielles Risiko?' (ITAS project page showing the subproject and 2024\u20132025 timeline). ([itas.kit.edu](https://www.itas.kit.edu/projekte_heil24_seri2.php?utm_source=chatgpt.com))",
      "KIT / ITAS news: 'AI risks: call for expert assessments' (announces tenders including 'Existential risks of AI: technical conditions' and states proposal deadline 31 March 2025). ([itas.kit.edu](https://www.itas.kit.edu/english/2025_004.php?utm_source=chatgpt.com))",
      "Attempt to open the tender PDF at the URL given in the post (https://www.itas.kit.edu/downloads/projekt/projekt_heil24_seri2_tc2.pdf) returned an error page on KIT's site (page not found). ([itas.kit.edu](https://www.itas.kit.edu/downloads/projekt/projekt_heil24_seri2_tc2.pdf))",
      "KIT newsroom / project overview: 'Projekt zu systemischen und existenziellen KI-Risiken' (KIT press article describing the two-part project and funding). ([kit.edu](https://www.kit.edu/kit/202404-systemische-und-existenzielle-risiken-von-ki.php?utm_source=chatgpt.com))",
      "EA Forum post being evaluated: 'Open call: \"Existential risk of AI: technical conditions\"' by miller-max (original announcement referencing the ITAS tender). ([forum.effectivealtruism.org](https://forum.effectivealtruism.org/posts/nNu5cHxDDqjSqfkHz/open-call-existential-risk-of-ai-technical-conditions?utm_source=chatgpt.com))"
    ]
  }
}