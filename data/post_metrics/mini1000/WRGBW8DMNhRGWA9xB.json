{
  "PostValue": {
    "post_id": "WRGBW8DMNhRGWA9xB",
    "value_ea": 5,
    "value_humanity": 2,
    "explanation": "Useful but incremental for the EA/AI-safety community. Longview\u2019s new HNW-focused advising and Frontier AI Fund can help channel donations into underfunded, high-leverage AI safety projects and broaden donor diversity\u2014so it could meaningfully improve funding allocation and nudge some micro-to-medium funding flows. However it\u2019s not foundational: it doesn\u2019t introduce new theory, evidence, or dramatically change the overall funding landscape on its own. For general humanity, the announcement is low-impact \u2014 it\u2019s a niche fundraising/service update whose effects are indirect and likely small unless it scales to mobilize very large sums."
  },
  "PostRobustness": {
    "post_id": "WRGBW8DMNhRGWA9xB",
    "robustness_score": 2,
    "actionable_feedback": "1) Be explicit about conflicts of interest, fees, and fund terms. The post currently gives no details on whether Longview (or staff) have financial stakes in recommended grantees or the Frontier AI Fund, what fees/commissions or carry apply, and what governance/reporting donors receive. Actionable: add a short clear statement (1\u20132 sentences) about whether recommendations are independent, whether Longview takes fees or carry from the Frontier AI Fund or grantees, and link to or summarize the Frontier Fund\u2019s LP terms, minimums, and reporting cadence. This removes an obvious \u201cown goal\u201d for skeptical HNW donors. \n\n2) Substantiate the claim that marginal donations are \u201cseveral times more valuable.\u201d That is a strong, mission-critical claim but the post gives no evidence or examples. Actionable: include one or two brief concrete examples or metrics (e.g., past grants where marginal donors unlocked work, projects that were underfunded but high-impact, or a short explanation of the criteria used to judge marginality). If you don\u2019t have space for examples, replace \u201cseveral times more valuable\u201d with a softer, evidence-free phrasing and invite readers to ask for case studies on a call.\n\n3) Clarify selection, matching, and eligibility process. Readers will want to know how you choose and prioritise grants, how you match donors to opportunities, timelines, and whether the $100k threshold is flexible. Actionable: add 3\u20134 bullets that state (a) core selection criteria for recommended grants, (b) what due diligence you perform, (c) what a donor can expect in the first call (timeline, deliverables), and (d) whether donors < $100k can still engage (or are referred elsewhere). This reduces friction and prevents repeated clarifying questions.",
    "improvement_potential": "The feedback pinpoints three high-impact omissions that could undermine donor trust or generate avoidable follow-up questions: (1) lack of explicit COI/fee/terms disclosure is a significant own-goal for soliciting wealthy donors, (2) the \u2018several times more valuable\u2019 claim is a strong, mission-critical assertion that needs evidence or softer wording, and (3) unclear selection/matching processes cause friction and repeated queries. Each suggestion is actionable and can be implemented with minimal added length (short sentences/bullets), so addressing them would substantially improve credibility and reduce donor hesitation."
  },
  "PostAuthorAura": {
    "post_id": "WRGBW8DMNhRGWA9xB",
    "author_fame_ea": 3,
    "author_fame_humanity": 1,
    "explanation": "Longview Philanthropy appears to be a minor/occasional contributor or pseudonymous entity rather than a well-known EA leader. It has limited visibility and citation within EA/rationalist spaces and no notable public presence outside those circles."
  },
  "PostClarity": {
    "post_id": "WRGBW8DMNhRGWA9xB",
    "clarity_score": 8,
    "explanation": "The post is well structured and easy to follow: clear headings, a short executive summary, explicit target audience, concrete services, and multiple clear calls-to-action. It concisely communicates who is eligible and what Longview offers. Minor weaknesses: a bit repetitive (contact info repeated), some small ambiguities about selection/eligibility details and fund mechanics, and one claim about marginal donation value is asserted without support \u2014 none of which seriously impede understanding."
  },
  "PostNovelty": {
    "post_id": "WRGBW8DMNhRGWA9xB",
    "novelty_ea": 2,
    "novelty_humanity": 3,
    "explanation": "This is primarily an operational/marketing announcement about Longview offering a new tier of donor advising and a private Frontier AI Fund. For EA Forum readers, the idea (philanthropic advising for AI safety, matching donors to niche grants, donor-targeted funds) is familiar and incremental, so it\u2019s not conceptually novel. For the general public it\u2019s slightly more novel because HNW donor-advising for AI risk is a niche practice, but the underlying idea of advisory services and targeted philanthropic funds is commonplace."
  },
  "PostInferentialSupport": {
    "post_id": "WRGBW8DMNhRGWA9xB",
    "reasoning_quality": 5,
    "evidence_quality": 3,
    "overall_support": 4,
    "explanation": "The post is clear and well-structured as an announcement and its basic claims are plausible (Longview offers a new service for HNW donors, team and fund exist). However, the argumentative claims that marginal donations are \u201cseveral times more valuable\u201d and that this service fills critical funding gaps are asserted without supporting analysis, examples, or metrics. Evidence offered is mostly organizational (track record since 2018, staff bios, a Frontier AI Fund size, and a few anecdotal events) rather than empirical outcomes, impact evaluations, or case studies demonstrating donor matching effectiveness or improved grant impact. Overall the announcement is credible as a service offering but weakly supported for its stronger causal/value claims."
  },
  "PostExternalValidation": {
    "post_id": "WRGBW8DMNhRGWA9xB",
    "emperical_claim_validation_score": 8,
    "validation_notes": "Most major empirical claims in the post are supported by Longview\u2019s own public materials and an official EA Forum announcement. Longview\u2019s website confirms the Emerging Challenges Fund, the named staff (Simran Dhaliwal, Aidan O\u2019Gara, Zach Freitas\u2011Groff), and that Longview directs a private \u201cFrontier AI Fund\u201d launched around Dec 2024; Longview\u2019s careers pages describe that fund as having raised roughly $9M since ~Dec 2024 (the EA Forum post states $8.6M). Longview\u2019s site also documents bespoke advising for large donors (the organisation commonly references >$1M donors), and that it provides grant recommendations to donors making large gifts. Minor issues: the exact donor threshold \u201c> $100k/year\u201d appears in the EA Forum announcement but Longview\u2019s public site more generally describes services for \u201clarge\u201d or >$1M donors and \u201cdonors wishing to make large gifts\u201d; the two cited dollar figures for the Frontier AI Fund ($8.6M vs ~ $9M) are a small timing/rounding discrepancy; specific claims about two dinners in Dec 2024 and Feb 2025 were stated in the EA Forum post but I could not find independent public pages documenting those particular events. Overall, the post is well\u2011supported by primary sources from Longview, with only small, explainable discrepancies and one minor unverified event-level detail.",
    "sources": [
      "Longview Philanthropy \u2014 EA Forum post (Longview is now offering AI grant recommendations to donors giving >$100k / year), Apr 11, 2025. https://forum.effectivealtruism.org/posts/WRGBW8DMNhRGWA9xB/longview-is-now-offering-ai-grant-recommendations-to-donors-1",
      "Longview Philanthropy \u2014 Emerging Challenges Fund (fund page, describes public fund and donor recommendations). https://www.longview.org/fund/emerging-challenges-fund/",
      "Longview Philanthropy \u2014 About / Team (Simran Dhaliwal CEO). https://www.longview.org/about/simran-dhaliwal/",
      "Longview Philanthropy \u2014 Aidan O\u2019Gara (AI Programme Officer bio). https://www.longview.org/about/aidan-ogara/",
      "Longview Philanthropy \u2014 Dr Zach Freitas\u2011Groff (AI Programme Officer bio). https://www.longview.org/about/zach-freitas-groff/",
      "Longview Philanthropy \u2014 Careers / AI Grantmaker & AI Programme Director pages (states private Frontier AI Fund has raised ~ $9M since ~Dec 2024). https://www.longview.org/careers/ai-grantmaker-generalist-or-us-ai-policy/ and https://www.longview.org/careers/ai-programme-director/",
      "Longview Philanthropy \u2014 Home / Grantmaking (notes bespoke advising and services for donors giving over $1M). https://www.longview.org/",
      "InfluenceWatch \u2014 Longview Philanthropy (background; founding year 2018). https://www.influencewatch.org/non-profit/longview-philanthropy/",
      "Open Philanthropy \u2014 Longview Philanthropy \u2014 General Support (Oct 2024 grant page; corroborates Longview's role and funder relationships). https://www.openphilanthropy.org/grants/longview-philanthropy-general-support-october-2024/"
    ]
  }
}