{
  "PostValue": {
    "post_id": "85yGkxJYfKnfCd9ME",
    "value_ea": 7,
    "value_humanity": 6,
    "explanation": "This is an important, actionable critique for the EA/AI\u2011governance ecosystem: it identifies a concrete institutional failure (insular grantmaking, lack of mainstream philanthropic and political expertise, and opaque/no formal rubrics) that plausibly biases funding toward academic research over advocacy. For the EA community this matters a lot because it affects how hundreds of millions of dollars are allocated and could materially change the mix of interventions (and therefore marginal impact) in a high\u2011stakes domain. For humanity the post is moderately important: changes in grantmaking could indirectly improve AI governance and lower existential risk, which would be very consequential, but the argument is about process improvement rather than a novel foundational insight and its real\u2011world effect is uncertain and contingent on follow\u2011through."
  },
  "PostRobustness": {
    "post_id": "85yGkxJYfKnfCd9ME",
    "robustness_score": 3,
    "actionable_feedback": "1) Overgeneralization from an informal census \u2014 the claim that AI safety grantmakers have \"essentially zero mainstream grantmaking experience\" rests on a quick LinkedIn/bio scan and reads as a sweeping empirical claim. That risks being wrong or easily dismissed. Actionable fix: either (a) make the methodology explicit (who was checked, what counts as \u2018mainstream grantmaking experience\u2019, dates, inclusion/exclusion rules), or (b) soften the claim to describe a plausible skill-gap pattern you observed and provide anonymized examples or counts with confidence intervals. If you can, add a table or appendix (anonymized) showing your coding so reviewers can judge it.  \n\n2) Underappreciation of why funders avoid rigid rubrics and limited engagement \u2014 you undercut your case by treating formal rubrics as an obvious universal win while not seriously engaging with major, plausible counterarguments (donor confidentiality and heterogenous donor preferences, difficulty/uncertainty of counterfactuals and long time-horizons, limited evaluator bandwidth, political risk, and legitimate reasons for holistic review). Actionable fix: add a section that (a) catalogs the main legitimate reasons funders give for informal/holistic evaluation, (b) explains why those reasons don\u2019t fully justify current practice, and (c) gives concrete mitigations (e.g., templates for confidential donor-side weightings, rubric designs that incorporate huge uncertainty/error bars, staged/portfolio funding approaches, or hybrid processes that combine scores + allowed exceptions). This will make your proposal more credible to grantmakers.  \n\n3) Proposals lack concrete, low-friction next steps and ignore tradeoffs of hiring mainstream staff \u2014 \u2018\u2018hire X people\u2019\u2019 is fine as a headline, but the post doesn\u2019t address cultural fit risks (mainstream grantmakers may not share longtermist priors), how to recruit them given mission differences, onboarding, or a pilot plan to test whether this actually changes outcomes. Actionable fix: provide a practical implementation plan: a short job-spec template emphasizing desired transferable skills (e.g., experience running advocacy grant portfolios, designing rubrics, measuring policy outcomes), a 6\u201312 month pilot process (hire 1\u20132 contractors to design rubrics and score a subset of grants, compare decisions), and a set of measurable success criteria for the pilot (e.g., improved applicant clarity, reduced time-to-decision, percent of applicants given actionable feedback). Also consider recommending publishing a sample rubric or two (even a coarse one) in the post so readers can see exactly what you mean.",
    "improvement_potential": "The proposed feedback targets three major, practical weaknesses: an overbroad empirical claim based on a quick LinkedIn scan, failure to engage key counterarguments (why funders legitimately avoid rigid rubrics), and lack of actionable, low-friction implementation details and trade-off analysis for the hiring/rubric proposals. Fixing these would materially increase credibility and persuasiveness without requiring radical restructuring of the piece\u2014add a clear methods note for the census, a succinct rebuttal-and-mitigation section for common funder objections, and a short pilot/hiring template (job spec, pilot timeline, and success metrics). These changes address core vulnerabilities that readers (especially grantmakers) will notice and could otherwise dismiss the argument."
  },
  "PostAuthorAura": {
    "post_id": "85yGkxJYfKnfCd9ME",
    "author_fame_ea": 1,
    "author_fame_humanity": 1,
    "explanation": "I have no record up to mid\u20112024 of a notable EA/rationalist figure named Jason Green\u2011Lowe. No major publications, affiliations, conference appearances, or widely cited posts in EA/rationalist venues are associated with that name; it may be a pseudonym or an obscure/new author. Therefore they appear unknown in EA circles and have no measurable global public profile."
  },
  "PostClarity": {
    "post_id": "85yGkxJYfKnfCd9ME",
    "clarity_score": 8,
    "explanation": "The post is well-structured and easy to follow: it has a clear thesis, logical sections (problem, evidence, best-practices, recommendations), concrete examples and persuasive anecdotes. Strengths include explicit headings, illustrative quotes/emails, and actionable recommendations. Weaknesses are length and some repetition (the same points are reiterated many times), occasional informal detours and assumed insider jargon (EA bubble, CAIP) that could slow a first-time reader; an executive summary or tighter editing would improve conciseness and punch."
  },
  "PostNovelty": {
    "post_id": "85yGkxJYfKnfCd9ME",
    "novelty_ea": 4,
    "novelty_humanity": 6,
    "explanation": "For EA readers the core claims (we fund research over advocacy, EA insularity causes bias, rubrics are useful) are familiar and have been discussed before; the post\u2019s novelty is mainly in the concrete evidence and anecdotes (the staff census, quoted funder emails, and the detailed critique of specific funders' processes). For a general educated reader this is more novel because it reveals inside-baseball details about how major AI safety donors actually make decisions and argues for importing mainstream philanthropic practices \u2014 a specific, operational critique most outside the movement are unlikely to have seen."
  },
  "PostInferentialSupport": {
    "post_id": "85yGkxJYfKnfCd9ME",
    "reasoning_quality": 6,
    "evidence_quality": 4,
    "overall_support": 5,
    "explanation": "Strengths: The post is logically organized, identifies a plausible mechanism (insular hiring -> informal, subjective grant decisions -> bias toward familiar work), and draws sensible recommendations (hire political/philanthropic staff; publish rubrics). The author provides concrete illustrative evidence (census of staff bios, multiple rejection emails, funder quotes, and comparisons to mainstream foundation practices) and acknowledges important caveats (difficulty of measuring impact, Goodhart\u2019s Law). Weaknesses: The argument over-generalizes from a small, partly anecdotal sample (one applicant's set of rejection emails and a LinkedIn census whose methods are not fully transparent), and relies heavily on anecdote and interpretation rather than systematic, representative data about grantmaking processes, outcomes, or counterfactuals. Key claims (e.g., near-zero mainstream grantmaking experience across AI funders; research being systematically overfunded relative to advocacy for unjustified reasons) are plausible but not robustly demonstrated. Alternative explanations (confidential internal rubrics; strategic reasons to favor research; funder constraints) are not fully explored. Overall: a well-argued, constructive critique with useful proposals, but supported mainly by illustrative rather than systematic evidence, so the central thesis is moderately supported rather than decisively proven."
  },
  "PostExternalValidation": {
    "post_id": "85yGkxJYfKnfCd9ME",
    "emperical_claim_validation_score": 5,
    "validation_notes": "Mixed. Several of the movement-level observations are supported (e.g., Open Phil\u2019s AI Governance & Policy team explicitly aims to distribute >$100M/year; Open Phil publishes RFP guidance and says EOIs are often not given feedback; LTFF uses a -5 to +5 PI scoring system; SFF provides anonymous recommender feedback). The author\u2019s anecdotes about receiving vague email rejections are plausible and consistent with public funder practices. However, key blanket claims are overstated or incorrect: there are documented instances of mainstream philanthropy grantmaking experience among people involved in this space (e.g., Carl Robichaud previously served as a program officer at the Carnegie Corporation), and several funders (Open Phil, Longview, LTFF, SFF) publish at least some formal criteria or descriptions of their grant investigation/scoring processes. Overall the post mixes accurate, well-supported facts with broad generalizations that aren\u2019t fully supported by public evidence.",
    "sources": [
      "EA Forum \u2014 original post by Jason Green-Lowe (Mainstream Grantmaking Expertise, Post 7 of 7). ([forum.effectivealtruism.org](https://forum.effectivealtruism.org/posts/85yGkxJYfKnfCd9ME/mainstream-grantmaking-expertise-post-7-of-7-on-ai?utm_source=openai))",
      "Open Philanthropy \u2014 \"New Roles on Our Global Catastrophic Risks Team\" (states AIGP aims to distribute >$100M/year). ([openphilanthropy.org](https://www.openphilanthropy.org/research/new-roles-on-our-gcr-team/?utm_source=openai))",
      "Open Philanthropy \u2014 Request for Proposals: AI governance (published criteria; description of application/feedback process). ([openphilanthropy.org](https://www.openphilanthropy.org/request-for-proposals-ai-governance/?utm_source=openai))",
      "Open Philanthropy \u2014 \"Our Progress in 2024 and Plans for 2025\" (spending figures / context on technical AI safety commitments). ([openphilanthropy.org](https://www.openphilanthropy.org/research/our-progress-in-2024-and-plans-for-2025/?utm_source=openai))",
      "Longview Philanthropy \u2014 Grantmaking page (describes their investigation questions and process). ([longview.org](https://www.longview.org/grantmaking?utm_source=openai))",
      "Long-Term Future Fund \u2014 EA Forum post 'What Does a Marginal Grant at LTFF Look Like?' (describes PI -5 to +5 scoring and thresholds). ([forum.effectivealtruism.org](https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding?utm_source=openai))",
      "Survival & Flourishing Fund (SFF) \u2014 Website / FAQ (explains S-Process, recommender anonymity, and that anonymous feedback is provided to applicants). ([survivalandflourishing.fund](https://survivalandflourishing.fund/?utm_source=openai))",
      "Evidence of mainstream philanthropy grantmaking experience: reporting showing Carl Robichaud formerly a Carnegie Corporation program officer. ([macfound.org](https://www.macfound.org/press/press-releases/carnegie-corporation-macarthur-award-11-grants-support-new-approaches-nuclear-security?utm_source=openai), [prnewswire.com](https://www.prnewswire.com/news-releases/six-nuclear-security-programs-win-major-grants-from-carnegie-corporation-of-new-york-300167742.html?utm_source=openai))",
      "Macroscopic Ventures \u2014 EA Forum overview and organization page (context on funder size and visibility). ([forum.effectivealtruism.org](https://forum.effectivealtruism.org/topics/macroscopic-ventures?utm_source=openai))"
    ]
  }
}